---
title: "3.1 Simple Linear Regression"
---
#simple-linear-regression #regression 

- Straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes there is a linear relationship between X and Y at the population level, ie. that the following formula holds
$$Y pprox eta_0 + eta_1X$$
> [!info]
> This can be referred to as *regressing* $Y$ *on/onto* $X$

- $eta_0$ and $eta_1$ are two unknown constants (i.e. model parameters) that represent the *intercept* and *slope* of the linear model.
  - We will use our training data to estimate $\hat{eta}_0$ and $\hat{eta}_1$, ie. the intercept and slope derived from our sample.
  - We can then estimate what the response will be for a given value of $x$ using:
$$\hat{y} = \hat{eta}_0 + \hat{eta}_1x$$
> [!info]
> We use the caret to denote an *estimated* value for an unknown parameter or coefficient (as opposed to the population value)

## 3.1.1 Estimating the Coefficients

- In practice, the population $eta_0$ & $eta_1$ are unknown. We must use data to estimate these coefficients.
- Our goal is to obtain coefficient estimates $\hat{eta}_0$ and $\hat{eta}_1$ such that the linear model fits the available data well (ie. $y_i pprox \hat{eta}_0 + \hat{eta}_1x_i$ for $i = 1, \ldots ,n$)
  - Put simply, we want to find intercept $eta_0$ and slope $eta_1$ such that the resulting line fits as *close as possible* to the *n* training data points.
  - There are many ways of measuring closeness, but weâ€™ll be focusing on minimising the *least squares* criterion in this chapter.

- $\hat{y}_i = \hat{eta}_0 + \hat{eta}_1x_i$ is the prediction for Y based on the *i*th value of X.
  - Then $e_i = y_i - \hat{y}_i$ is the *i*th *residual*; the distance between the observed value ($y_i$) and the predicted value (\hat{y}_i)
  - Summing and squaring (so that negative/positive residuals donâ€™t cancel each other out) all residuals, we get the *residual sum of squares*. This is a measure of the total distance between modelled and observed values for the dataset.
  - ==The least squares approach to linear regression aims to find $\hat{eta}_0$ and $\hat{eta}_1$ such that residual sum of squares is minimised.==

> [!info] Residual Sum of Squares (RSS)
> $$RSS = e_1^2 + e_2^2 + \ldots + e_n^2$$
> Expanding with $e_i = y_i - \hat{y}_i$ & $\hat{y}_i = \hat{eta}_0 + \hat{eta}_1x_i$ we get:
> $$RSS = (y_1 - \hat{eta}_0 - \hat{eta}_1x_1)^2 + (y_2 - \hat{eta}_0 - \hat{eta}_1x_2)^2 + \ldots + (y_n - \hat{eta}_0 - \hat{eta}_1x_n)^2$$

> [!info] Least Squares approach to Regression
> Using calculus, one can show that the values of $\hat{eta}_0$ and $\hat{eta}_1$ that minimise the RSS are:
> $$eta_1 = rac{\sum_{i=1}^{n} (x_i - ar{x})(y_i - ar{y})}{\sum_{i=1}^{n} (x_i - ar{x})^2}$$
> $$\hat{eta}_0 = ar{y} - \hat{eta}_1ar{x}$$
> Where $ar{y} quiv rac{1}{n}\sum_{i=1}^{n} y_i$ and $ar{x} quiv rac{1}{n}\sum_{i=1}^{n} x_i$ are the sample means.

^40a4e4
## 3.1.2 Assessing the Accuracy of the Coefficient Estimates

- In linear regression we assume the *true* relationship between X and Y is linear and hence takes the form $Y = f(X) + psilon =eta_0 + eta_1X + psilon$. This is the *population* regression line, ==our best linear approximation to the true relationship between X and Y==.
  - $eta_0$ and $eta_1$ are the intercept and slope estimates. $psilon$ is the catch-all (and mean-zero) error term for what we miss by approximating reality with this simple model.Â 
  - The true relationship is likely not linear, we may be missing other X variables that cause variation in Y, and there might be measurement error.
- The [[#^40a4e4|least squares estimates]] for $eta_0$ and $eta_1$ define the *least squares* line.

- We can see the difference between the two in the graph below. The red line is the *population line* plotted from a known linear relationship ($Y = 2 + 3X$). The blue line is the *least squares* line fit on data generated from that linear relationship *plus* the $psilon$ error term which is normally distributed with mean zero.
  - Further, the light blue lines in the graph on the right are least squares lines fit on samples of the generated data.

![](https://lh3.googleusercontent.com/J_H-NEQ6QEDM65B9crCrA-1_Jz4fKp2KKqPhjv6O9BgX1sCgnHE3KRou6zqNw-kVvqaqcceDHxAhNclFyg3zQ0o22WmqeCKsH-IL9dRsQa6dtuXDN4wCTN1ttmm2uVQoqzitfzuiJ4kKBb66cbwRqJI)

> [!info] Why are there multiple lines?
> - In practice, the population linear relationship (red line) is not known and must be approximated with data (any of the blue lines).Â 
> - Fundamentally, this is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population.
> ----
> - For example, say weâ€™re interested in knowing the population mean $\mu$ of some random variable $Y$.
> - Unfortunately, $\mu$ is unknown, but we do have access to $n$ observations from $Y$, $y_1, \dots, y_n$ from which we can reasonably estimate $mu pprox ar{y} = rac{1}{n} \sum_{i=1}^{n} y_i$ (we are estimating the population mean using the sample mean).
> - In other words, the sample mean and population mean are different, but the sample mean provides a good approximation of the population mean. This same concept applies to the approximation of $eta_0$ and $eta_1$ using $\hat{eta_0}$ and $\hat{eta_1}$ estimated from sample data.

### Bias & Variance
#### Unbiased estimator

- $\hat{eta_0}$ and $\hat{eta_1}$ are *unbiased estimates* of the true population parameters, which means that over multiple samples, the estimates will on average equal the true parameter.
  - I.e. These estimates will not systematically underor over-estimate the true population parameters.
#### Standard error

- The natural follow-up question is how accurate are $\hat{eta_0}$ and $\hat{eta_1}$ as estimates of $eta_0$ and $eta_1$? We answer this question by computing the *standard error* of $\hat{\mu}$ ($SE\left(\hat{\mu}ight)$).
  - The standard error tells us the average amount that the parameter estimates differ from the population parameter value.

> [!info] Standard error of the population mean
> $$	ext{Var}(\hat{\mu}) = 	ext{SE}(\hat{\mu})^2 = rac{\sigma^2}{n}$$

- Where $\sigma$ is the standard deviation of each of the realisations $y_i$ of $Y$.
  - The equation above also tells us that the deviation shrinks with $n$ - more observations means smaller standard error.
  - $\sigma$ and $\sigma^2$ are population parameters and are not known, but can be estimated from the data.

> [!info] Residual standard error (our estimate of $\sigma$)
> $$\hat{\sigma} = RSE = \sqrt{rac{RSS}{n - 2}}$$
> Where RSS is the [[Residual Sum of Squares]] - the total distance between modelled and observed values for the dataset

- The equations for standard error of $\hat{eta_0}$ and $\hat{eta_1}$ are:

> [!info] Standard error of the intercept and slope coefficients
> $$	ext{SE}(\hat{eta}_0)^2 = \sigma^2 \left( rac{1}{n} + rac{ar{x}^2}{\sum_{i=1}^{n} (x_i - ar{x})^2} ight)$$
> $$	ext{SE}(\hat{eta}_1)^2 = rac{\sigma^2}{\sum_{i=1}^{n} (x_i - ar{x})^2}$$

> [!info] Intuition
> - $	ext{SE}(\hat{eta}_1)^2$ (standard error for the slope estimate) is smaller when the $x_i$ are more spread out. Intuitively, we have more leverage to estimate the slope when this is the case.
> - $	ext{SE}(\hat{eta}_0)^2$ (standard error for the intercept estimate) is the same as $	ext{SE}(\hat{\mu})$ if $ar{x} = 0$. If this were the case, $\hat{eta}_0)$ would equal $ar{y}$.
### Confidence intervals

- We can use standard errors to compute [[2 Confidence Intervals|confidence intervals]].
  - A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter.
  - In other words, if we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter.

- In linear regression, the 95% confidence intervals for $eta_1$ approximately takes the form:

> [!info] Confidence intervals for $eta_0$ and $eta_1$
> $$eta_1 \pm 2 