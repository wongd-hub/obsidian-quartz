{"/":{"title":"The Second ðŸ§ ","content":"\nThis is a second brain that contains notes on things we want to learn.\n\nNotes are organised into several overarching topics:\n\n* Data Science\n    * Statistics\n        * [[(Course) Inferential Statistics - Coursera]]\n        * [[(Textbook) Introduction to Statistical Learning (2nd edn)]]\n    * Programming\n* Web Development\n    * Programming","lastmodified":"2023-08-18T01:52:42.621986303Z","tags":[]},"/.trash/1-Introduction":{"title":"1 Introduction","content":"#textbook-intro-to-statistical-learning\n\n## 1.1 An Overview of Statistical Learning\n\n\n\n## 1.2 Notation and Simple Matrix Algebra\n","lastmodified":"2023-08-18T01:52:42.617986262Z","tags":[]},"/.trash/2-Statistical-Learning":{"title":"2 Statistical Learning","content":"#textbook-intro-to-statistical-learning\n\n## 2.1. What is Statistical Learning?\n\n- We assume there's a relationship between $Y$ and $X = (X_1, X_2, \\dots, X_p)$ which can be represented in the general form:\n$$Y = f(x) + \u001bpsilon$$\n- Where $f$ is a fixed but unknown function of $X_1, \\dots, X_p$, and $\u001bpsilon$ is a random error term (independent of $X$ and mean 0). $f$ represents the systematic information that $X$ provides about $Y$, and is something we need to estimate based on observed points.\n- Statistical learning refers to the set of approaches for estimating $f$\n\n\n\n### 2.1.2 How Do We Estimate f?\n\n- Techniques for estimating $f$ can be characterised as either *parametric* or *non-parametric*.\n- *Parametric* techniques require that a functional form be assumed for the data, which provides a set of parameters that one needs to estimate using the data. i.e. it reduces the problem of estimating f down to one of estimating a set of parameters.\n\t- In general, fitting a more flexible parametric model involves fitting more parameters\n- *Non-parametric* techniques do not make assumptions about the functional form of $f$. However, since they don't reduce the problem down to that of fitting a finite set of parameters, they require a large number of observations to obtain an accurate estimate for $f$.\n\n### 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability\n\n- Why would we use a more restrictive method instead of a flexible approach? There is often a trade-off between how restrictive a model is and how interpretable it is.\n- If our goal is inference, then we'd likely choose a more restrictive model due to the high interpretability it allows. On the other hand, some methods are so flexible that they lead to complicated estimates of $f$ that are difficult to understand. In this case, it may be difficult to understand how any individual predictor is associated with the response. \n- However, if our goal is prediction accuracy, then we'd choose the method that provides the greatest performance over our dataset. This may not necessarily be the most flexible model type, due to the possibility that the flexibility allows the model to [[overfit]].\n\n### 2.1.4 Supervised Versus Unsupervised Learning\n\n- *Supervised learning* refers to the problem set where for each observation of the predictor measurements ($x_i$, $i=1,\\dots,n$) there is an associated response measurement ($y_i$). We aim to fit a model that relates the response to the predictors with either prediction or inference as the goal. ^31a009\n\t- Many statistical learning methods fall into this domain: linear/logistic regression, Generalised Additive Models, boosting, and support vector machines.\n- *Unsupervised learning* describes the situation where for every observation ($i=1,\\dots,n$), we have a vector of measurements ($x_i$) but no associated response ($y_i$). We do not have response data to supervise our analysis, which limits what we're able to achieve. ^0ebfb2\n\t- We can instead seek to understand the relationships between the variables or the observations using:\n\t\t- [[Cluster analysis|Cluster analysis/clustering]]: where the goal is to determine whether observations fall into distinct groups (i.e. categorising potential customers)\n- There are also *semi-supervised* situations where we have responses for some of the $n$ observations, but no responses for the rest of them. This may arise if it is expensive to obtain response data. We would want to use learning methods that can incorporate the observations for which there are responses, but also the ones without responses - however this is beyond the scope of this book.\n\n### 2.1.5 Regression Versus Classification Problems\n\n- Variables are either *quantitative* (numerical) or *qualitative* (categorical).\n- We refer to problems with a quantitative response as *regression* problems; whereas those with qualitative responses are referred to as *classification* problems.\n\t- Some techniques step into both definitions such as logistic regression which estimates class probabilities (regression), but then turns these into a binary response (classification).\n\t- Some techniques can be used in either quantitative or qualitative situations such as [[K-nearest neighbours]] and [[boosting]].\n\n## 2.2. Assessing Model Accuracy\n","lastmodified":"2023-08-18T01:52:42.617986262Z","tags":[]},"/.trash/3-Linear-Regression":{"title":"3 Linear Regression","content":"\n","lastmodified":"2023-08-18T01:52:42.617986262Z","tags":[]},"/.trash/Course-Reinforcement-Learning-Google-DeepMind/David-Silver":{"title":"David Silver","content":"\n","lastmodified":"2023-08-18T01:52:42.617986262Z","tags":[]},"/README":{"title":"README","content":"# A second ðŸ§ \n#moc \n\nTODO:\n- This vault has been published using [Quartz](https://quartz.jzhao.xyz/) to https://wongd-hub.github.io/obsidian-quartz. However there are some issues:\n    - Using `rsync` to copy files over from one repo to another upon push to this repo. This might be modifying certain symbols causing LaTeX to break (e.g. \u000c -\u003e FF). Need to fix.\n    - Need a better way to traverse the notes; add more Map of Contents\n- [ObsidianGPT](https://www.reddit.com/r/ObsidianMD/comments/1522umt/a_gpt_assistant_within_obsidian_trained_on_your/?utm_source=share\u0026utm_medium=ios_app\u0026utm_name=ioscss\u0026utm_content=2\u0026utm_term=3)\n","lastmodified":"2023-08-18T01:52:42.621986303Z","tags":[]},"/data-science-programming/r/advanced-r/2-Names-and-Values/2.1-Names-and-Values-Introduction":{"title":"2.1 Names and Values - Introduction","content":"#textbook_advanced-r #r\n\nIt is important to understand the distinction between an object and its name in R. This will help you:\n\n- More accurately predict the performance and memory usage of your code\n- Write faster code by avoiding accidental copies (which is a major contributor to slow code)\n- Better understand R's functional programming tools\n\nThe `lobstr` package will be used to dig into the internal representation of R objects\n","lastmodified":"2023-08-18T01:52:42.621986303Z","tags":[]},"/data-science-programming/r/advanced-r/2-Names-and-Values/2.2-Binding-Basics":{"title":"2.2 Binding Basics","content":"#textbook_advanced-r #r \n\n```r\nx \u003c- c(1, 2, 3)\n```\n\n- It's easy to read this as 'create an object named \"x\", containing the values 1, 2, 3', but this is an oversimplification and a misrepresentation of what occurs in the back end.\n- More accurately, R is:\n    - Creating an object (the vector of 1, 2, 3); and,\n    - Binding that object to a name, `x`\n\nIn other words, names are references to values. `c(1, 2, 3)` will now be given an address in memory; you can get this address by running `lobstr::obj_addr(x)`.\n\n- You can bind another name to an existing object in memory by doing this. Now both `x` and `y` point to the one object with a single address in memory\n\n```r\ny \u003c- x # now both x \u0026 y point to the same address\n\nlobstr::obj_addr(x) # address is 0x7feba894c538\nlobstr::obj_addr(y) # address is 0x7feba894c538 (the same)\n```\n\nIn a similar manner, if you're accessing a particular object by referencing it in different ways, it'll always point back to the same address.\n\n```r\nlobstr::obj_addr(mean)              #\u003e \"0x7febc89be000\"\nlobstr::obj_addr(base::mean)        #\u003e \"0x7febc89be000\"\nlobstr::obj_addr(get('mean'))       #\u003e \"0x7febc89be000\"\nlobstr::obj_addr(evalq(mean))       #\u003e \"0x7febc89be000\"\nlobstr::obj_addr(match.fun('mean')) #\u003e \"0x7febc89be000\"\n```\n\n## 2.2.1 Non-syntactic names\n\nA *syntactic* (i.e. valid) object name in R must consist of letters, digits, `.` and `_` but can't begin with `_` or a digit (it can start with a dot, as long as that dot isn't followed by a number). You also can't use reserved keywords (like `TRUE`, `if`, `function` - see full list with `?Reserved`).\n\nTrying to name something with a non-syntactic name the usual way will result in an error, if you want to do that, you'll need to wrap the name in backticks.\n\n```r\n`_abc` \u003c- 1\n`if` \u003c- 2\n```\n\nIt's unlikely that you'll want to name things like this, but you need to know how they work since you'll run into them when loading data.\n\n- `read.csv()` will by default automatically convert non-syntactic column names to syntactic ones Remove this behaviour by setting `check.names = FALSE`.\n    - It does this by calling `make.names()` which prepends `X` if necessary, converts invalid characters to `.`, and appends dots to R keywords.\n","lastmodified":"2023-08-18T01:52:42.621986303Z","tags":[]},"/data-science-programming/r/advanced-r/2-Names-and-Values/2.3-Copy-on-modify":{"title":"2.3 Copy-on-modify","content":"#textbook_advanced-r #r \n\nThe following code creates a vector and binds `x` and `y` names to that object. It then modifies `y`.\n\n```r\nx \u003c- c(1, 2, 3)\ny \u003c- x\n\ny[[3]] \u003c- 4\nx\n#\u003e [1] 1 2 3\n```\n\nModifying `y` didn't change `x`, so what happened to the shared object address?\n\n- The value associated with `y` changed. R created a new object to associate with `y` - a copy of `x` with one value changed. i.e. The copy only occurred when `y` was modified (*copy-on-modify*).\n- R objects are generally *immutable* (with a few [[important exceptions, including rules that lead to modify-in-place behaviour]]).\n\n## 2.3.1 `tracemem()`\n\nA useful tool for seeing how an object gets copied is by running `tracemem()` on the object. This will print a message telling you when the object was copied, its new address, and the sequence of calls that led to the copy.\n\n```r\nx \u003c- c(1, 2, 3)\ncat(tracemem(x), '\n') \n#\u003e \u003c0x7feb7bfeaf98\u003e \n\ny \u003c- x\ny[[3]] \u003c- 4L\n#\u003e tracemem[0x7feb7bfeaf98 -\u003e 0x7feb7bfb13f8]: \n\n# Turn memory tracing off\nuntracemem(x)\n```\n\nNote that if we modify `y` again, there won't be another copy. The new object that `y` is bound to will just get updated instead. This is because there is only one name bound to this object, so R applies [[modify-in-place optimisation]].\n\n## 2.3.2 Function calls\n\nThe same rules for copying also apply to function calls. The following code creates a function that calls the object passed as an argument and assigns it to the variable name we provide using arrow assignment.\n\n```r\nf \u003c- function(a) {\n  a\n}\n\nx \u003c- c(1, 2, 3)\ncat(tracemem(x), \"\n\")\n#\u003e \u003c0x7feb7e83c858\u003e \n\nz \u003c- f(x)\n# No copy here\n```\n\nWhile `f()` is running, the `a` points to the same value as the `x` does outside the function. Since it points to the same object with no modifications, the behaviour is the same as at the start of this page - i.e. `z` now points to the same value as `x` does.\n\nIf `f()` did modify `x`, then R would create a new modified copy and bind that to `z`\n\n## 2.3.3 Lists\n\nIt's not just names that point to variables, elements of lists do as well.\n\n```r\nl1 \u003c- list(1, 2, 3)\nl2 \u003c- l1\n```\n\nInstead of storing the values (1, 2, 3), the list stores *references* to them. When we copy the list, we're making a *shallow* copy; i.e. the list object and its bindings are copied, but the values pointed to by the bindings are not. \n\n![[Pasted image 20230804115514.png]]\n\nWhen we modify a list, only the modified items have their bindings changed\n\n```r\nl2[[3]] \u003c- 4\n```\n\n![[Pasted image 20230804115506.png]]\n\nTo see the values shared across lists, use `lobstr::ref()` which prints the memory address of each object along with a local ID so you can easily cross-reference shared components.\n\n```r\n\u003e lobstr::ref(l1, l2)\n#\u003e â–ˆ [1:0x7feb7bc31858] \u003clist\u003e \n#\u003e â”œâ”€[2:0x7feb7914d270] \u003cdbl\u003e \n#\u003e â”œâ”€[3:0x7feb7914d238] \u003cdbl\u003e \n#\u003e â””â”€[4:0x7feb7914d200] \u003cdbl\u003e \n#\u003e  \n#\u003e â–ˆ [5:0x7feb69e430d8] \u003clist\u003e \n#\u003e â”œâ”€[2:0x7feb7914d270] \n#\u003e â”œâ”€[3:0x7feb7914d238] \n#\u003e â””â”€[6:0x7feb7ef98500] \u003cdbl\u003e \n```\n\n## 2.3.4 Data-frames\n\nData frames are just lists of vectors, so the concepts in [[#2.3.3 Lists]] are easily extensible to this case.\n\nWhen you modify a column, only that column needs to be modified; the others will still point to their original references.\n\n```r\nd1 \u003c- data.frame(x = c(1, 5, 6), y = c(2, 4, 3))\nd2 \u003c- d1\nd2[, 2] \u003c- d2[, 2] * 2\n```\n\n![[Pasted image 20230804115842.png]]\n\nBut if you modify a row, you're changing values across every column, so all columns will be copied\n\n```r\nd3 \u003c- d1\nd3[1, ] \u003c- d3[1, ] * 3\n```\n\n![[Pasted image 20230804115924.png]]\n\n## 2.3.5 Character vectors\n\nThe final place where R uses references is in character vectors. How character vectors actually work is that there is a *global string pool*, and each element of a character vector points to a unique string in the pool. You can request the reference of each string in the pool using `lobstr::ref(., character = TRUE)`.\n\n```r\nx \u003c- c(\"a\", \"a\", \"abc\", \"d\")\nlobstr::ref(x, character = TRUE)\n#\u003e â–ˆ [1:0x7feb79c75b78] \u003cchr\u003e \n#\u003e â”œâ”€[2:0x7fec002d8fc8] \u003cstring: \"a\"\u003e \n#\u003e â”œâ”€[2:0x7fec002d8fc8] \n#\u003e â”œâ”€[3:0x7febca7dda88] \u003cstring: \"abc\"\u003e \n#\u003e â””â”€[4:0x7fec003c6e70] \u003cstring: \"d\"\u003e \n```\n\n![[Pasted image 20230804120121.png]]\n","lastmodified":"2023-08-18T01:52:42.621986303Z","tags":[]},"/data-science-programming/r/advanced-r/2-Names-and-Values/2.4-Object-Size":{"title":"2.4 Object Size","content":"#textbook_advanced-r #r \n\nYou can find how much memory an object takes with `lobstr::obj_size()`.\n- Elements of lists are just references to values to the following list won't be as large as you might think. It's only 80 B larger, and 80 B is the size of an empty list with three NULL elements.\n    - Due to this, `obj_size(x) + obj_size(y)` will only equal `obj_size(x, y)` if there are no shared values (see code block below for what `x` and `y` are)\n- Similarly, because of R's global string pool. Repeating a string 100 times will not take up 100 times as much memory.\n\n```r\n# Normal example\nobj_size(letters)\n#\u003e 1,712 B\nobj_size(ggplot2::diamonds)\n#\u003e 3,456,344 B\n\n# List example\nx \u003c- runif(1e6)\nobj_size(x)\n#\u003e 8,000,048 B\n\ny \u003c- list(x, x, x)\nobj_size(y)\n#\u003e 8,000,128 B\n\n# Character vector example\nbanana \u003c- \"bananas bananas bananas\"\nobj_size(banana)\n#\u003e 136 B\nobj_size(rep(banana, 100))\n#\u003e 928 B\n```\n\nR 3.5.0+ have a feature called ALTREP (*alternative representation*) which allows R to represent certain vectors very compactly. \n    - A common example of this is when using `:` to create an integer range. Instead of storing every number in the sequence, R just stores the start and end values. This means that every sequence, no matter how large, is the same size\n\n```r\nobj_size(1:3)\n#\u003e 680 B\nobj_size(1:1e3)\n#\u003e 680 B\nobj_size(1:1e6)\n#\u003e 680 B\nobj_size(1:1e9)\n#\u003e 680 B\n```\n","lastmodified":"2023-08-18T01:52:42.621986303Z","tags":[]},"/data-science-programming/r/advanced-r/2-Names-and-Values/2.5-Modify-in-Place":{"title":"2.5 Modify-in-Place","content":"#textbook_advanced-r #r \n\nThere are some exceptions to when R makes a copy of an object if it gets modified:\n- Objects with a single name binding get a special performance optimisation\n- Environments (a special type of object) are always modified in place\n\n## 2.5.1 Objects with a single binding\n\nThis behaviour is hard to predict due to some complications:\n- R can only count 0, 1, or many bindings to an object - so once an object receives two names and one goes away, R still counts it as 'many'.\n- The vast majority of functions make a (additional?) reference to the object when called on it. The exception are specially written 'primitive' C functions.\n\nIt's better to determine this empirically with `tracemem()` due to these complications.\n\nThis topic can be explored in the context of for-loops. For-loops have a reputation for being slow in R but this is because every iteration of the loop might be creating a copy.\n\n```r\nx \u003c- data.frame(matrix(runif(5 * 1e4), ncol = 5))\nmedians \u003c- vapply(x, median, numeric(1))\ncat(tracemem(x), '\n')\n#\u003e \u003c0x7feb8f240138\u003e \n\nfor (i in 1:5) {x[[i]] \u003c- x[[i]] - medians[[i]]}\n#\u003e tracemem[0x7feb8f240138 -\u003e 0x7febb07fc508]: \n#\u003e tracemem[0x7febb07fc508 -\u003e 0x7febb07fc498]: [[\u003c-.data.frame [[\u003c- \n#\u003e tracemem[0x7febb07fc498 -\u003e 0x7febb07fc428]: \n#\u003e tracemem[0x7febb07fc428 -\u003e 0x7febb07fc3b8]: [[\u003c-.data.frame [[\u003c- \n#\u003e tracemem[0x7febb07fc3b8 -\u003e 0x7febb07fc348]: \n#\u003e tracemem[0x7febb07fc348 -\u003e 0x7febb07fc2d8]: [[\u003c-.data.frame [[\u003c- \n#\u003e tracemem[0x7febb07fc2d8 -\u003e 0x7febb07fc268]: \n#\u003e tracemem[0x7febb07fc268 -\u003e 0x7febb07fc1f8]: [[\u003c-.data.frame [[\u003c- \n#\u003e tracemem[0x7febb07fc1f8 -\u003e 0x7febb07fc188]: \n#\u003e tracemem[0x7febb07fc188 -\u003e 0x7febb07fc118]: [[\u003c-.data.frame [[\u003c- \n```\n\nWe can see the data-frame is copied twice every iteration (note that it's actually copied 3 times in the book). This is because:\n- Copies are made by `\\[\\[.data.frame` \n- `\\[\\[.data.frame`  is a regular function that increments the reference count of `x`\n\nInstead of this, we can use lists. Modifying lists uses internal C code, so references are not incremented and only a single copy is made.\n\n```r\ny \u003c- as.list(x)\nuntracemem(x)\ncat(tracemem(y), '\n')\n#\u003e \u003c0x7feb6d2306b8\u003e \n\nfor (i in 1:5) {y[[i]] \u003c- y[[i]] - medians[[i]]}\n#\u003e tracemem[0x7feb6d2306b8 -\u003e 0x7feb7eed1878]: \n```\n\nIf you find yourself resorting to exotic tricks to avoid copies, it might be time to [[rewrite your function in C++]].\n\n## 2.5.2 Environments\n\nMore on this in [[Chapter 7]].\n\nAn environment's behaviour is different from other objects - they are always modified in place. this property is sometimes described as *reference semantics* because when you modify an environment, all existing bindings to that environment continue to have the same reference.\n\n```r\ne1 \u003c- rlang::env(a = 1, b = 2, c = 3)\ne2 \u003c- e1\n\n# Modifying one and viewing the other\ne1$c \u003c- 4\ne2$c\n#\u003e [1] 4\n```\n\nThis idea can be used to create functions that 'remember' their previous state ([[Section 10.2.4]]), this property is also used to implement the R6 object-oriented programming system ([[Chapter 14]]).\n\nBecause of this, environments can contain themselves; this is a unique property.\n\n```r\ne \u003c- rlang::env()\ne$self \u003c- e\n\nref(e)\n#\u003e â–ˆ [1:0x7fe114214cd8] \u003cenv\u003e \n#\u003e â””â”€self = [1:0x7fe114214cd8]\n```\n","lastmodified":"2023-08-18T01:52:42.621986303Z","tags":[]},"/data-science-programming/r/advanced-r/2-Names-and-Values/2.6-Unbinding-and-the-Garbage-Collector":{"title":"2.6 Unbinding and the Garbage Collector","content":"#textbook_advanced-r #r \n\nWhen we do this:\n\n```r\nx \u003c- 1:3\nx \u003c- 2:4\nrm(x)\n```\n\nWe're binding `1:3` to `x`, then binding `2:4` to `x` instead, then removing that binding. We're left with objects `1:3` and `2:4` with no bindings on either.\n\nHow do these objects get deleted? That's the job of the *garbage collector* (GC) which frees up memory by deleting R objects that are no longer used and requesting more memory from the operating system if needed.\n\nR uses a *tracing* GC, it traces every object that's reachable from the global environment and all objects that are reachable from those objects. The GC will run automatically whenever R needs more memory to create a new object.\n- It is hard to predict when R is going to run the GC, if you want to be alerted every time it runs, call `gcinfo(TRUE)` for console messages whenever it runs\n- You can force garbage collection by calling `gc()`, but there's never any *need* to call it yourself unless you need more memory released to your operating system so other programs can use it (or for the side effect which shows how much memory is being used).\n\nTo find the total memory used you can also run `lobstr::mem_used()`.\n\n```r\ngc() \n#\u003e           used (Mb) gc trigger  (Mb) limit (Mb) max used (Mb)\n#\u003e Ncells  884876 47.3    1698228  90.7         NA  1478961   79\n#\u003e Vcells 5026893 38.4   17228590 131.5      16384 17226182  132\n\nlobstr::mem_used()\n#\u003e 89,748,952 B\n```\n\nThese numbers won't match with the memory reported by your operating system because:\n1. It includes objects created by R but not by the R interpreter\n2. Both R and the operating system are lazy, they won't reclaim memory until it's actually needed. R might be holding on to memory because the OS hasn't yet asked for it back\n3. R counts the memory occupied by objects but there may be empty gaps due to deleted objects. This problem is known as memory fragmentation\n","lastmodified":"2023-08-18T01:52:42.621986303Z","tags":[]},"/data-science-programming/r/advanced-r/3-Vectors/3.1-Vectors-Introduction":{"title":"3.1 Vectors - Introduction","content":"#textbook_advanced-r #r \n\nVectors are the most important family of data types in base R. This chapter will go through how all the types fit together as a whole.\n\nVectors come in two flavours: atomic vectors and lists. they differ in terms of their element's types.\n- In atomic vectors, all elements must have the same type\n- For lists, elements can have different types\n\n`NULL` is closely related to vectors and often serves the role of a generic zero length vector. It is, however, not a vector.\n\n![[Pasted image 20230811162216.png]]\n\nEvery vector can have *attributes*, which can be thought of as a named list of arbitrary metadata. Two attributes are particularly important:\n- The *dimension* attribute, which turns vectors into matrices and arrays. We'll learn by R considers these 2D structures to be vectors.\n- The *class* attribute, which powers the [[S3 object system (Chapter 13)]]\n\nWe'll revisit S3 in Chapter 13, but here we'll discuss some of the most important S3 vectors: factors, date and times, data frames, and tibbles.\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/data-science-programming/r/advanced-r/3-Vectors/3.2-Atomic-Vectors":{"title":"3.2 Atomic Vectors","content":"#textbook_advanced-r #r \n\nThe four primary types of atomic vectors are:\n- Logical\n- Integer\n- Double\n- Character\n\nThere are two rarer types, complex (as in complex numbers) and raw (binary data). We won't discuss them since we will rarely ever use them.\n\n![[Pasted image 20230811162516.png]]\n\n## 3.2.1 Scalars\n\nEach type of vector has special syntax for creating a single value:\n\n- Logical: `TRUE`, `FALSE` or `T`, `F`\n- Integer: Same as double, but must be followed by an `L`, e.g. `4L` (this comes from the long integer type in C)\n- Double: `0.1234`, `1.23e4` (scientific), `0xcafe` (hexadecimal), `Inf`/`-Inf`/`NaN`\n- Character: Strings surrounded by `\"` or `'`, special characters escaped with `\\`\n\n## 3.2.2Â Making longer vectors withÂ `c()`\n\nCreate longer vectors by putting shorter ones into `c()`. Note that `c()` automatically *flattens* its inputs. You can determine the length or type of a vector with `length()` and `typeof()`. \n\n```r\nc(c(1, 2), c(3, 4))\n#\u003e [1] 1 2 3 4\n```\n\n## 3.2.3 Missing values\n\nR represents missing values as `NA`, these tend to be infectious so most computations involving a missing value will return another missing value. The only case where this doesn't apply is when the computation is an identity that holds for all possible inputs.\n\nThis also means that things like `x == NA` will always return `NA`. Use `is.na()` to check for missing-ness instead.\n\n```r\nNA \u003e 5\n#\u003e [1] NA\n!NA\n#\u003e [1] NA\n\nNA ^ 0\n#\u003e [1] 1\nNA | TRUE\n#\u003e [1] TRUE\n```\n\nThere are technically four different types of `NA`: `NA` (logical), `NA_integer_` (integer), `NA_real_` (double), and `NA_character_` (character).\n\n## 3.2.4 Testing and coercion\n\nYou can test if a vector is of a given type using `is.logical()`, `is.integer()`, `is.double()`, and `is.character()`. Avoid `is.numeric()`, `is.atomic()`, and `is.vector()` since they don't do what you'd expect them to do - check the documentation to find out more.\n\nFor atomic vectors, all elements must be the same type (type is a property of the *entire* vector). If you try to combine multiple types of values, they'll be coerced to the lowest common denominator: \n\n\u003e logical \u003c integer \u003c double \u003c character\n\nCoercion will generally happen automatically, most mathematical functions will coerce to numeric (`TRUE` will coerce to 1, `FALSE` to 0). You can deliberately coerce with `as.*()`.\n\n```r\nstr(c('a', 1))\n#\u003e chr [1:2] \"a\" \"1\"\n```\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/data-science-programming/r/advanced-r/3-Vectors/3.3-Attributes":{"title":"3.3 Attributes","content":"\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/data-science-programming/r/mastering-shiny/1.1-Your-first-Shiny-app-Introduction":{"title":"1.1 Your first Shiny app - Introduction","content":"\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/course-frontmatters/Course-Inferential-Statistics-Coursera":{"title":"(Course) Inferential Statistics - Coursera","content":"#course_coursera-inferential-stats #moc\n\nInstructor: Mine Ã‡etinkaya-Rundel | [Link to course](https://www.coursera.org/learn/inferential-statistics-intro)\n\n## About this Course\nThis course covers commonly used statistical inference methods for numerical and categorical data. You will learn how to set up and perform hypothesis tests, interpret p-values, and report the results of your analysis in a way that is interpretable for clients or the public. Using numerous data examples, you will learn to report estimates of quantities in a way that expresses the uncertainty of the quantity of interest. You will be guided through installing and using R and RStudio (free statistical software), and will use this software for lab exercises and a final project. The course introduces practical tools for performing data analysis and explores the fundamental concepts necessary to interpret and report results for both categorical and numerical data\n\n## Table of Contents\n\n- Week 1\n    - Administrative tasks, no course content\n- Week 2\n    - [[1 Central Limit Theorem and Sampling Distributions]]\n    - [[2 Confidence Intervals]]\n- Week 3\n    - [[3 Hypothesis Testing]]\n    - [[4 Statistical Significance]]\n- Week 4\n    - [[5 t-distribution and Comparing Two Means]]\n    - [[6 ANOVA and Bootstrapping]]\n- Week 5\n    - [[7 Inference for Proportions]]\n    - [[8 Simulation based inference for proportions and chi-square testing]]\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/course-frontmatters/Course-Reinforcement-Learning-Google-DeepMind":{"title":"(Course) Reinforcement Learning - Google DeepMind","content":"#course_google-deepmind-reinforcement-learning #moc \n\nInstructor: David Silver | [Link to course](https://www.youtube.com/watch?v=2pWv7GOvuf0) | [Website](davidsilver.uk/teaching)\n## Recommended textbooks\n\n- An Introduction to Reinforcement Learning, Sutton and Barto 1998\n    - Around 400 pages, good for intuition\n- Algorithms for Reinforcement Learning, Szepesvari 2010\n    - Less than 100 pages, more for the maths behind it all\n## Contents\n\n- **Part I: Elementary Reinforcement Learning**\n    - [[1 RL Introduction to Reinforcement Learning]]\n    - [[2 RL Markov Decision Processes]]\n    - [[3 RL Planning by Dynamic Programming]]\n    - [[4 RL Model-Free Prediction]]\n    - [[5 RL Model-Free Control]]\n- **Part II: Reinforcement Learning in Practice**\n    - [[6 RL Value Function Approximation]]\n    - [[7 RL Policy Gradient Methods]]\n    - [[8 RL Integrating Learning and Planning]]\n    - [[9 RL Exploration and Exploitation]]\n    - [[10 RL Case Study - RL in Games]]\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/course-frontmatters/Textbook-Introduction-to-Statistical-Learning-2nd-edn":{"title":"(Textbook) Introduction to Statistical Learning (2nd edn)","content":"#textbook_intro-to-statistical-learning #moc\n\nAuthors: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | [Website](https://www.statlearning.com/)\n\n## Table of contents\n\n1. Introduction\n\t- [[1.1 An Overview of Statistical Learning]]\n\t- [[1.2 Notation and Simple Matrix Algebra]]\n- Statistical Learning\n\t- [[2.1 What is Statistical Learning]]\n\t\t- [[2.1.1 Why Estimate f]]\n\t\t- [[2.1.2 How Do We Estimate f]]\n\t\t- [[2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability]]\n\t\t- [[2.1.4 Supervised Versus Unsupervised Learning]]\n\t\t- [[2.1.5 Regression Versus Classification Problems]]\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/forecasting-principles-and-practice/1.1-What-can-be-forecast":{"title":"1.1 What can be forecast","content":"","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/inferential-statistics/1-Central-Limit-Theorem-and-Sampling-Distributions":{"title":"1 Central Limit Theorem and Sampling Distributions","content":"#course_coursera-inferential-stats #statistics\n## Introduction\n\nWe start this course off with an example study by Pew Research which quotes:\n\n- **41%** of the public believe that young adults, rather than middle-aged or older adults, are having the toughest time in our economy.\n- Among all 18 to 34-year-olds, **49%** say they've taken a job they didn't want, just to pay the bills.\n\nFurther into the study, the margins of error are reported:\n\n- Margin of error is plus or minus **2.9** percentage points for results based on the total sample at the **95%** confidence level.\n- Margin of error is plus or minus **4.4** percentage points for adults aged 18-34 at the **95%** confidence level\n\nThis ultimately means that:\n\n- We are 95% confident that 38.1% to 43.9% (41%Â Â±Â 2.9%) of the public believe that young adults have the toughest time in this economy.\n- We are 95% confident that 44.6% to 53.4% (49%Â Â±Â 4.4%) of adults aged 18-34 have taken a job they didn't want, just to pay the bills.\n\nThe 41% and 49% both come from sample data. Querying the entire US population to come up with population estimates is clearly infeasible, so \u003cu\u003ewe use sample statistics (such as the 41% and 49%) as point estimates of unknown population parameters of interest\u003c/u\u003e.\n\nThe issue is that these sample statistics vary from sample to sample. Quantifying how they vary helps to provide a margin of error associated with the point estimate.\n\nThis unit will focus on sampling variability. Within this, we'll introduce the central limit theorem which describes shapes, centers and spreads of sampling distributions when certain conditions about the population, as well as the sampling scheme are met. After this, we'll move on to highlight techniques that are considered pillars of statistical inference, such as confidence intervals and hypothesis tests.\n\n## Sampling variability and Central Limit Theorem\n### Sampling distribution\n\nSay have a population, and we take a few different samples from that population. Say we then calculate some sample statistic from each distribution.\n\n![[Pasted image 20230711152456.png]]\n\nEach sample is it's own 'sample distribution', which is made up of randomly sampled observations from the population. The sample statistics themselves make up the 'sampling distribution', which is no longer made up of single randomly sampled observations but rather summary statistics from each of those samples.\n\n**Example**\n\nImagine we want to find the mean height of all women in the U.S.; $N$ = population size\n\n- If we had the height of all women, we could simply calculate $\\mu = \u000crac{x_1 + x_2 + \\dots + x_n}{N}$. The standard deviation would then be $\\sigma = \\sqrt{\u000crac{\\sum^N_{i=1}\\left(x_i - \u0008ar{x}\right)^2}{N}}$.\n\n- Assuming we don't have this information, assume we sample 1,000 women from each state and calculate the mean from each sample\n\n- AL: $x_{AL,1} + x_{AL,2} + x_{AL,1000} \to \u0008ar{x}_{AL}$\n\n- NC: $x_{NC,1} + x_{NC,2} + x_{NC,1000} \to \u0008ar{x}_{NC}$, etc\n\n- The mean of all $\u0008ar{x}_{STATE}$ will be a close approximation to the population mean, $\\mu$ ($mean(\u0008ar{x}) \u0007pprox \\mu$)\n\n- However $SD(\u0008ar{x}) \\lt \\sigma$; we would expect the average height from each state to be pretty close to one another\n\n- The standard deviation of the sample means is called the *standard error*\n\n- The mean of a random sample of 1,000 women is very unlikely to be very low or very high\n\n- So as $n$ increases, we expect each sample to yield more consistent sample statistic estimates, and hence the standard error decreases\n\n- The standard deviation of each sample should be roughly equal to the population sample deviation\n\n**Another example**\n\nSee [here](https://gallery.shinyapps.io/CLT_mean/) for an interactive example of how the Central Limit Theorem works.\n\n- We can see that increasing the sample size decreases the standard error\n\n### The Central Limit Theorem\n\n\u003e [!info]+ Central Limit Theorem (CLT)\n\u003e The distribution of sample statistics is nearly normal, centred at the population mean, and with a standard deviation equal to the population standard deviation divided by the square root of the sample size. i.e. $\u0008ar{x} \\sim N\\left(mean = \\mu, SE = \u000crac{\\sigma}{\\sqrt{n}}\right)$; where $\\sigma$ is the population standard deviation, however we can use a sample standard deviation, $s$ instead.\n\n\u003e [!tip] Obsidian Callouts\n\u003e **Note to self** - [more documentation on callouts in Obsidian](https://help.obsidian.md/Editing+and+formatting/Callouts)\n\n- This is called the 'central' limit theorem, since it's central to much of statistical inference theory.\n- The CLT tells us about the shape (normal), the center (the mean), and the spread of the sampling distribution (standard error).\n\n\u003e [!warning] Conditions for the CLT\n\u003e 1. **Independence**: Sampled observations must be independent; this is difficult to verify, but if the following conditions are met then this is more likely to be the case  \n\u003e     - Random sampling/assignment\n\u003e     - If sampling without replacement, n \u003c 10% of the population. We've previously said that we like large samples, and now we're saying to curb their size. We'll discuss this in more detail later.\n\u003e 2. **Sample size/skew**: Either the population distribution is normal, or if the population distribution is skewed, the sample size is large (e.g. n \u003e 3%). The more skewed the population, the larger we need the sample size to be.\n\u003e     - It's hard to verify that your population distribution is normal, but if you plot a sample distribution and it looks approximately normal, you can start to gain confidence that your population is nearly normal as well.\n\nFocusing in on the 10% condition: 'If sampling without replacement, n \u003c 10% of the population.'\n\n- Say you live in a very small town, the population is only 1,000 people; and your family lives here as well.\n    \n- Say a genetic research wants to take random samples from your town.\n    - If we want to take a sample of 10 out of 1,000; the likelihood that your family will be in this sample as well as you is low\n    - If we instead take a sample of 100 out of 1,000; the likelihood that your family will be in this sample as well as you will be higher\n\n- So while a larger sample is better, we also want to keep the size of our sample proportional to our population.\n    \n- When we're sampling without replacement (which isn't something we do in survey settings since you don't want to survey someone twice), the probability that we'll get both you and your family in a sample is static, which is why the 10% rule matters less there.\n\nFocusing in on the sample size/skew condition\n\n- A smaller sample size leads to a sampling distribution that mimics the population distribution; which isn't what we want if the population distribution is skewed.\n\n![[Pasted image 20230711153848.png]]\n\n- However, a larger sample size starts to approximate a normal distribution\n\n![[Pasted image 20230711153902.png]]\n\nWhy are we obsessed with having a normal distribution? Because once we have a normal sampling distribution, we can start calculating probabilities, [[2 Confidence Intervals|confidence intervals]], and p-values for [[3 Hypothesis Testing|hypothesis tests]].\n\n**Example**\n\nSeeÂ [here](https://gallery.shinyapps.io/CLT_mean/); and pick a skewed or uniform distribution for an interactive example of this works.\n\n- We can see that increasing the sample size increases the likelihood that the sampling distribution will be approximately normal.\n\n## CLT (for the mean), further examples\n\n### Example 1\n\nSuppose my iPod has 3,000 songs; the distribution of song lengths is shown below.\n\n- We also know that the mean length of songs on this iPod is 3.45 minutes, and the standard deviation is 1.63 minutes.\n- Calculate the probability that a randomly selected song lasts more than 5 minutes.\n\n![[Pasted image 20230711153925.png]]\n\n- How we usually approach these questions is to calculate a z-score and then find the associated probability.\n- However, this method can only be used when the distribution we're looking at is normal; and this is not the case for this example - our distribution is right-skewed.\n- What weÂ *can*Â do here is use the histogram and the height of the bars to estimate what percentage of songs fall between 4-5 minutes, 5-6 minutes, etc.\n- Eyeballing these bars, here's what we get:\n\n![[Pasted image 20230711153937.png]]\n\n### Example 2\n\nI'm going on a 6 hour roadtrip, and I make a playlist of 100 songs. What is the probability that my playlist lasts the entire drive?\n\n![[Pasted image 20230711153948.png]]\n\n- We haven't worked with sums of probabilities yet, but this is equivalent to the average song being at least 3.6 minutes (360 minutes / 100 songs).\n\n- Given we're working with the sample mean ($\u0008ar{x}$), we can use the Central Limit Theorem to figure out how the sample mean is distributed.\n\n- The CLT says $\u0008ar{x}$ will be distributed normally with mean equal to the population mean ($\\mu =$ 3.45 minutes), and standard error equal to the population standard deviation divided by the square root of the sample size ($SE = \u000crac{\\sigma}{\\sqrt{n}} = \u000crac{1.63}{\\sqrt{100}} = 0.163$); i.e. $\u0008ar{x} \\sim N\\left(3.45, 0.165)\right)$\n\n- Having all this information on how $\u0008ar{x}$ is distributed should prompt you to draw a curve to properly visualise what's going on.\n\n- To calculate probabilities over a certain range of the distribution, we'll use a Z-score. Note we divide by the standard error - not the population standard deviation, since we're looking at the distribution of the mean song length; not looking at a single song.\n  \n$$Z = \u000crac{\u0008ar{x} - \\mu}{SE} = \u000crac{3.6 - 3.45}{0.163} = 0.92$$\n\n- Using a standard normal distribution ($N\\left(0, 1\right)$), we can find $P(Z \u003e 0.92)$, which is 0.179. This is the probability that the playlist will last the entire roadtrip.\n\n![[Pasted image 20230711154025.png]]\n\n### Example 3\n\n![[Pasted image 20230711154034.png]]\n\n- 2 -\u003e B, since a sample from a population should yield roughly the same distribution as the parent population.\n- 3 -\u003e A, since samples from non-normal distributions with smaller sizes will approach but not reach normality.\n- 4 -\u003e C, since larger samples from non-normal distributions will follow the CLT and be normal.\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/inferential-statistics/2-Confidence-Intervals":{"title":"2 Confidence Intervals","content":"#course_coursera-inferential-stats #statistics \n## Introduction\n\nA plausible range of values for the population parameter is called a confidence interval.\n\n- Using only a sample statistic to estimate a population parameter is like fishing in a murky lake with a spear. Using a confidence interval is like fishing with a net.\n\n- In other words, if we report a point estimate, we are less likely to have the correct population parameter. However, if we report a confidence interval, we have a good shot at capturing the population parameter.\n\nBased on one sample's mean, $\u0008ar{x}$, how do we figure out what the range of plausible values is going to be?\n\n- Our sample mean is indeed our best guess of what the population parameter is, so our interval will be centred around this value.\n\n- From the [[1 Central Limit Theorem and Sampling Distributions|CLT]], we know $\u0008ar{x}$ is distributed nearly normally, and the centre of the distribution is the unknown population mean.\n\n- The final thing to note here is the 68%, 95%, 99.7% rule. This tells us that roughly 95% of sample means will be within 2 standard deviations of the population mean. So for 95% of random samples, the true population is going to be within 2 standard errors of that sample's mean.\n\n- So a 95% confidence interval can be constructed as the sample mean plus or minus two standard errors; i.e. $\u0008ar{x} \\pm 2SE$\n\n- What comes after the $\\pm$ is referred to as the *Margin of Error* (ME).\n![[Pasted image 20230711231359.png]]\n\n### Example problem\n\nOne of the earliest examples of behavioral asymmetry is a preference in humans for turning the head to the right rather than to the left during the final weeks of gestation and for the first six months after birth. This is thought to influence subsequent development of perceptual and motor preferences. A study of 124 couples found that 64.5% turn their heads to the right when kissing. The standard error associated with this estimate is roughly 4%. Which of the below is false?\n\n1. A higher sample size will yield a lower standard error.\n\n2. The margin of error for a 95% CI for the percentage of kissers who turn their heads to the right is roughly 8%.\n\n3. The 95% CI for the percentage of kissers who turn their heads to the right is roughly $64.5\\% \\pm 4\\%$.\n\n4. The 99.7% CI for the percentage of kissers who turn their heads to the right is roughly $64.5\\% \\pm 12\\%$.\n\nThe false option is 3 - the correct margin of error for a 95% confidence interval is 2 $\times$ SE, so 8%.\n\n## Confidence interval (for a mean)\n\nMore formally, the confidence interval for the population mean is the sample mean plus/minus the margin of error (which is the critical value corresponding to the middle x% of the normal distribution, multiplied against the standard error of the sampling distribution.)\n\n$$\u0008ar{x} \\pm z^*\u000crac{s}{\\sqrt{n}}$$\n\nWe mentioned before that the margin of error for a 95% confidence interval is 2 $\times$ the standard error. This is actually not exact.\n\n- So how do we find the exact critical value we need?\n\n- Remember that the confidence interval is the middle portion of the distribution, and that if we want a 95% confidence interval, we want the middle 95%.\n\n- We could use a Z-score to calculate the probability of getting the remaining 5% in the tail ends; but this really just equates to finding the area under the curve between the Z-scores.\n\n- In this case, since this is a probability distribution, the total area is 1; and therefore the critical value is $(1 - 0.95) / 2 = 0.025$\n\n- Performing an inverse normal distribution calculation (in R, `qnorm(0.025`) on the critical value of 0.025, we get a Z-score of 1.96, which is our exact value.\n\n```r\nqnorm((1 - 0.95) / 2)\n#\u003e -1.95996398454005\n```\n\nThere are some conditions that must be satisfied for this formula to apply, but since this confidence interval is based on the [[1 Central Limit Theorem and Sampling Distributions|CLT]], the conditions are the exact same:\n\n1. Independence\n\n2. Size and skew - need a larger sample size (n \u0026ge; 30) if the population is very skewed\n\nThe second condition is a little stricter than what we saw with the CLT as it imposes a minimum sample size (30). We'll talk about what to do if our sample size is smaller than 30 in a later lecture; so let's focus on 'large' sample sizes for now.\n\n## Accuracy vs. Precision (of confidence intervals)\n\n- Accuracy is defined as whether or not the confidence interval contains the true population parameter.\n\n- Precision is defined as the width of the confidence interval.\n\n- There is a trade-off between these two aspects which we will discuss now.\n\n**Confidence levels**\n\n- Suppose we took many samples and built confidence intervals for each one using the equation $\text{point estimate} \\pm 1.96 \times SE$.\n\n- Then 95% of these confidence intervals will contain the true population mean, $\\mu$.\n\n- Obviously this isn't how we generally come up with confidence intervals since we generally only work with one sample.\n\n- Further, the confidence level isn't something we calculate, and rather something we come up with that then affects the rest of our calculations.\n\nChanging the confidence level simply means adjusting the critical value aspect of the confidence interval formula.\n\n- Increasing the confidence level inherently means creating a wider confidence interval; in turn, this confidence interval is more likely to capture the true population parameter.\n\n- In other words, increasing our accuracy by increasing our confidence level will also decrease our precision.\n\n- For example, say the weather forecast predicts that the temperature tomorrow will be between $-20^\text{o}C$ and $100^\text{o}C$. Is this accurate? Yes, because it's likely to contain the true temperature for tomorrow. However, is it precise or in any way informative? No.\n\nIs it possible to increase both accuracy and precision without making trade-offs? Yes - the answer is to increase sample size, which shrinks our standard error and therefore our margin of error.\n\n#### Example\n\nThe General Social Survey (GSS) is a sociological survey used to collect data on demographic characteristics and attitudes of residents of the United States. In 2010, the survey collected responses from 1,154 U.S. residents. Based on the survey results, a 95% confidence interval for the average number of hours Americans have to relax or pursue activities that they enjoy after an average workday, was found to be 3.53 to 3.83 hours. Determine if each of the following statements are true or false.  \n\n1. 95% of Americans spend 3.53 to 3.83 hours relaxing after a work day\n\n- This is not true because the confidence interval is not about individuals in the population but instead about the true population parameter.\n\n1. 95% of random samples of 1,154 Americans will yield confidence intervals that contain the true average number of hours Americans spend relaxing after a work day\n\n- This is the exact definition of the confidence level: the percentage of random samples that will yield confidence intervals that contain the true population parameter.\n\n1. 95% of the time the true average number of hours Americans spend relaxing after a work day is between 3.53 and 3.83 hours\n\n- This is not true because the population parameter is not a moving target that is sometimes within an interval and sometimes outside of it.\n\n1. We are 95% confident that Americans in this sample spend on average 3.53 to 3.83 hours relaxing after a work day\n\n- This is not true because the confidence interval is not about the sample mean, but is instead about the population mean. We know with 100% certainty what the sample mean is here since we've observed this sample.\n\n## Required sample size for Margin of Error\n\nWhat would we need n to be to achieve a given ME?\n\n- With a target margin of error, the confidence level, and information on the variability of the sample, we can determine the required sample size to achieve a certain margin of error.\n\n- We do this by re-arranging the ME equation:\n\n$$ME = z^*\u000crac{s}{\\sqrt{n}} \to n = \\left(\u000crac{z^*s}{ME}\right)^2$$\n\nWe can see from this that as sample size increases, ME will decrease.\n\n#### Example\n  ![[Pasted image 20230711231600.png]]\n\n- The critical value associated with a 90% confidence level is 1.65 (`qnorm()`)\n\n- Note that this means that the *minimum* required sample size is 55.13; so we need to round up to get our answer.\n\n```r\nqnorm((1 - 0.90) / 2)\n#\u003e -1.64485362695147\n```\n\n#### Example 2\n\n![[Pasted image 20230711231635.png]]\n\n- In other words, to cut your margin of error by half, you'll need to increase your sample sizeÂ 22=4Â fold.\n\n## CI (for the mean) examples\n\n#### Example\n\n![[Pasted image 20230711231704.png]]\n![[Pasted image 20230711231709.png]]\n![[Pasted image 20230711231715.png]]\n![[Pasted image 20230711231719.png]]\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/inferential-statistics/3-Hypothesis-Testing":{"title":"3 Hypothesis Testing","content":"#course_coursera-inferential-stats  #statistics\n## Another Introduction to Inference\n\nWe'll start with an example of a first-principles hypothesis test that the last course ended on.\n\n#### Example\n\nIn the previous course, we looked at an experiment done on bank managers who were given dossiers on an equal amount of male and female employees who were all equally qualified. The percentage of males promoted was 88%, whereas the percentage of females promoted was 58%:\n\n![[Pasted image 20230711232609.png]]\n\nLooking at this information, there are two competing claims we could make:\n\n1. There is nothing going on - i.e. our Null Hypothesis\n\nThis posits that promotion and gender areÂ *independent*Â and hence there is no gender discrimination. In other words, the observed difference in proportions is simply due to chance.\n\n1. There is something going on - i.e. our Alternative Hypothesis\n\nPromotion and gender areÂ *dependent*, and there is gender discrimination here. The observed difference in proportions is not due to chance.\n\n#### How do we decide which claim has more evidence?\n\nThis hypothesis test was approached with a simulation-based approach:\n\n- The difference in proportions of males and females promoted was simulatedÂ *x*Â times under the assumption of the null hypothesis being true.\n- The probability of obtaining the results we got in our actual data or something more extreme was small, so we reject the null hypothesis in favour of the alternative hypothesis.\n\n![[Pasted image 20230711232624.png]]\n\n### Recap of example\n\nSo to conduct this hypothesis test we:\n\n- Set up a null hypothesis ($H_0$) that represents the status quo\n\n- Set up an alternative hypothesis ($H_A$) that represents our research question/what we're testing for\n\n- Conducted a hypothesis test under the assumption that the null hypothesis is true using either a simulation approach (as in the example) or theoretical methods (this course will focus on methods that rely on the [[1 Central Limit Theorem and Sampling Distributions|CLT]])\n\n- If the test results show that the data do not provide convincing evidence for the alternative hypothesis we stick with the null hypothesis, and vice versa.\n\n## Hypothesis Testing (for a mean)\n\n\u003e [!info] Note\n\u003e The hypotheses will always be about the ==population parameter==, and not the sample statistics. We already have the sample and therefore already know the sample statistics.\n\n- $H_0$: Often a skeptical perspective or a claim to be tested. Generally set the parameter of interest equal to same value.\n\n- $H_A$: An alternative claim under consideration, represented by a range of possible values (\u003c, \u003e, $\neq$)\n\nThe skeptic will not abandon $H_0$ unless the evidence in favour of $H_A$ is so strong that they reject $H_0$ in favour of $H_A$.\n\n#### Example\n\nEarlier we calculated a 95% [[2 Confidence Intervals|confidence interval]] for the average number of exclusive relationships college students have been in to be 2.7 to 3.7. Based on this confidence interval, do these data support the hypothesis that college students on average have been in more than three exclusive relationships?\n\nHere, $H_0$: $\\mu = 3$ \u0026 $H_A$: $\\mu \\gt 3$\n\nNote that the 95% confidence interval includes 3 in it; and any value within the confidence interval could conceivably be the population mean. Therefore, we cannot reject the null hypothesis in favour of the alternative.\n\nThis is a quick and dirty approach to hypothesis testing; but doing it this way we cannot calculate the likelihood of observing certain outcomes, i.e. the p-value.\n\n### p-value\n\nThe p-value is defined as: $P\\left(\text{observed or more extreme outcome}|H_0\text{ is true}\right)$\n\nIn the context of the example above, if we queried a sample of 50 students and got a mean of 3.2; and we find that the sample standard deviation is 1.74:\n\n$$n = 50; \u0008ar{x} = 3.2; s = 174 \to SE = 0.246$$\n\n1. Our p-value is then defined as $P\\left(\u0008ar{x} \u003e 3.2 | H_0: \\mu = 3\right)$\n\n2. Since we're assuming the null hypothesis to be true, we can construct the sampling distribution based on the CLT: $\u0008ar{x} \\sim N\\left(\\mu = 3, SE = 0.246\right)$\n\n3. The z-score for a sample mean of 3.2 is then $Z = \u000crac{3.2 - 3}{0.246} = 0.81$ (this is also called a test statistic since we're going to be using it to calculate our p-value)\n\n4. The p-value is then equal to $P(Z \u003e 0.81) = 0.209$ (`pnorm` gives the area under the Cumulative Distribution Function to the left of the entered z-score)\n\nIf the population mean of exclusive relationships in college students is truly 3, then there is a 21% chance that a random sample of 50 college students would yield a sample mean of 3.2 or higher.\n\n```r\n1 - pnorm(0.81)\n#\u003e 0.208970087871602\n```\n\n**Decision based on the p-value**\n\nWe just used the z-score to calculate the p-value, which is the probability of observing data at least as favourable to the alternative hypothesis as our current dataset - given that the null hypothesis is true.\n\n- If the p-value is low (lower than the *significance level*, $\u0007lpha$) we say that it would be very unlikely to observe the data if the null hypothesis were true, and therefore reject $H_0$.\n\n- If the p-value is high, then it is very likely to observe the data we currently have given that the null hypothesis were true, and we do not reject $H_0$.\n\nOur p-value in the previous sample was higher than the standard significance level of 5%, and therefore we do not reject the null hypothesis. In the context of the example, that means that there is not enough evidence to reject the null hypothesis that sets the population average of number of exclusive relationships college students have been in to 3. This suggests that the sample mean of 3.2 that the original sample yielded is likely due to chance or sampling variability.\n\n### Two-sided tests\n\nOften, instead of looking for divergence from the null hypothesis in a specific direction, we're interested in divergence in any direction. Tests that look at this are called two-tailed.\n\nThe definition of the p-value is the same across oneand two-tailed tests; however the calculation is slightly different since we need to consider extremity on two sides of the distribution.\n\nIn the context of our previous example, the p-value would instead be equal to $P\\left(\u0008ar{x} \u003e 3.2 \text{ OR } \u0008ar{x} \u003c 2.8| H_0: \\mu = 3\right) = P(Z \u003e 0.81) + P(Z \u003c -0.81) = 0.209 \times 2 = 0.418$\n\n### Recap\n\nRecall that asserting that the shape of the sampling distribution is normal means that we're using the [[1 Central Limit Theorem and Sampling Distributions|CLT]]; so the same conditions for CLT will apply here.\n\n![[Pasted image 20230711233136.png]]\n\n## HT for the mean examples\n\n#### Example 1\n\nResearchers investigating characteristics of gifted children collected data from schools in a large city on a random sample of 36 children who were identified as gifted children soon after they reached the age of four. In this study, along with variables on the children, the researchers also collected data on their mothers' IQ scores. The histogram shows the distribution of these data, and also provided our some sample statistics.\n\n![[Pasted image 20230711233151.png]]\n\nWe're asked to perform a hypothesis test to evaluate if these data provide convincing evidence of a difference between the average IQ score of mothers of gifted children and the average IQ score for the population at large, which happens to be 100. We're also asked to use a significance level of 0.01.\n\nOur parameter of interest is the mean here, so we set $\\mu = \text{average IQ score for mothers of gifted children}$.\n\n1. Set the hypotheses: Our null hypothesis is that $H_0: \\mu = 100$; and our alternative is $H_A: \\mu \neq 100$\n\n2. Calculate the point estimate: This is given to us as our sample mean ($\u0008ar{x} = 118.2$)\n\n3. Check the conditions:\n\n- We have a random sample, and 36 \u003c 10% of all gifted children; so we can assume independence (i.e. the IQ of one mother in this sample is independent of another one)\n\n- We know that the sample size is \u0026ge; 30, and the sample distribution appears non-skewed so we can expect a normally distributed sampling distribution\n\n- Knowing this, we can assert that, if the null hypothesis is true, then $\u0008ar{x} \\sim N\\left(100, \u000crac{6.5}{\\sqrt{36}} \u0007pprox 1.083\right)$\n\n1. Calculate the test statistic:\n\n$$Z = \u000crac{118.2 - 100}{1.083} = 16.8 \to P(Z \u003e 16.8) + P(Z \u003c -16.8) \u0007pprox 0$$\n\n1. Make a decision and interpret in context of the research question\n\nThe smaller the p-value, the stronger the evidence is against the null hypothesis. Since our p-value is smaller than the significance level, we can conclude that the data provides strong enough evidence that the average IQ of mothers of gifted children is different to the average IQ score for the population at large.\n\n#### Example 2\n\nA statistics student interested in sleep habits of domestic cats took a random sample of 144 cats and monitored their sleep. The cats slept an average of 16 hours per day. According to our online resources, domestic dogs actually sleep on average 14 hours a day. We want to find out if these data provide convincing evidence of different sleeping habits for domestic cats and dogs with respect to how much they sleep. Note that the test statistic calculated was 1.73.\n\nWe're told that our sample mean is 16 hours, and that dogs sleep 14 hours per day. For us, $\\mu = \text{average hours cats sleep}$, and from the sample, $\u0008ar{x} = 16$.\n\n1. Set the hypothesis: $H_0: \\mu = 14$; $H_A: \\mu \neq 14$\n\n2. Calculate the point estimate: $\u0008ar{x} = 16$\n\n3. Check the conditions: Here we assume the conditions are okay, or have already been checked\n\n4. Calculate the test statistic:\n\n$$Z = \u000crac{16 - 14}{\u000crac{s}{\\sqrt{144}}} = 1.73 \to P(Z \u003e 1.73) + P(Z \u003c -1.73) = 0.0836$$\n\n```r\npnorm(-1.73) * 2\n#\u003e 0.0836302752271899\n```\n\n1. What is the interpretation of this p-value in the context of the research question?\n\nThe probability of obtaining a random sample of 144 cats whose average house slept per day is greater than 16 or smaller than 12 - conditional on cats truly sleeping 14 hours per day on average - is 8.4%.\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/inferential-statistics/4-Statistical-Significance":{"title":"4 Statistical Significance","content":"#course_coursera-inferential-stats #statistics \n## Inference for Other Estimators\n\nThe methods we've been learning can be applied to other estimators that have nearly-normal sampling distributions, which are listed here:\n\n- The sample mean ($\u0008ar{x}$)\n- The difference between two sample means ($\u0008ar{x}_1 - \u0008ar{x}_2$)\n- Sample proportion ($\\hat{p}$)\n- The difference between sample proportions ($\\hat{p}_1 - \\hat{p}_2$)\n\n### Unbiased estimator\n\nAn important assumption about these point estimates is that they are unbiased - their sampling distribution is centred on the true population parameter it estimates. In other words, it does not naturally overor under-estimate the population parameter.\n\nFor nearly-normal estimators, the confidence interval equation remains $\text{point estimate} \\pm z^*SE$; all we'll need to do is swap out the formula for the standard error of a different estimate.\n\nWe'll go through high level examples here using different standard errors, but we'll go into more detail on each of these later on.\n\n#### Example 1\n\nA 2010 Pew Research foundation poll indicates that among 1,099 college graduates, 33% watch the Daily Show. An American late-night TV Show. The standard error of this estimate is 0.014. We are asked to estimate the 95% confidence interval for the proportion of college graduates who watch The Daily Show.\n\nWe have a few pieces of information given to us:\n\n- $n = 1,099$\n- $\\hat{p} = 0.33$\n- $SE = 0.014$\n\nNow to our [[2 Confidence Intervals|confidence interval]]: $\text{point estimate} \\pm z^*SE \to 0.33 \\pm 1.96 \times 0.014 \to \\left(0.303, 0.357\right)$. The critical value is still 1.96 because this point estimate is still nearly-normal.\n\nSo we are 95% confident that between 30.3% and 35.7% of college graduates watch the Daily Show.\n\nSince this estimator is unbiased, and again - the sampling distribution of this estimator is nearly-normal, we can calculate a z-score from which we can use to produce probabilities.\n\n$$Z = \u000crac{\text{point estimate} - \text{null value}}{SE}$$\n\n#### Example 2\n\nThe third national health and nutrition examination survey NHANES, collected body fat percentage and gender data from over 13,000 subjects in ages between 20 to 80. The average body fat percentage for the 6,580 men in the sample was 23.9%. And this value was 35% for the 7,021 women. The standard error for the difference between the average male and female body fat percentages was 0.114. Do these data provide convincing evidence that men and women have different average body fat percentages? You may assume that the distribution of the point estimate is nearly normal.\n\nThe population parameter we'll be estimating here is $\\mu = \text{the average body fat percentage in the population}$; but across two different population groups.\n\n1. Set hypotheses: $H_0: \\mu_{men} = \\mu_{women}$, $H_A = \\mu_{men} \neq \\mu_{women}$\n\n2. Calculate the point estimate: $\\mu_{men} - \\mu_{women} = 23.9 - 35 = -11.1$\n\n3. Check conditions: We're told we can assume that the distribution is nearly normal; and since this is a nationwide survey we can assume that random sampling has been used and the observations are independent with respect to their body fat percentages.\n\n4. Calculate the test statistic\n\nWhat does our sampling distribution centre on? It usually centres on the null hypothesis value of the population parameter, but the original form of our hypothesis does not have an asserted value. For tests between two population means, we can re-write the null hypothesis as:\n\n$$H_0: \\mu_{men} = \\mu_{women} \to \\mu_{men} - \\mu_{women} = 0$$\n\nSo our sampling distribution here centres on 0, and our p-value is looking at the area under the CDF for values smaller than -11.1 and values greater than 11.1.\n\nWe do not yet know how to calculate the sample standard error of the difference between two population means, but we are given this value for now, so we can calculate our z-score as $Z = \u000crac{-11.1 - 0}{0.114} = -97.4$.\n\n$$P(Z \u003c -97.4) + P(Z \u003e 97.4) \u0007pprox 0$$\n\n1. What is the interpretation of this p-value in the context of the research question?\n\nThere is strong evidence to reject the null hypothesis as the p-value is very small.\n\nIf there were truly no difference between the average body fat percentage of males and females, there would be a nearly 0% chance of getting data like or more extreme than ours. This is strong evidence to reject the hypothesis that there is no difference between males' and females' average body fat percentage.\n\n### Decision Errors\n\n[[3 Hypothesis Testing|Hypothesis tests]] are not flawless. Just like in court cases, innocent people can be incarcerated and guilty people can walk free. However, we have the tools necessary to quantify how often we make errors in statistics.\n\nThere are two main types of errors in hypothesis testing, the Type I and Type II errors - whose rates are inversely proportional.\n\n- A Type I error is rejecting $H_0$ when $H_0$ is true\n\t- This is like declaring the defendant guilty, when they are actually innocent\n- A Type II error is failing to reject $H_0$ (i.e. rejecting $H_A$) when $H_A$ is true\n\t- This is like declaring the defendant innocent, when they are actually guilty\n\n![[Pasted image 20230712215313.png]]\n\nWe almost never know if the null or the alternative hypothesis are true with 100% certainty, but we need to consider all possibilities.\n\n#### Which error is the worst to make?\n\nThis is a subjective question; however this brings to mind a quote by William Blackstone: 'better that 10 guilty persons escape, than one innocent person suffers' (so a Type I is worse in this case).\n\n#### Type I errors\n\n- We reject $H_0$ when the p-value is below the significance level ($\u0007lpha$), which is generally set at 5%.\n\n- This translates to us not wanting to reject $H_0$ incorrectly at least 5% of the time.\n\nHence, the probability of making a Type I error is equal to $P\\left(\text{Type I error} | H_0\text{ true}\right) = \u0007lpha$; this is why we prefer smaller values of $\u0007lpha$.\n\nSo how do we choose our $\u0007lpha$?\n\n- If a Type I error is especially dangerous or costly, set your significance level small to reduce the chance of a Type I error. By doing this, we demand very strong evidence that the null hypothesis is not the case in order to reject it.\n\n- However, if a Type II error is more costly, choose a larger significance level. This translates to us being cautious about failing to reject the null hypothesis when the null hypothesis is actually false.\n\n#### Probabilities\n\n![[Pasted image 20230712215340.png]]\n\nSo:\n\n- Given that the null hypothesis is true; you can either:\n\t- Incorrectly reject the null hypothesis, with probability $\u0007lpha$\n\t- Correctly retain the null hypothesis, with probability $1 - \u0007lpha$\n\n- Given that the alternative hypothesis is true; you can either:\n\t- Incorrectly reject the alternative hypothesis, with probability $\u0008eta$ (your Type II error)\n\t- Correctly reject the null hypothesis, with probability $1 - \u0008eta$. This probability is referred to as the *power* of your test, and is the probability of correctly rejecting $H_0$ (given $H_A$ is true). Note that $\u0008eta$ is not trivial to calculate.\n\nOur goal is to keep both $\u0007lpha$ and $\u0008eta$ low at the same time; but we know that if we push one down, the other will shoot up. We generally want to find a good balance between the two.\n\n#### Type II errors\n\n- If the alternative hypothesis is actually true, what is the probability that we incorrectly reject it?\n\n- The answer is not obvious.\n\t- If the true population mean is very close to the null hypothesis, then it will be hard to detect a difference and therefore reject $H_0$.\n\t- Clearly then, $\u0008eta$ is dependent on the effect size ($\\delta$) - which is the difference between the point estimate and the null value.\n\n## Significance vs. Confidence Level\n\nSo far we've looked at two inferential techniques; [[2 Confidence Intervals|confidence intervals]] and [[3 Hypothesis Testing|hypothesis testing]]. These methods are related and therefore it makes sense that their results will agree with each other if the confidence and significance levels are set the same.\n\nBroadly speaking, we can say that the significance level and confidence level are complements of each other.\n\n- The most commonly used significance level is 5%, whereas the most commonly used confidence level is 95%. These are complements.\n\n- However, whether this complement rule works or not depending on if we're doing a oneor two-sided hypothesis test.\n\t- Recall that confidence intervals are centred on the mean value of the sampling distribution. This means that they will be most related to two-sided hypothesis tests that are also centered on the mean of the sampling distribution.\n\nSo:\n\n- A two-sided hypothesis test with significance level of $\u0007lpha$ is equivalent to a confidence interval with CL = $1 - \u0007lpha$.\n- A one-sided hypothesis test with significance level of $\u0007lpha$ is equivalent to a confidence interval with CL = $1 - 2 \times \u0007lpha$.\n- If $H_0$ is rejected, a CI that agrees with its corresponding hypothesis test should *not include* the null value.\n- If $H_0$ is failed to be rejected, a CI that agrees with its corresponding HT should *include* the null value.\n\n## Statistical vs. Practical Significance\n\nWhile sometimes statistical and practical significance go hand in hand, that is not always the case.\n\nConsider two scenarios, ceteris paribus, in which scenario will the p-value be lower if hypothesis tests were done with:\n\n1. We have n = 100; and,\n2. We have n = 10,000\n\nWe can see how this will affect the hypothesis test by considering the components that it affects first: the standard error.\n\n$$Z = \u000crac{\u0008ar{x} - \\mu}{SE} = \u000crac{\u0008ar{x} - \\mu}{\u000crac{s}{\\sqrt{n}}}$$\n\nAs $n$ increases, the SE will decrease, driving up the Z-score which means that the point at which you measure your p-value on the distribution (i.e. your test statistic) is being moved further from the center; thus decreasing p-value.\n\n#### Another example\n\nConsider the following example where the sample statistic is very close to the null hypothesis parameter value:\n\n$$H_0: \\mu = 49.5, H_A: \\mu \u003e 49.5, \u0008ar{x} = 50, s = 2$$\n\nIf n = 100; $Z = \u000crac{50-49.5}{\u000crac{2}{100}} = \u000crac{50-49.5}{\u000crac{2}{10}} = 0.5/0.2 = 2.5$\n\nIf n = 10,000; $Z = \u000crac{50-49.5}{\u000crac{2}{10000}} = \u000crac{50-49.5}{\u000crac{2}{100}} = 0.5/0.02 = 25$\n\nClearly this is a very large difference in outcomes. A z-score of 25 will have a near-zero p-value, and so is a highly statistically significant finding - however, is it practically significant?\n\nWhen we're thinking about practical significance, we focus on the effect size ($\\delta$), the difference between your po int estimate and your null value (the numerator in the z-score calculation). In both of the scenarios above, we have a relatively small $\\delta$ which may not be practically significant, we are able to find a statistically significant result *simply by inflating our sample size*.\n\nTo recap:\n\n- Real difference between the point estimate and the null value are easier to detect with larger sample sizes.\n\n- However, very large samples will result in statistical significance for very small effect sizes that may not be practically significant.\n\n- Usually to mitigate this, analysis is done a priori to the hypothesis test to determine what size sample should be gathered. This likely will include consultation with statisticians.\n\n\"To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination. He may be able to say what the experiment died of.\" - R.A. Fisher\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/inferential-statistics/5-t-distribution-and-Comparing-Two-Means":{"title":"5 t-distribution and Comparing Two Means","content":"#course_coursera-inferential-stats #statistics \n## Inference for Numerical Variables\n\nWe'll build upon the tools we've used to perform inference on numerical variables in this unit. We'll be looking at:\n\n- Comparing multiple means against one another (two, or more using [[6 ANOVA and Bootstrapping#ANOVA|ANOVA]])\n- A technique that allows one to create confidence intervals for estimators that do not adhere to the Central Limit Theorem (such as the median): [[6 ANOVA and Bootstrapping#Bootstrapping|bootstrapping]]\n    - This is a simulation-based method which does not impose as rigid conditions as using the CLT does\n    - We'll learn how to bootstrap as well as when to bootstrap\n- Working with small samples (n \u003c 30) - spoiler: the t-distribution is used here\n\n## t-distribution\n\nThe t-distribution will be useful for describing the distribution of the sample mean when the population $\\sigma$ is unknown, which is almost always. Let's discuss the conditions for inference so far, as a motivation for why we need the t-distribution.\n\n### Conditions\n\n- What purpose does a large sample serve? As long as your observations are independent, and the population distribution is not extremely skewed, a large sample will ensure that:\n\t- The sampling distribution of the mean is nearly normal; and,\n\t- The estimate of the standard error is reliable.\n\t\t- Recall that $SE = \u000crac{s}{\\sqrt{n}}$, where $s$ is the standard deviation of our sample which is our best estimate of the unknown population standard deviation. If our sample is large enough, it'll be more likely that $s$ is a good estimate of the population standard deviation, and therefore the standard error estimation is reliable.\n\nWhat if your sample size is small?\n\n- The uncertainty of the standard error estimate is addressed by using the t-distribution.\n\n### t-distribution details\n\n- When the population $\\sigma$ is unknown (almost always), use the t-distribution to address the uncertainty of the standard error estimate\n\n- Similar to the normal distribution, but with thicker tails, and a less tall modal point\n\n![[Pasted image 20230712220139.png]]\n\n- This means that observations are more likely to fall greater than 2 standard deviations from the mean\n\t- As a result, [[2 Confidence Intervals|confidence intervals]] constructed from the t-distribution will be wider, and therefore more conservative than those from the normal distribution\n\ni.e. these thick tails are helpful for mitigating the effect of a less reliable estimate of the standard error of the sampling distribution.\n\n- The t-distribution is centred at 0 like the normal distribution, and has one parameter - degrees of freedom (df) - which determines the thickness of the tails.\n\t- As a reminder, the normal distribution has two parameters - the mean and standard deviation\n\t- As the df increases, the t-distribution approaches the normal distribution\n\n### t-statistic\n\nHow do we actually use the t-distribution in statistical inference?\n\n- Use the t-distribution on a single mean, or for comparing two means, when the population standard deviation is unknown (almost always).\n\n- Calculate the T-statistic just as you would a Z-score: $T = \u000crac{\text{obs} - \text{null}}{SE}$, then find the corresponding p-value from the t-distribution\n\n#### Examples\n\nWe can see that the t-statistic estimate using the same Z-score is closer to the normal distribution estimate when the degrees of freedom are higher. As the degrees of freedom decrease, the tails become heavier, and the p-value associated with a given T-statistic increases.\n\n```r\n# P(|Z| \u003e 2)\npnorm(-2) * 2\n#\u003e 0.0455002638963584\n\n#P(|t(df = 50)| \u003e 2)\npt(-2, df = 50) * 2\n#\u003e 0.0509470687376933\n\n#P(|t(df = 10)| \u003e 2)\npt(-2, df = 10) * 2\n#\u003e 0.0733880347707404\n```\n\nIf we were considering a [[3 Hypothesis Testing|hypothesis test]] with a test statistic of 2, under which of these scenarios would you reject a null hypothesis at the 5% level?\n\n1. You would reject the null hypothesis\n\n2. You would not reject the null hypothesis\n\n3. You would not reject the null hypothesis\n\nAs we get more conservative with a t-distribution with lower degrees of freedom, we also become less likely to reject the null hypothesis. We'll discuss how to calculate degrees of freedom later, but in general - the smaller your sample size, the lower the df, and the harder it is to reject the null hypothesis.\n\n### Origins of the t-distribution\n\n- Developed by William Gosset (whose pseudonym was Student, hence Student's t-distribution)\n- Gosset worked at Guinness Brewing Company as the Head Experimental Brewer, and his job was to gradually create a more consistent and economical Guinness brew.\n- Working under constraints, Gosset generally only had small sample sizes to work with, so most of the groundwork laid for this distribution was done in service to the Guinness stout.\n\n## Inference for a mean\n\n#### Example\n\nResearch paper discussing and analysing the effects of being distracted while you eat, and how it affects your recall of what you ate. Hypothesis is that people who did not recall what they ate for lunch will be more prone to snacking later on.\n\n44 people in this study, 22 men and 22 women. Randomised into two groups; one played solitaire whilst eating, and one who just ate. After lunch, they were offered biscuits to snack on\n\nResults are as follows. The goal for this video is to estimate average snacking level for distracted eaters.\n\n![[Pasted image 20230712220330.png]]\n\n**Estimating the mean**\n\nLet's aim to estimate the mean biscuit intake of the sample who played video games.\n\n- To estimate the mean, we'll create a confidence interval. For t-distributions, the calculation of the standard error is the same as the normal distribution, so the only thing that changes in our CI formula is the critical value:\n$$\text{point estimate} \\pm t_{df}^* \times \u000crac{s}{\\sqrt{n}}$$\n\n- When working with data from one sample, and working with one single mean,\n$$df = n - 1$$\n\n- We lose one degree of freedom because we're estimating the standard error of the sample mean using the sample standard deviation.\n\n- To get the critical t-statistic, we use the inverse t-distribution function, `qt(percentile, df)`:\n\n```r\nqt(0.025, df = 22 - 1) # since the sample size was 22\n#\u003e -2.07961384472768\n```\n\nWe now have our mean, sample standard deviation, sample size, and critical t-statistic. Our confidence interval is then:\n\n$$52.1 \\pm 2.08 \times \u000crac{45.1}{\\sqrt{22}} = \\left(32.1, 72.1\right)$$\n\nTherefore, we are 95% confident that distracted eaters consume between 32.1g and 72.1g of snacks post-meal.\n\n**Performing a hypothesis test**\n\nNext, consider that the suggested serving size of biscuits is 30g; do these data suggest that there is a statistically significant difference between the suggested serving size and the amount that distracted eaters eat?\n\n- $H_0: \\mu = 30, H_A: \\mu \neq 30$\n- $T = \u000crac{52.1 - 30}{9.62} = 2.3$\n- $df = 22 - 1$\n\n```r\npt(-2.3, 21) * 2\n#\u003e 0.0318022759865702\n```\n\nThis is less than the significance level of 5%, so we reject the null hypothesis and conclude that distracted eaters consume, on average, more than the suggested serving size of 30.\n\n**Conditions**\n\nLooping back around to check that the conditions are met:\n\n- **Independence**: assignment was random, and 22 \u003c 10% of all distracted eaters\n- **Sample size/skew**: we are given the sample mean and sample standard deviation, we can tell that the distribution will be cut off near 0 since the standard deviation is so close to value of the mean.\n\n![[Pasted image 20230712221251.png]]\n\n## Inference for comparing two independent means\n\n**Confidence intervals**\n\nTo produce confidence intervals for the difference between two means, we do the following:\n\n$$\\left(\u0008ar{x}_1 - \u0008ar{x}_2\right) \\pm t^*_{df}\times\\sqrt{\u000crac{s_1^2}{n_1} + \u000crac{s_2^2}{n_2}}$$\n\nNote that we add the two variances even though we're looking for the standard error of the difference between the two means. Conceptually, this is bringing together two measures with an inherent variability around them. When you bring two unknowns together, the result should always be more variable (regardless of if you're adding or subtracting them).\n\n$$df = min(n_1-1, n_2-1)$$\n\nThis is a conservative (since it relies on the lower df) estimate, because the exact df value is tedious to compute by hand.\n\n**Conditions**\n\n- Independence\n\t- Within groups: sampled observations must be independent\n\t\t- Random sampling/assignment\n\t\t- If sampling without replacement, both $n_1$ and $n_2$ to be \u003c 10% of their respective populations\n\t- Between groups: groups must be independent of one another (non-paired)\n\t\t- If they are paired, we can use other methods\n- Sample size/skew\n\t- The more skew in the population distibution, the larger the sample size we need\n\n**Example**\n\nComing back to our example,\n\n$$\\left(\u0008ar{x}_{wd} - \u0008ar{x}_{wod}\right) \\pm t^*_{df}\times\\sqrt{\u000crac{s_{wd}^2}{n_{wd}} + \u000crac{s_{wod}^2}{n_{wod}}} = \\left(52.1 - 27.1\right) \\pm 2.08 \times \\sqrt{\u000crac{45.1^2}{22} + \u000crac{26.4^2}{22}} = \\left(1.83, 48.17\right)$$\n\nSo we are 95% confident that those who eat with distractions consume 1.83 g and 48.17 g *more* snacks than those who eat without distractions, on average.\n\n**Hypothesis tests**\n\nDo these data provide convincing evidence of a difference in post-meal consumption between the two groups?\n\n- $H_0: \\mu_{wd} - \\mu_{wod} = 0, H_A: \\mu_{wd} - \\mu_{wod} \neq 0$\n\n- $T_{21} = \u000crac{25 - 0}{11.24} = 2.24$; 25 is the difference between the two means, and 11.24 is the previously calculated SE\n\n- The corresponding p-value is:\n\n```r\npt(-2.24, 21) * 2\n#\u003e 0.0360369621567888\n```\n\nHence, there is strong evidence in favour of the alternative hypothesis that there is a difference between the two groups. Note this result agrees with our confidence interval which does not include 0.\n\n## Inference for comparing two paired means\n\nWe discuss here methods for dealing with two means that are dependent on one another, this is especially useful for studies that take multiple measurements over the same group of people. We could also design studies as paired if we were studying groups that we suspect are not independent, such as twins or partners. We'll start with an example:\n\n![[Pasted image 20230712221306.png]]\n\n- The median writing score is higher than the median reading score\n- Both distributions seem fairly symmetric, but the reading score is slightly more right skewed (the median is closer to the left side of the distribution)\n\nCan a student's reading and writing score be assumed independent of one another? Likely not, as these are adjacent skillsets.\n\n- When two sets of datasets have this special correspondence, they are said to be *paired*.\n- When looking at paired distributions, it's often useful to look at the difference between the two: diff = read - write\n\nHere, the parameter of interest is $\\mu_{diff}$, the average difference between the reading and writing scores of *all* high school students.\n\n- Since we don't have this, we'll estimate the average difference between the reading and writing scores of the sampled high school students: $\u0008ar{x}_{diff}$\n\nFor the hypothesis test - we're now doing a hypothesis test on a *single* population mean:\n\n- $H_0: \\mu_{diff} = 0, H_A: \\mu_{diff} \neq 0$\n- We're also told the following: $\u0008ar{x}_{diff} = -0.545, s_{diff} = 8.887, n_{diff} = 200$\n- So $T = \u000crac{-0.545 - 0}{\u000crac{8.887}{\\sqrt{200}}} = -0.87$; $df = 200 - 1 = 199$\n\n```r\npt(-0.87, 199) * 2\n#\u003e 0.385348599393971\n```\n\nSo the probability is 38.5% to obtain a random sample of 200 students where the average difference between reading and writing is at least 0.545, if there truly is no average difference between reading and writing scores.\n\n## Power\n\nRecall that the power of the test is the probability of correctly rejecting $H_0$ when it is indeed false. Recall also that there is a trade-off between $\u0007lpha$ (the probability of committing a Type I error) and $\u0008eta$ (probability of Type II error), and that one way to decrease both is by raising the sample size.\n\nTwo competing considerations when designing experiments:\n\n- We want to detect enough data so that we can detect important effects\n- But collecting data is expensive\n\nIn this lecture, we'll look at the example of designing a medicine study so that we have an 80% chance of detecting a practically significant effect (i.e. we have 80% power - a commonly used power cut-off).\n\n#### Example\n\nSuppose a pharmaceutical company has developed a new drug for lowering blood pressure and they are preparing a clinical trial to test the drug's effectiveness. They recruit people who are taking a particular standard blood pressure medication, and half of the subjects are given the new drug, this is the treatment group. And the other half continued to take their medication through generic-looking pills to ensure blinding, this is our control group. What are the hypotheses for a two-sided hypothesis test in this context?\n\n- $\\mu$ will be the average blood pressure in any one of our groups.\n\t- $H_0: \\mu_{treatment} - \\mu_{control} = 0, H_A: \\mu_{treatment} - \\mu_{control} \neq 0$\n\nSuppose researchers would like to run this clinical trial on patients with systolic blood pressures between 140 and 180 millimeter of mercury. Suppose previously published studies suggest that the standard deviation of the patients' blood pressures will be about 12 millimeters of mercury and that the distribution of patients' blood pressures will be approximately symmetric.\n\nIf we had 100 patients per group, what would be the approximate standard error for difference in sample means of the treatment and control groups?\n\n- This is a test of difference between two independent groups, therefore the standard error is defined as:\n\n$$SE = \\sqrt{\u000crac{12^2}{100} + \u000crac{12^2}{100}} = 1.70 mmHg$$\n\nFor what values of the difference between the observed average of blood pressure in treatment and control groups would we reject the null hypothesis at the 5% level?\n\n```r\nqnorm(0.025, 0, 1.7); qnorm(0.975, 0, 1.7)\n#\u003e -3.33193877371809\n#\u003e 3.33193877371809\n\npnorm(-0.2)\n#\u003e 0.420740290560897\n```\n\n- Any difference between the means outside of this range would cause a rejection of the null hypothesis.\n\n**Power from first principles**\n\nSuppose that the company researchers care about finding any effect on blood pressure that is 3 millimeters of mercury or larger versus the standard medication. What is the power of the test that can detect this effect?\n\n- 3mmHg is the minimum effect size of interest\n\n- Power is the probability that we will reject the null hypothesis, conditional on the alternative hypothesis being true\n\t- If the alternative hypothesis were true, the true distribution of differences will be normally distributed (as previously given to us about the original distribution) around the -3 difference\n\t- We also know that we will reject the null hypothesis if the difference were at least -3.33\n\nWe have enough information to calculate the probability now: \n$$Z = \u000crac{-3.33 - (-3)}{1.70} = -0.20 \to P(Z \u003c -0.20) = \\dots$$\n\n```r\npnorm(-0.20)\n#\u003e 0.420740290560897\n```\n\n![[Pasted image 20230712221400.png]]\n\nSo the power of this test is around 42% when the minimum effect size is -3 and the group has a sample size of 100. This is clearly lower than the desired 80% power.\n\n**Deriving the sample size from a given power level**\n\n- Note that the minimum effect size remains at -3, but the standard error will change since this varies with the sample size\n\n- Now that we know the power of the test is equivalent to the probability to the left of the critical value that would cause the null hypothesis to be rejected\n\t- So we will revisit the distribution of differences if the true difference were -3, calculate the Z-score where the area to the left of the distribution is 0.8, then see where that sits on the null hypothesis distribution.\n\nThe Z-score that marks the 80th percentile of the normal curve is 0.84:\n\n```r\nqnorm(0.8)\n#\u003e 0.841621233572914\n```\n\nSo the distance between the centre of the alternative hypothesis distribution and the cutoff region is $0.84 * SE$, where the $SE$ is still unknown.\n\n![[Pasted image 20230712221425.png]]\n\nAnother piece of information is that we know the cutoff region is 1.96SE from the mean of the null hypothesis distribution, so 0 - 1.96SE. What we're about to do assumes that the distribution between the alternative distribution and the null distribution are the same, and this would be true if the drug reduced the average blood pressure, but did not change its overall variability.\n\nFrom this, we know now that the distance between 3mmHg is spanned by 0.84 + 1.96 standard errors, and we can solve for $n$:\n\n![[Pasted image 20230712221435.png]]\n\n(Instructor really got in the way here)\n\nOne final point is that, we know that power increases as sample sizes does, but there are diminishing returns past a certain point. The range of powers that correspond to the following sample sizes are charted as follows:\n\n![[Pasted image 20230712221441.png]]\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/inferential-statistics/6-ANOVA-and-Bootstrapping":{"title":"6 ANOVA and Bootstrapping","content":"#course_coursera-inferential-stats #statistics\n## Comparing more than two means\n\nThe data we'll consider here comes from the General Social Survey. We're interested in the average vocabulary score split by self-identified social class:\n\n![[Pasted image 20230712222908.png]]\n\nWe can use Analysis of Variance (ANOVA) to analyse 3+ means (whereas the [[5 t-distribution and Comparing Two Means|t-distribution]] only works for comparing two means).\n\n- Note that, as with the t-test, comparison groups with means that are further apart and with tighter variances will be more likely to be found as statistically significantly different from the other means.\n\n- ANOVA works with a new test statistic called $F$, which is accompanied by the F-distribution\n\n**ANOVA**\n\n- As with in a t-test, ANOVA compares the means of multiple groups to see if they're so far apart that their difference cannot be reasonably attributed to sampling variability.\n\n- $H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_k, H_A = \text{At least one pair of means are different to each other}$\n\t- Where $k$ is the number of groups\n\n- As with the t-statistic (a ratio of size of effect to standard error), the F-statistic is a ratio of the variability between groups over the variability within groups.\n\t- $F = \u000crac{\text{variability between groups}}{\text{variability within groups}}$\n\t- Recall that large test statistics lead to small p-values, which means you're more likely to find that there is at least one difference between a pair of means. Naturally, this means that you have a higher variability between groups relative to the variability within groups.\n\n- The F-distribution is right skewed and always positive (as its a ratio of two variances, which can never be negative)\n\n![[Pasted image 20230712222919.png]]\n\n## ANOVA\n\nANOVA works on the idea of *variability partitioning*, which is the act of taking the overall variance and splitting it into:\n\n- Variance caused by the independent variable (i.e. the between group variability)\n- Variance caused by all other factors (i.e. the within group variability)\n\n![[Pasted image 20230712222930.png]]\n\n### Breaking down ANOVA output\n\nHere's a look at what an ANOVA output table looks like; we'll go through each component step by step.\n\n- The first row is about between group variability, the second is about within group variability.\n- The third row displays the totals.\n\n![[Pasted image 20230712222939.png]]\n\n#### Sum of Squares column\n\n*Sum of Squares Total (SST)*\n\nThe last value in this column is Sum of Squares Total (SST), and it measures the total variability in the response variable. It is calculated in a similar manner to variance, but without adjusting for sample size. In other words, this is the sum of squared deviation from the mean in the response variable.\n\n$$SST = \\sum^n_{i=1}\\left(y_i-\u0008ar{y}\right)^2$$\n\nWhere $y_i$ represents each observation in the dataset, and $\u0008ar{y}$ is the grand mean across all observations.\n\n*Sum of Squares between Groups (SSG)*\n\nThe first value in this column is called the Sum of Squares between Groups (SSG), and it measures the variability in the response variable that is explained by the independent variable.\n\nThis is calculated as the sum of deviations of group means from the overall mean, weighted by the sample size of each group:\n\n$$SSG = \\sum^k_{j=1}n_j\\left(\u0008ar{y}_j-\u0008ar{y}\right)$$\n\nWhere $n_j$ is the number of observations in group $j$, $\u0008ar{y}_j$ is the mean of the response variable in group $j$, and $\u0008ar{y}$ is the grand mean across all observations.\n\nFor this example, the SSG is about 7.6% of the SST; i.e. 7.6% of total variability in the vocabulary scores is explained by difference in social class, and the remainder is explained by other factors.\n\n*Sum of Squares Error (SSE)*\n\nThis is the second value in the Sum of Square column, and it measures the unexplained variability due to all other variables. The simplest way to calculate this is:\n\n$$SSE = SST - SSG$$\n\n#### Mean Square Values column\n\nTo get from the Sum of Squares column to the Mean Square Values column, we need to scale the sum of squares by a measure that takes into account both:\n\n- Sample size; and,\n- Number of groups\n\nWe can use degrees of freedom to represent this; the degrees of freedom associated with ANOVA are:\n\n- Total: $df_T=n-1$\n- Group: $df_G=k-1$\n- Error: $df_E=df_T-df_G$\n\nNext, the Mean Squares are calculated as follows:\n\n- Group: $MSG=SSG/df_G$\n- Error: $MSE=SSE/df_E$\n\n#### F-statistic\n\nThe F-statistic is the ratio between the average between and within group variabilities:\n\n$$F = \u000crac{MSG}{MSE}$$\n\n#### P-value\n\nWe can finally calculate our p-value which may be interpreted as the probability of observing our ratio of between and within group variabilities if fact the means of all groups are equal. A few things to note:\n\n- The F-distribution uses two degrees of freedom: degrees of freedom between groups ($df_G$), and degrees of freedom of errors ($df_E$)\n\n- The F-statistic can **never be negative** and hence we only consider the right side of the distribution\n\t- As a result of this, a more extreme statistic will always be more extreme in the positive direction\n\n```r\npf(21.735, 3, 791, lower.tail = FALSE)\n#\u003e 1.55985531902116e-13\n```\n\n### Conclusion\n\nAs with previous [[3 Hypothesis Testing|hypothesis tests]]:\n\n- If the p-value is small/less than $\u0007lpha$, we reject the null hypothesis and conclude that there is evidence that at least one pair of means is different (although we can't tell which one at this stage).\n- Otherwise if the p-value is larger than $\u0007lpha$, we do not reject the null hypothesis and conclude that there is no evidence that any pair of means is statistically significantly different from the other.\n\n## Conditions for ANOVA\n\nThere are three main conditions that need to be met for ANOVA to be valid:\n\n1. Independence\n\t- Within groups: sampled observations must be independent\n\t- Between groups: the groups must be independent of each other (i.e. non-paired)\n\n2. Approximate normality: each group must be approximately normal\n\n3. Equal variance: groups should have roughly equal variance\n\n### Independence\n\n*Independence within groups*\n\nWe can be confidence that this is the case if we have random sampling/assignment, and if each sample size is less than 10% of the respective population.\n\n*Independence between groups*\n\nGroups must be independent of another, and this requires careful consideration of the results to ensure there is no paired structure that is being caused by the study design.\n\n- Not the end of the world if this isn't met, there is a method called Repeated Measures ANOVA that can deal with this case.\n\n### Approximately normal\n\nEspecially important when the sample sizes are small, but will also be harder to check if that's the case. We can use normal probability plots within each group to check this assumption.\n\n### Constant/equal variance\n\ni.e. homoskedastic groups. Especially important if sample sizes vary between groups. Can check this using side-by-side box-plots and by calculating standard deviation within each group.\n\n## Multiple comparisons\n\nFinding a statistically significant result with ANOVA only tells us that one pair of means is different - but not which pairs differ. The method for determining which pair of means differs is simply to perform many pair-wise t-tests between each group's means. This is referred to as 'multiple comparisons'. Note, to perform these tests, we'll need to reconsider our degrees of freedom and standard error; as well as our significance level.\n\nRecall that the Type I error rate is the probability that you will commit a Type I error; performing many pair-wise tests simultaneously inflates your Type I error rate which is undesirable.\n\nThe solution to this is simple, use a modified, more conservative significance level. This is achieved by using the *Bonferroni correction*, which adjusts $\u0007lpha$ by the number of comparisons being considered.\n\n**Bonferroni correction**\n\nThe adjusted significance level is defined as:\n\n$$\u0007lpha^* = \u0007lpha/K$$\n\nWhere $K = \u000crac{k\\left(k-1\right)}{2}$ is the number of comparisons that will take place in your multiple comparisons process ($k$ being the number of means you have on hand to compare).\n\nBack to our example, we have four means so $k = 4$, and our initial $\u0007lpha$ is $0.05$. For this example, $K = 6$, and $\u0007lpha^* = 0.05 / 6 \u0007pprox 0.0083$\n\n**Standard error for multiple pairwise comparisons**\n\n$$SE = \\sqrt{\u000crac{MSE}{n_1} + \u000crac{MSE}{n_2}}$$\n\nThis is similar to the independent groups t-test, however the numerators are using the *mean squared error from the ANOVA output* instead of the individual sample standard deviations.\n\nRecall that the mean squared error is essentially the average within group variance, so we're still getting at the same concept as when we were using the individual variances. If the constant variance assumption is satisfied, this measure will be very close to the sample standard deviations anyway.\n\n**Degrees of freedom for multiple pairwise comparisons**\n\n$$df = df_{E}$$\nInstead of $min\\left(n_1-1, n_2-1\right)$ in the independent means t-test, this is the degrees of freedom for the error in the ANOVA output.\n\n#### Example\n\nLet's pick a pair and compare the means of lower and middle class vocabulary scores.\n\n- Lower class ~ n: 41, mean: 5.07\n- Middle class ~ n: 331, mean: 6.76\n\nOn to the hypothesis test:\n\n- $H_0: \\mu_{middle} - \\mu_{lower} = 0, H_A: \\mu_{middle} - \\mu_{lower} \neq 0$\n\n- $T = \u000crac{\\left(\u0008ar{x}_{middle} - \u0008ar{x}_{lower}\right) - 0}{\\sqrt{\u000crac{MSE}{n_{middle}} + \u000crac{MSE}{n_{lower}}}} = \u000crac{6.76 - 5.07}{\\sqrt{\u000crac{3.628}{41} + \u000crac{3.628}{331}}} = 5.365$\n\n- $df = 791$ (from the table at the start of this article)\n\n```r\npt(-5.365, 791) * 2\n#\u003e 1.06389466632633e-07\n```\n\n1.06e-07 \u003c 0.0083, hence we reject the null hypothesis and note there is strong evidence that there is a statistically significant difference between the average vocabulary scores of lower and middle class Americans.\n\n## Bootstrapping\n\nSay we take a random sample of 20 rental properties in Durham and want to estimate the population median (and construct a confidence interval for it).\n\n![[Pasted image 20230712222959.png]]\n\nThis is a small sample and the sample distribution looks fairly skewed, so CLT-based methods would not be reliable here. This is where we draw on simulation-based methods such as bootstrapping.\n\nIn bootstrapping, we assume that each observation in the sample represents others in the population at large. This means that the bootstrap population is made up of the same observations that are in your sample, repeated an arbitrary number of times.\n\nWe don't actually create the bootstrap population, but instead simulate it by drawing from the original sample multiple times *with replacement*. The overall bootstrapping methodology is:\n\n- Randomly sample from your original sample with replacement, until you have the same number of observations as in your original sample\n\n- Calculate your sample/bootstrap statistic with the bootstrapped sample\n\n- Repeat the previous two steps as many times as you need to create a bootstrap distribution (which is a sampling distribution, just using bootstrapped samples instead of samples drawn from the population)\n\nOnce we have the bootstrap distribution, we can calculate confidence intervals in 2 ways:\n\n1. Percentile method: estimate a 95% (for example) confidence interval as the middle 95% of the distribution - the bounds of which would be the 2.5th and 97.5th percentiles of the bootstrap distribution.\n\n2. Standard error method (ostensibly more accurate): estimate the interval as $\text{sample statistic} \\pm \\space t^*_{df=n-1} \times SE_{boot}$, where $n$ is the original sample size.\n\n**Example**\n\nThis dot plot shows the distribution of medians of 100 bootstrap samples from the original sample. We want to estimate the 90% bootstrap confidence interval for the median rent, based on this bootstrap distribution, using the standard error method.\n\n![[Pasted image 20230712223008.png]]\n\nFirst, to get our t-statistic:\n\n```r\nqt(0.05, 20 - 1, lower.tail = FALSE)\n#\u003e 1.72913281152137\n```\n\nThen, given that our sample median is 887, we get:\n\n$$\text{sample median} \\pm t^*_{19} \times SE_{boot} = 887 \\pm 1.73 \times 89.5758 = (732.1, 1041.9)$$\n\nWe're 90% confident that the population median of rental prices in Durham falls between these bounds.\n\n#### Percentile vs. Standard Error methods\n\nWe can see that the bounds using either method are close to each other - but are not exactly the same in this case.\n\n![[Pasted image 20230712223017.png]]\n\n#### Closing remarks\n\n- Simulation methods do not have as rigorous assumptions as CLT-based methods, however a larger sample size will yield more reliable results as usual.\n\n- If the bootstrap distribution is extremely skewed or sparse, the bootstrap interval might be unreliable.\n\n- A representative sample is still required - garbage in, garbage out.\n\n- Bootstrap vs sampling distributions\n\t- Sampling distributions are created by taking random samples with replacement from the population\n\t- Bootstrapping distributions are created by taking random samples with replacement from the sample\n\t- Both are distributions of sample statistics\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/inferential-statistics/7-Inference-for-Proportions":{"title":"7 Inference for Proportions","content":"#course_coursera-inferential-stats #statistics \n## Introduction\n\nThis week we discuss categorical variables where the metric of interest is a proportion (e.g. proportion of elderly patients taking antihypertensive medications that experience falls).\n\n- We'll start off by looking at one categorical variable at a time, focusing on binary categorical variables first\n    - Then we'll expand to one categorical variable with multiple levels (e.g. socioeconomic status = low/medium/high)\n\n- Then we'll move to looking at two categorical variables where both are binary (e.g. does gender influence whether or not someone goes into a scientific field?)\n    - Finally we'll look at the case where there are two categorical variables and both have multiple levels (e.g. does socioeconomic status affect education attainment?)\n\n## Sampling variability and CLT for proportions\n\nThis lesson begins by setting up the premise of sampling distributions just as the first lesson did; however in this case, within each sample we are calculating a proportion based on a categorical variable (e.g. x% of the sample are smokers) - this is just another sample statistic.\n\n- Your sampling distribution will be made up of sample proportions; and,\n- Given theÂ [[#Conditions for CLT for proportions|CLT for proportions assumptions]]Â are met, the sampling distribution for the proportion will be approximately normal:\n\n$$\\hat{p} \\sim N\\left(\text{mean} = p, SE = \\sqrt{\u000crac{p(1-p)}{n}}\right)$$\n\n### Conditions for CLT for proportions\n\nThese are similar to the conditions for the normal CLT:\n\n1. Independence\n\t- Random sampling/assignment\n\t- If sampling without replacement, n \u003c 10% of the population\n\n2. Sample Size/Skew\n\t- There should be at least 10 successes and 10 failures in each sample, i.e. $np \\ge 10$ \u0026 $n(1-p) \\ge 10$ (if $p$ is unknown, use $\\hat{p}$)\n\n#### What if the Sample Size/Skew assumption is not met?\n\n- The center of the sampling distribution will still be around the true population proportion\n\n- The standard error will still adhere to the equation shown earlier\n\nHowever, the shape of the distribution will depend on whether the true population proportion is closer to 0 or 1.\n\n- Notice that a failure to meet the Sample Size/Skew assumption may be seen as due to having a sample size that is too small\n\n- This means that standard error will be high, and the sampling distribution will be spread much more loosely around the mean\n\n- Since proportions are bounded at 0 and 1, a true population proportion close to either side will yield a distribution that is cut off - a low $p$ will exhibit a right-skewed distribution, a high $p$ will exhibit a left-skewed distribution\n\n![[Pasted image 20230712224406.png]]\n\nIf the sample size is high enough, the distribution will be tighter around the population proportion, making the skew less of an issue.\n\n**Example**\n\nWe're told that 90% of all plant species are classified as angiosperms. If you were to randomly sample 200 plants from the list of all known plant species, what is the probability that at least 95% of the plants in your sample will be flowering plants?\n\nFirst we check assumptions:\n\n- Observations are randomly sampled, and 200 \u003c 10% of the total population of all plants\n- $200 \times 0.9 \\ge 10$ \u0026 $200 \times 0.1 \\ge 10$\n\nWe know that:\n\n- $n = 200$\n- $p = 0.9$\n- Hence, $\\hat{p} \\sim N\\left(0.9, \\sqrt{\u000crac{0.9 \times 0.1}{200}} \u0007pprox 0.0212\right)$\n\nAnd we want to find $P(\\hat{p} \u003e 0.95)$.\n\n- The Z-score of 0.95 is $\u000crac{0.95 - 0.9}{0.0212} \u0007pprox 2.36$, which is a fairly high Z-score, and soâ€¦\n\n```r\npnorm(2.36, lower.tail = FALSE)\n#\u003e 0.00913746753057268\n```\n\n## Confidence interval for a proportion\n\n**Confidence Interval Example**\n\nWe are shown that 571 of 670 (~85%) of sampled Americans have good intuition for experimental design. What proportion of all Americans have good intuition for experimental design?\n\nNote that:\n\n- We want to estimate $p$, the proportion of all Americans that have good intuition for experimental design; using,\n\n- $\\hat{p}$, the proportion of sampled Americans that have good intuition for experimental design.\n\nNote also that this scenario meets the assumptions for CLT, with a randomly sampled sample that is less than 10% of all Americans, and with the number of successes (571) and failures (99) being \u0026ge; 10.\n\nThe formula for a confidence interval around a proportion estimate is:\n$$\text{proportion point estimate} \\pm z^* \times SE_{\\hat{p}} = \text{proportion point estimate} \\pm z^* \times \\sqrt{\u000crac{\\hat{p}(1-\\hat{p})}{n}} = 0.85 \\pm 1.96\\sqrt{\u000crac{0.85\times0.15}{670}} = (0.823, 0.877)$$\nSo we are 95% confident that 82.3% to 87.7% of all Americans have good intuition for experimental design.\n\n**Margin of Error Example**\n\nThe margin of error for the previous example was 2.7%, say we want to decrease this to 1% while keeping the confidence level constant. How large of a sample will we need?\n\n$$ME = z^*SE_{\\hat{p}} \\Rightarrow 0.01 = 1.96\times\\sqrt{\u000crac{0.85 \times 0.15}{n}} \\Rightarrow n = 4898.04$$\n\nSo we need at least `ceiling(4898.04) =` 4,899 people in our sample to get a 1% margin of error.\n\n**Closing Remarks about Margin of Error**\n\n- Note that if there has been a previously estimated $\\hat{p}$, we can use that in our standard error calculation as seen above.\n- However, if we have no idea what $\\hat{p}$ might be, we use $0.5$ instead.\n\t- This is because $\\hat{p} = 0.5$ is the most conservative estimate and would lead to the largest sample size required\n\t- It is also not a bad guess if the outcome were binary (dubious)\n\n## Hypothesis test for a proportion\n\nRecall that the steps to complete a hypothesis test are as follows:\n\n1. Assert the two hypotheses: $H_0$ and $H_A$\n2. Calculate the point estimate: $\\hat{p}$\n3. Check independence and sample size/skew conditions\n4. Draw sampling distribution, shade p-value, calculate the Z-score noting that $p$ will be the value in the null hypothesis\n5. Make a decision and interpret it in the context of your scenario - if p-value is \u0026le; $\u0007lpha$, then the data provides convincing evidence for $H_A$\n\nTo clarify the usage of $p$ vs. $\\hat{p}$; we use the sample proportion when there is nothing else known (i.e. in confidence intervals), but we use the hypothesised population proportion when doing hypothesis tests.\n\n![[Pasted image 20230712224422.png]]\n\nWhy are we just talking about this now when we went through 3 weeks of performing inference for means? This is because we had no need for a null value for $\\mu$ as the standard error equation for a mean did not include the mean itself. For proportions, this is not the case - you need a null value for proportions to calculate the corresponding standard error. This will also come into play for when we perform inference for the difference between two proportions; most dramatically when attempting hypothesis tests (since that requires a null value to be asserted).\n\n**Example**\n\nA 2013 Pew Research poll found that 60% of 1,983 randomly sampled American adults believe in evolution. Does this provide convincing evidence that majority of Americans believe in evolution?\n\n1. $H_0: p = 0.5$, $H_A: p \u003e 0.5$\n2. $\\hat{p} = 0.6$\n3. Sample is randomly picked and is smaller than 10% of the American adult population. Successes and failures are both \u0026ge; 10. CLT assumptions are met.\n4. $Z = \u000crac{\\hat{p} - p}{\\sqrt{\u000crac{p(1-p)}{n}}} = \u000crac{0.6-0.5}{0.0122} \u0007pprox 8.92$, we know this is a high Z-score and hence will have a very small p-value.\n\n```r\npnorm(8.92, lower.tail = FALSE)\n#\u003e 2.33144105408587e-19\n```\n\n1. Hence, the data provide convincing evidence for the alternative hypothesis, which is that the majority of Americans believe in evolution.\n\nThis p-value is interpreted as that there is almost 0% chance of obtaining a random sample of 1,983 Americans where 60% or more believe in evolution, if in fact 50% of Americans believe in evolution.\n\n## Estimating the difference between two proportions\n\n**Example**\n\nWe'll be comparing a Gallup poll (U.S. only) against a Coursera poll (international) where participants are asked whether or not they believe there should be laws restricting the ability to possess handguns. The results are shown below:\n\n| Sample | 'Yes' | n | $\\hat{p}$ |\n| :- | -: | -: | -: |\n| Gallup | 275 | 1028 | 0.25 |\n| Coursera | 59 | 83 | 0.71 |\n\nHow do Coursera students and the American public at large compare with respect to their views on laws banning possession of handguns? Use a 95% confidence level.\n\n- The parameter of interest we are estimating is $p_{Coursera} - p_{US}$, the difference between the proportions of all Coursera users and all Americans who believe there should be a ban on handguns.\n\n- The point estimate is $\\hat{p}_{Coursera} - \\hat{p}_{US}$, the difference between the proportions of sampled Coursera users and sampled Americans who believe there should be a ban on handguns.\n\nBefore we dive in, let's check that the assumptions hold:\n\n- Independence\n\t- Within groups: sampled observations must be independent of each other\n\t\t- Gallup - randomly sampled, and n \u0026le; 10% of the American population\n\t\t- Coursera - a voluntary poll, so not randomly sampled. Must be careful about generalising to the total Coursera student population.\n\t- Between groups: the two groups must be independent of each other (i.e. non-paired)\n\t\t- This is met here.\n\n- Sample Size/Skew: each sample must meet the success-failure condition\n\t- $n_1p_1 \\ge 10$ \u0026 $n_1(1-p_1) \\ge 10$\n\t- $n_2p_2 \\ge 10$ \u0026 $n_2(1-p_2) \\ge 10$\n\t\t- This is met in both groups\n\nSince these assumptions are mostly met, we can assume that the difference between the proportions is approximately normally distributed.\n\nThe formula for estimating the confidence interval for a difference in proportions is as follows:\n\n$$\text{point estimate} \\pm \text{margin of error} \\Rightarrow \\left(\\hat{p}_1 - \\hat{p}_2\right) \\pm z^*SE_{\\hat{p}_1 - \\hat{p}_2}, \text{where } SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\u000crac{\\hat{p}_1(1 - \\hat{p}_1)}{n_1} + \u000crac{\\hat{p}_2(1 - \\hat{p}_2)}{n_2}}$$\n$$\therefore \\left(\\hat{p}_{Coursera} - \\hat{p}_{US}\right) \\pm z^*SE_{\\hat{p}_{Coursera} - \\hat{p}_{US}} \\Rightarrow (0.71 - 0.25) \\pm 1.96 \times \\sqrt{\u000crac{0.71\times0.29}{83} + \u000crac{0.25\times0.75}{1028}} = (0.36, 0.56)$$\n\nNote that if we were asked to perform a hypothesis test on this same scenario, we would end up rejecting the null hypothesis (that posits there is no difference between the two proportions) since this confidence interval does not include 0.\n\n#### Does the order matter?\n\nWhat would happen if we switch the order of the Coursera proportion vs. the US proportion? Let's break down the confidence interval equation:\n\n- $\\left(\\hat{p}_1 - \\hat{p}_2\right)$ can be either positive or negative\n- $z^*$ must always be positive\n- $SE_{\\hat{p}_1 - \\hat{p}_2}$ must always be positive - hence the margin of error will always be positive\n\nSo if we switch the order, the overall conclusion will be the same - just that the order of comparison will be different.\n\n![[Pasted image 20230712224441.png]]\n\nSo for the above:\n\n- In the first case we can interpret this as we are 95% confident that the Coursera proportion is 36-56% higher than the US proportion.\n- In the second, we are 95% confident that the US proportion is 36-56% lower than the Coursera proportion which is the converse and therefore still correct and equivalent.\n\n## Hypothesis test for comparing two proportions\n\nRecall that for confidence intervals, we estimate bounds using the sample proportion (and we get these from our samples). For hypothesis tests, we use the null value for the proportion in our calculations (since that's assumed to be true). However, for the difference between proportions, there *is no null value asserted*; the only assertion is that $p_1 = p_2$.\n\nHence, for our calculations when performing hypothesis tests on the difference between two proportions, we need to use the concept of a *pooled proportion*.\n\n$$H_0: p_1 = p_2 = ? \\Rightarrow \\hat{p}_{pool} = \u000crac{\text{total successes}}{\text{total n}} = \u000crac{\text{num of successes}_1 + \text{num of successes}_2}{n_1 + n_2}$$\n\nTo recap, when working with two proportions:\n\n![[Pasted image 20230712224207.png]]\n\n**Example**\n\nConsider a study in which parents are asked whether or not their kids have been bullied at school. Only one parent can answer of the pair of them. There are some reasons why the proportion of bullied kids may vary by the parent that answers the survey:\n\n- Same-sex parents may slightly skew the results\n- Male vs. female parents may differ in their likelihood to be aware of bullying or to even be told by their children\n\nThe study results are as follows:\n\n| Result | Male | Female |\n| :- | -:| -: |\n| Yes | 34 | 61 |\n| No | 52 | 61 |\n| Not sure | 4 | 0 |\n| Total | 90 | 122 |\n| $\\hat{p}$ | 0.38 | 0.5|\n\nSo:\n\n- $\\hat{p}_{pool} = \u000crac{34 + 61}{90 + 122} \u0007pprox 0.45$\n\nChecking assumptions before we dive into the hypothesis test\n\n- Independence is met\n\t- Within groups: All sampled individuals are independent of each other\n\t- Between groups: No reason to suspect that the samples are paired (only one partner can answer per parent pair)\n\n- Sample Size/Skew is met (note that we use pooled proportions here)\n\t- Males: $90 \times 0.45 = 40.5$ \u0026 $90 \times 0.55 = 49.5$\n\t- Females: $122 \times 0.45 = 54.9$ \u0026 $122 \times 0.55 = 67.1$\n\nConditions for CLT are met, so $\\left(\\hat{p}_{male} - \\hat{p}_{female}\right) \\sim N\\left(0, SE = \\sqrt{\u000crac{0.45\times0.55}{90} + \u000crac{0.45\times0.55}{122}}\u0007pprox0.0691\right)$\n\nWe can now perform our hypothesis test\n\n1. $H_0: p_{male} - p_{female} = 0, H_A: p_{male} - p_{female} \neq 0$\n2. $\\hat{p}_{male} - \\hat{p}_{female} = -0.12$\n3. As noted earlier, CLT assumptions are met\n4. $Z = \u000crac{\\left(\\hat{p}_{male} - \\hat{p}_{female}\right) - 0}{SE_{\\hat{p}_{male} - \\hat{p}_{female}}} = \u000crac{-0.12 - 0}{0.0691} \u0007pprox -1.74$, and noting this is a two-tailed hypothesis testâ€¦\n\n```r\npnorm(-1.74) * 2\n#\u003e 0.0818590179576147\n```\n\n1. Based on this p-value, we can conclude that there is evidence that there is no difference in males and females with respect to likelihood of reporting their kids being bullied.\n","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/inferential-statistics/8-Simulation-based-inference-for-proportions-and-chi-square-testing":{"title":"8 Simulation based inference for proportions and chi-square testing","content":"#course_coursera-inferential-stats #statistics \n## Small sample proportions\n\nIf the success-failure â‰¥ 10 rule (i.e. the Sample Size/Skew condition) is not satisfied, then the sampling distribution for your given scenario will not adhere to the CLT, reducing the effectiveness of CLT-based methods.\n\nHere, we turn to simulation-based methods such as bootstrapping.\n\n**Example**\n\nPaul the Octopus once picked the winner of all 8 World Cup games in a year correctly. The 'experiment' design was to give Paul the choice of eating out of one box or another, where each box represented the flag of the competing teams that day.\n\nOur hypothesis test will aim to determine whether Paul is psychic, or if he picked all games right just by chance.\n\n$$H_0: p = 0.5, H_A: p \u003e 0.5, n = 8, \\hat{p} = 1$$\n\nChecking to see if the conditions for CLT inference are met:\n\n- Independence\n\t- We can assume that Paul's picks each day are independent from each other for now\n\n- Sample Size/Skew\n\t- $0.5 \times 8 = 4$ which does not meet this condition.\n\nHence, CLT assumptions are not met and so CLT-based methods do not apply here. Instead, let's use simulation-based methods.\n\n- The p-value for this hypothesis test is $P(\text{observed or more extreme outcome}\\space|\\space H_0 \text{ is true}) \\Rightarrow P(\\hat{p} \\ge 1.0 \\space|\\space p = 0.5)$\n\n- So what we must do is devise a simulation scheme that can replicate Paul's trials, and perform these simulations under the assumption that the null hypothesis is true\n\n- Repeat this simulation N times, recording the relevant sample statistic each time\n\n- Calculate the p-value as the proportion of simulations that yielded sample statistics that were favourable to the alternative hypothesis\n\nTo do this, we must simulate Paul's trials. Since he is picking between two boxes, with an assumed null probability of 50% chance for each box, we can approximate these trials by flipping a coin 8 times per simulation (flipping a coin is used as an example since it is conceptually equivalent to Paul's trial, no other significance here).\n\nBelow we:\n\n- Flip a coin 8 times each simulation, naming 'heads' a success and binding this to 1 (and tails to 0)\n\n- The proportion of heads in each 8 flips is then $\\hat{p}_i$, for the $i$th simulation; we repeat this N = 100,000 times\n\n- These 100,000 $\\hat{p}_i$s form the sampling/bootstrap distribution, see the histogram of results below\n\n- The p-value for this test will then be $P(\\hat{p} \\ge 1.0 \\space|\\space p = 0.5) \\Rightarrow \u000crac{\text{num. simulations where }\\hat{p} \\ge 1}{N} \u0007pprox 0.00390$\n\n```r\nsuppressPackageStartupMessages(library(dplyr));\nsuppressPackageStartupMessages(library(ggplot2))\n\noptions(repr.plot.width = 15, repr.plot.height = 7)\n\nsimulate_binary \u003c- function(N, n = 8) {\n\n\tresults \u003c- vector('double', N)\n\t\n\tsapply(\n\t\tresults,\n\t\tfunction(z) sample(c(0, 1), n, replace = T) |\u003e mean()\n\t)\n\t\n}\n\nresults \u003c- simulate_binary(100000)\n\ntibble(p_hat = results) |\n\tggplot(aes(x = p_hat)) +\n\tgeom_histogram(bins = 30) +\n\tlabs(\n\t\ttitle = 'Simulated p-hat for this experiment',\n\t\ty = 'Count',\n\t\tx = 'Simulated p-hat'\n\t) +\n\tscale_y_continuous(labels = scales::comma) +\n\tscale_x_continuous(labels = scales::percent)\n```\n\n![[Pasted image 20230712230158.png]]\n\n```r\nscales::percent(\n\tFilter(function(x) {x \u003e= 1}, results) |\u003e length() / length(results), \n\t\taccuracy = 0.001\n)\n#\u003e '0.390%'\n```\n\nThis is under the 5% significance level, hence we reject the null hypothesis as there is strong evidence for the alternative. Does this really mean that Paul is psychic? Probably not. We're likely making a Type I error here or possibly asking the wrong question.\n\n## Examples\n\nSuppose we recruit 12 volunteers to test the validity of the idiom \"to know it like the back of one's hand\". Each volunteer is shown 10 pictures of gloved hands and must correctly choose which picture contains their hand. Say 11 of 12 people complete this task successfully - is this sufficiently strong evidence that people truly know the backs of their hands?\n\n$$H_0: \text{volunteers are no better than randomly guessing, ie }p = \u000crac{1}{10}; H_A: p \u003e \u000crac{1}{10}$$\n\nTo devise a simulation scheme that is suitable for this experiment, we note that if the volunteers were truly randomly guessing then their true chance of picking their hand correctly is 0.1.\n\n- We can roll a 10-sided fair die to represent the sampling space, calling '1' a success (recognising one's own hand) and all other outcomes a failure\n\n- For each simulation ($i$), we roll the die 12 times (to represent the 12 original volunteers), calculating $\\hat{p}_i$ each time.\n\n- Run the simulation 100 times, creating a bootstrap distribution, then finding the proportion of simulations where the proportion is equal to or more extreme than the observed point estimate: $11/12 \u0007pprox 0.9167$.\n\nPerforming this in a similar manner to the simulation demo shown previously, we get a p-value of very close to 0 and hence we reject the null hypothesis. There is strong evidence that the volunteers are better than just randomly guessing which of the 10 pictures was their own hand.\n\n## Comparing two small sample proportions\n\nSuppose we extend the experiment we performed in the previous example to also test whether volunteers can recognise the palm side of their hands. We get the following results:\n\n| Outcome | Back | Palm | Total |\n| :- | -: | -: | -: |\n| Correct | 11 | 7 | 18 |\n| Incorrect | 1 | 5 | 6 |\n| Total | 12 | 12 | 24 |\n| $\\hat{p}$ | 0.9167 | 0.5833 | 0.7500 |\n\nDo these data provide sufficient evidence that there is a difference in how good individuals are at recognising the backs of their hands vs. recognising the palm of their hands?\n\n$H_0: p_{back} - p_{palm} = 0, H_A: p_{back} - p_{palm} \neq 0$\n\nChecking assumptions\n\n- Independence\n\t- Within groups: we can assume that the guess of each volunteer is independent from the other volunteers' guesses\n\t- Between groups: we have the same volunteers guessing across two tests - this is in fact a paired structure. For the purposes of illustration we will continue with this example though.\n\n- Sample Size/Skew\n\t- Since we're comparing between two population proportions here, we need to calculate number of successes and failures using the pooled proportion. $\\hat{p}_{pool}$ is calculated in the table above as 0.75.\n\t- $12 \times 0.75 = 9$, and $12 \times 0.25 = 3$; neither of these meet the standard of 10, so CLT assumptions are not met - use simulation methods\n\nSimulation scheme - the same principles as before apply, apart from the need to use the pooled proportion in this scheme\n\n- Set up 24 index cards - each representing a volunteer and their two guesses across the two tests\n\n- Using the pooled proportion - mark 18 cards as 'correct' and 6 cards as 'incorrect'; note that this is done *before* apportioning the cards into the two groups\n\n- Apportion the 24 cards into two groups of 12 cards, representing back and palm tests\n\n- Calculate the difference in proportion between correctness in back and palm test groups. Resimulate many times, each difference in proportion is another entry in your bootstrap distribution\n\nOnce your bootstrap distribution is constructed\n\n- The point estimate of the difference is ~33% ($0.9167 - 0.5833$), so we want to find the proportion of simulations where $|\\hat{p}| \u003e 0.33$\n\n- This comes out to around 16%, so we would fail to reject the null hypothesis and we say there isn't strong evidence to suggest that there is a difference between people recognising the palm of their hand vs. the back of it.\n\n```r\nsimulate_mythbusters \u003c- function(\n\tN, n = 12, \n\tsample_space = c(rep(1, 18), rep(0, 6))\n) {\n\n\tresults \u003c- vector('double', N)\n\t\n\tsapply(\n\t\tresults,\n\t\tfunction(x) {\n\t\t\n\t\t\tidx \u003c- sample(1:(2*n), n, replace = F)\n\t\t\n\t\t\tp_hat_back \u003c- sample_space[idx] |\u003e mean()\n\t\t\tp_hat_palm \u003c- sample_space[-idx] |\u003e mean()\n\t\t\t\n\t\t\tp_hat_back - p_hat_palm\n\t\t\n\t\t}\n\t)\n}\n\nresults_mb \u003c- simulate_mythbusters(100000)\n\ntibble(p_hat_diff = results_mb) |\u003e\n\tggplot(aes(x = p_hat_diff)) +\n\tgeom_histogram(bins = 30) +\n\tlabs(\n\t\ttitle = 'Simulated p-hat difference between back and palms for this experiment',\n\t\ty = 'Count',\n\t\tx = 'Simulated p-hat(back) - p-hat(palm)'\n\t) +\n\tscale_y_continuous(labels = scales::comma) +\n\tscale_x_continuous(labels = scales::percent)\n```\n![[Pasted image 20230712230444.png]]\n```r\npoint_est_diff \u003c- 0.9167 - 0.5833 \n\nscales::percent( \n\tFilter(function(x) {abs(x) \u003e= point_est_diff}, results_mb) |\u003e length() / length(results_mb), \n\taccuracy = 0.001 \n)\n#\u003e '1.397%'\n```\n\nThis is below the significance level so we can say we have strong evidence for the alternative hypothesis that there is indeed a difference in people's ability to pick out the back of their hand vs. their palm.\n\n*Note: the course quotes a p-value of 15.66; unclear why this is so high and far off the value we get as they seem to do a substantial number of simulations as well*\n\n## Chi-Square GOF test\n\nSuppose the court of a small county is being accused of racial discrimination, with the claim being that the jury is not being picked in line with the ethnicity seen in the population of the county. See below the true proportion of each ethnicity within the county, as well as the number of individuals from each ethnicity seen in the jury. The expected row is the true population proportion multipled by 2500 which is the number of jurors picked.\n\n| Population | White | Black | Native American | Asian | Other | Total |\n| :- | -: | -: | -: | -: | -: | -: |\n| Census | 80.29% | 12.06% | 0.79% | 2.92% | 3.94% | 100% |\n| Jury | 1920 | 347 | 19 | 84 | 130 | 100% |\n| # Expected | 2007 | 302 | 20 | 73 | 98 | 2500 |\n\nBased on hypothesis testing principles, our null hypothesis would be that there is nothing going on (i.e. the distribution of jurors matches the distribution of the population); whereas our alternative hypothesis would be that the distributions are not the same.\n\nHere we want to be able to quantify how different the observed counts are from the expected counts, large differences from the expected counts provide evidence to the alternative hypothesis\n\n- This is called a goodness-of-fit (GOF) test since we're evaluating how well the observed distribution *fits* to the expected distribution.\n- The name of this test is the Chi-Square ($","lastmodified":"2023-08-18T01:52:42.625986345Z","tags":[]},"/statistics/introduction-to-statistical-learning/1-Introduction/1.1-An-Overview-of-Statistical-Learning":{"title":"1.1 An Overview of Statistical Learning","content":"\n#textbook_intro-to-statistical-learning\n\n- Statistical learning refers to a set of tools for understanding data.\n- Can be [[2.1.4 Supervised Versus Unsupervised Learning#^31a009|supervised]] (predicting/estimating an output using inputs) or [[2.1.4 Supervised Versus Unsupervised Learning#^0ebfb2|unsupervised]] (inputs but no supervising output - all about learning relationships and structure of the data).\n\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/1-Introduction/1.2-Notation-and-Simple-Matrix-Algebra":{"title":"1.2 Notation and Simple Matrix Algebra","content":"\n#textbook_intro-to-statistical-learning\n\n**General notation**\n- $n$ denotes the number of distinct data points available\n- $p$ denotes the number of variables that are available for use\n- Variable names are referred to with coloured monospace font\n\n**Feature space**\n- $x_{ij}$ represents the value of the $j$th variable for the $i$th observation, where $i = 1, 2, \\dots, n$ and $j = 1, 2, \\dots, p$\n- $\\textbf{X}$ denotes an $n \\times p$ matrix whose $(i, j)$th element is $x_{ij}$ \n\nSkipping taking notes for the rest of this chapter for nowâ€¦","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1-What-is-Statistical-Learning":{"title":"2.1 What is Statistical Learning","content":"#textbook_intro-to-statistical-learning\n\n- For any dataset, we assume there's a relationship between $Y$ and $X = (X_1, X_2, \\dots, X_p)$ which can be represented in the general form:\n$$Y = f(x) + \u001bpsilon$$\n- Where $f$ is a fixed but unknown function of $X_1, \\dots, X_p$, and $\u001bpsilon$ is a random error term (independent of $X$ and mean 0). $f$ represents the systematic information that $X$ provides about $Y$, and is something we need to estimate based on observed points.\n- Statistical learning refers to the set of approaches for estimating $f$\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.1-Why-Estimate-f":{"title":"2.1.1 Why Estimate f","content":"#textbook_intro-to-statistical-learning\n\n- Estimating $f$ can be useful for both *prediction* (predicting the response for future observations - prediction accuracy is paramount here) and *inference* (better understanding the relationship between the inputs and the output - interpretability is the priority).\n- There is often a [[2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability|trade-off between prediction power and interpretability]] (e.g. more complex models may provide better predictions, but are harder to interpret)\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.2-How-Do-We-Estimate-f":{"title":"2.1.2 How Do We Estimate f","content":"#textbook_intro-to-statistical-learning\n\n- Techniques for estimating $f$ can be characterised as either *parametric* or *non-parametric*.\n- *Parametric* techniques require that a functional form be assumed for the data, which provides a set of parameters that one needs to estimate using the data. i.e. it reduces the problem of estimating f down to one of estimating a set of parameters.\n\t- In general, fitting a more flexible parametric model involves fitting more parameters\n- *Non-parametric* techniques do not make assumptions about the functional form of $f$. However, since they don't reduce the problem down to that of fitting a finite set of parameters, they require a large number of observations to obtain an accurate estimate for $f$.\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.3-The-Trade-Off-Between-Prediction-Accuracy-and-Model-Interpretability":{"title":"2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability","content":"#textbook_intro-to-statistical-learning\n\n- Why would we use a more restrictive method instead of a flexible approach? There is often a trade-off between how restrictive a model is and how interpretable it is.\n- If our goal is inference, then we'd likely choose a more restrictive model due to the high interpretability it allows. On the other hand, some methods are so flexible that they lead to complicated estimates of $f$ that are difficult to understand. In this case, it may be difficult to understand how any individual predictor is associated with the response. \n- However, if our goal is prediction accuracy, then we'd choose the method that provides the greatest performance over our dataset. This may not necessarily be the most flexible model type, due to the possibility that the flexibility allows the model to [[2.2.1 Measuring the Quality of Fit (Regression)#Overfitting|overfit]].\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.4-Supervised-Versus-Unsupervised-Learning":{"title":"2.1.4 Supervised Versus Unsupervised Learning","content":"#textbook_intro-to-statistical-learning\n\n- *Supervised learning* refers to the problem set where for each observation of the predictor measurements ($x_i$, $i=1,\\dots,n$) there is an associated response measurement ($y_i$). We aim to fit a model that relates the response to the predictors with either prediction or inference as the goal. ^31a009\n\t- Many statistical learning methods fall into this domain: linear/logistic regression, Generalised Additive Models, boosting, and support vector machines.\n- *Unsupervised learning* describes the situation where for every observation ($i=1,\\dots,n$), we have a vector of measurements ($x_i$) but no associated response ($y_i$). We do not have response data to supervise our analysis, which limits what we're able to achieve. ^0ebfb2\n\t- We can instead seek to understand the relationships between the variables or the observations using:\n\t\t- [[Cluster analysis|Cluster analysis/clustering]]: where the goal is to determine whether observations fall into distinct groups (i.e. categorising potential customers)\n- There are also *semi-supervised* situations where we have responses for some of the $n$ observations, but no responses for the rest of them. This may arise if it is expensive to obtain response data. We would want to use learning methods that can incorporate the observations for which there are responses, but also the ones without responses - however this is beyond the scope of this book.\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.5-Regression-Versus-Classification-Problems":{"title":"2.1.5 Regression Versus Classification Problems","content":"#textbook_intro-to-statistical-learning\n\n- Variables are either *quantitative* (numerical) or *qualitative* (categorical).\n- We refer to problems with a quantitative response as *regression* problems; whereas those with qualitative responses are referred to as *classification* problems.\n\t- Some techniques step into both definitions such as logistic regression which estimates class probabilities (regression), but then turns these into a binary response (classification).\n\t- Some techniques can be used in either quantitative or qualitative situations such as [[2.2.3.2 K-Nearest Neighbours|K-nearest neighbours]] and [[boosting]].\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2-Assessing-Model-Accuracy":{"title":"2.2 Assessing Model Accuracy","content":"#textbook_intro-to-statistical-learning\n\n* There is no free lunch in statistics. No one method dominates all others over all possible datasets.\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.1-Measuring-the-Quality-of-Fit-Regression":{"title":"2.2.1 Measuring the Quality of Fit (Regression)","content":"#textbook_intro-to-statistical-learning\n\n- We need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation.Â \n- In the regression setting, the most commonly-used measure is the mean squared error (MSE)\n$$MSE = \u000crac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2$$\n* In general, we donâ€™t really care how well the method works on the training data. Instead, weâ€™re interested in the accuracy of predictions on data that was previously unseen by the model.\n\n## Overfitting\n\n- As the flexibility of the statistical method increases, we tend to observe the accuracy over the training set trending monotonically downwards; whereas the accuracy over the test set will decrease initially but hit an inflection point and then increase.\n\t- This is a fundamental property of statistical learning.\n\t- When the model has a low MSE for the training data but a high test MSE, the model is said to be *overfitting* the data.\n\t- This may happen because the statistical learning algorithm is working too hard to find patterns in the training data, and may be picking up patterns that are caused by random chance instead of the true properties of $f$.\n\t- We generally expect the training MSE to be lower than the test MSE regardless of if overfitting has occurred because the model seeks to minimise MSE on the training set.\n\n\u003e [!info] From the exercises...\n\u003e Ceteris paribus, the following make a dataset more prone to overfitting:\n\u003e - **A smaller dataset**: this makes the model more likely to fit to the noise as opposed to the true $f$ signal simply due to datapoints that may be missing\n\u003e - **A larger amount of predictors**: this provides a larger number of potential relationships between predictors and response, meaning that the chance that the model will pick up on a spurious relationship is increased\n\u003e - **Higher variance in the irreducible error**: this makes it easier for a flexible model to incorrectly classify changes in the irreducible error as relationships with the outcome variable\n\n- Methods will be discussed later for picking the correct training/test sets to most accurately find which model minimises test MSE. This will include methods such as [[cross-validation]].\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.2-The-Bias-Variance-Trade-Off":{"title":"2.2.2 The Bias-Variance Trade-Off","content":"#textbook_intro-to-statistical-learning\n\n- It is possible to show that the expected test MSE (for a given value $x_0$) can be decomposed into the sum of the *variance* of $\\hat{f}(x_0)$, the squared *bias* of $\\hat{f}(x_0)$ and the variance of the irreducible error terms $\u001bpsilon$. i.e.: ^3872be\n\n$$E\\left(y_0-\\hat{f}(x_0)\right)^2=\text{Var}\\left(\\hat{f}(x_0)\right)+\\left[\text{Bias}\\left(\\hat{f}(x_0)\right)\right]^2+Var\\left(\u001bpsilon\right)$$\n- Where $E\\left(y_0-\\hat{f}(x_0)\right)^2$ is the expected test MSE at $x_0$ (the average test MSE that we would obtain if we repeatedly estimated $f$ using a large number of training sets, and tested each at $x_0$)\n- From this equation, we can see that to minimise the expected test error, we need to achieve a low variance and low bias.\n\n\u003e [!info]\n\u003e The variance is inherently a non-negative quantity, and the bias is squared so must always be positive. This means that expected test MSE can never lie below $\text{Var}\\left(\u001bpsilon\right)$.\n\n- What do we mean by the *variance* and *bias* of a statistical learning method?\n    - Variance refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training set.\n    - Since the training data are used to fit the statistical learning method, if we change the training data then the model estimate will also change. However, ideally weâ€™d want it to not vary too much between training sets (because if it **did** vary a lot, that means weâ€™re fitting the peculiarities in the data as opposed to the underlying $f$).\n    - In general, more flexible statistical methods have higher variance.\n\n- Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. E.g. Assuming that there is a linear relationship in the data whereas in real life the relationship is likely to be slightly non-linear.\n    - As we use more flexible methods, the variance will increase but the bias will decrease. Generally, as we increase the flexibility, bias decreases faster than the variance increases - but at some point variance will start to increase much faster.\n    - The lowest MSE will sit around where the decrease in in the bias has slowed down, and the increase in the variance hasnâ€™t sped up yet.\n    - This relationship is referred to as the *bias-variance* trade-off.\n\n![[Pasted image 20230723122348.png]]\n\n\u003e [!info]\n\u003e In a real-life situation in which $f$ is unobserved, it is generally not possible to explicitly compute the test MSE, bias, nor variance for a statistical learning method.\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.3-Measuring-the-Quality-of-Fit-Classification":{"title":"2.2.3 Measuring the Quality of Fit (Classification)","content":"#textbook_intro-to-statistical-learning\n\n- The concepts discussed so far (like the bias-variance trade-off) apply to both regression and classification problems, albeit with some differences. \n- In classification problems, the output ($y_i$) is a category or class label instead of a number. \n- The training error rate is a common measure of the model accuracy in classification. It quantifies the proportion of the training data that the model misclassifies:\n    - Suppose we seek to estimate $f$ on the basis of training observations $\\{(x_1, y_1), â€¦ , (x_n, y_n)\\}$ where now $y_1, y_2, â€¦ , y_n$  are qualitative. The most common approach for quantifying the accuracy of our estimate $\\hat{f}$ is the training error rate, the proportion of mistakes that are made if we apply our estimate $\\hat{f}$ to the training observations: \n$$\u000crac{1}{n} \\sum_{i=1}^{n} I(y_i \neq \\hat{y}_i)$$\n  - Here $\\hat{y}_i$ is the predicted class label for the $i$th observation using $\\hat{f}$. $I(y_i \neq \\hat{y}_i)$ is an indicator variable that equals 1 if $y_i \neq \\hat{y}_i$ (incorrect classification), and zero if $y_i = \\hat{y}_i$ (correct classification). \n    - The test error rate is notated as: $Ave(I(y_i \neq \\hat{y}_i))$\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.3.1-The-Bayes-Classifier":{"title":"2.2.3.1 The Bayes Classifier","content":"#textbook_intro-to-statistical-learning #bayes-classifier\n\n- The Bayes Classifier minimises the test error rate by assigning each observation to the most likely class, given its predictor values. This is based on the conditional probability:Â \n$$\\Pr(Y = j | X = x_0)$$\n- Which is the probability that Y = j, given the observed predictor vector $x_0$.Â \n\n- In a two-class problem where there are only two possible response values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if $Pr(Y = 1|X = x_0) \u003e 0.5$, and class two otherwise.Â \n    - In this case, the Bayes decision boundary is the set of points where the probability of belonging to one class or another is exactly 50%.Â \n\n- The Bayes classifier produces the lowest possible test error rate, known as the Bayes error rate. The error rate at $X = x_0$ is $1âˆ’max_j Pr(Y = j|X = x_0)$.\n    - In general, the overall Bayes error rate is given by:\n  $$ \text{{Bayes Error Rate}} = 1 - E\\left[\\max_j Pr(Y = j|X)\right] $$   \n- Where the expectation averages the probability over all possible values of $X$.Â \n\n- For our simulated data example, the Bayes error rate is 0.133. It is greater than zero, because the classes overlap in the true population so $\\max_j Pr(Y = j|X = x_0) \u003c 1$ for some values of $x_0$. The Bayes error rate is analogous to the [[2.2.2 The Bias-Variance Trade-Off#^3872be|irreducible error]].Â \n  \n- The Bayes classifier is the theoretical gold standard for predicting qualitative responses, but itâ€™s often impossible to compute for real data because we donâ€™t know the conditional distribution of Y given X.\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.3.2-K-Nearest-Neighbours":{"title":"2.2.3.2 K-Nearest Neighbours","content":"#textbook_intro-to-statistical-learning #k-nearest-neighbours\n\n* Many methods try to estimate the conditional distribution of Y given X, and then classify an observation to the class with the highest estimated probability. One such method is the K-nearest neighbours (KNN) classifier.\n* Given a positive integer $K$ and a test observation $x_0$, the KNN classifier first identifies the $K$ points in the training data that are closest to $x_0$, represented by $N_0$. It then estimates the conditional probability for class $j$ as the fraction of points in $N_0$ whose response values equal $j$:\n$$Pr(Y = j|X = x_0) = \u000crac{1}{K} \\sum_{i \\in N_0} I(y_i = j)$$\n* Finally, KNN classifies the test observation $x_0$ to the class with the largest probability from the above equation.\n\n\u003e [!info]\n\u003e For a highly non-linear Bayes decision boundary, we would expect that a smaller $K$ provides a better fit, since this gives the model more flexibility. A high $K$ would provide more bias but less variance.\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/3-Linear-Regression/3-Linear-Regression":{"title":"3 Linear Regression","content":"#regression \n\n- Linear regression is a very simple supervised learning method for predicting a quantitative response.\n- It serves as a good jumping-off point for newer approaches since many more complex statistical learning approaches can be seen as generalisations or extensions of linear regression.\n- With linear regression, we can answer the following questions:\n    - Do the data provide evidence of an association between the predictor/s and the response?\n    - How strong is this relationship? Does knowledge of the predictor/s provide a lot of information about the response?\n    - Are all predictors associated with the response, or just a few? How do we separate out each individual predictorâ€™s association size and strength of relationship?\n    - How accurately can we use the predictors to predict future response values?\n    - Is the relationship linear?\n    - Is there synergy between the predictors in predicting the relationship? Ie. is the combination of two predictors stronger at predicting the response than the sum of their parts ([[interaction terms]])?\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/3-Linear-Regression/3.1-Simple-Linear-Regression":{"title":"3.1 Simple Linear Regression","content":"#simple-linear-regression #regression \n\n- Straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes there is a linear relationship between X and Y at the population level, ie. that the following formula holds\n$$Y \u0007pprox \u0008eta_0 + \u0008eta_1X$$\n\u003e [!info]\n\u003e This can be referred to as *regressing* $Y$ *on/onto* $X$\n\n- $\u0008eta_0$ and $\u0008eta_1$ are two unknown constants (i.e. model parameters) that represent the *intercept* and *slope* of the linear model.\n    - We will use our training data to estimate $\\hat{\u0008eta}_0$ and $\\hat{\u0008eta}_1$, ie. the intercept and slope derived from our sample.\n    - We can then estimate what the response will be for a given value of $x$ using:\n$$\\hat{y} = \\hat{\u0008eta}_0 + \\hat{\u0008eta}_1x$$\n\u003e [!info]\n\u003e We use the caret to denote an *estimated* value for an unknown parameter or coefficient (as opposed to the population value)\n\n## 3.1.1 Estimating the Coefficients\n\n- In practice, the population $\u0008eta_0$ \u0026 $\u0008eta_1$ are unknown. We must use data to estimate these coefficients.\n- Our goal is to obtain coefficient estimates $\\hat{\u0008eta}_0$ and $\\hat{\u0008eta}_1$ such that the linear model fits the available data well (ie. $y_i \u0007pprox \\hat{\u0008eta}_0 + \\hat{\u0008eta}_1x_i$ for $i = 1, \\ldots ,n$)\n    - Put simply, we want to find intercept $\u0008eta_0$ and slope $\u0008eta_1$ such that the resulting line fits as *close as possible* to the *n* training data points.\n    - There are many ways of measuring closeness, but weâ€™ll be focusing on minimising the *least squares* criterion in this chapter.\n\n- $\\hat{y}_i = \\hat{\u0008eta}_0 + \\hat{\u0008eta}_1x_i$ is the prediction for Y based on the *i*th value of X.\n    - Then $e_i = y_i - \\hat{y}_i$ is the *i*th *residual*; the distance between the observed value ($y_i$) and the predicted value (\\hat{y}_i)\n    - Summing and squaring (so that negative/positive residuals donâ€™t cancel each other out) all residuals, we get the *residual sum of squares*. This is a measure of the total distance between modelled and observed values for the dataset.\n    - ==The least squares approach to linear regression aims to find $\\hat{\u0008eta}_0$ and $\\hat{\u0008eta}_1$ such that residual sum of squares is minimised.==\n\n\u003e [!info] Residual Sum of Squares (RSS)\n\u003e $$RSS = e_1^2 + e_2^2 + \\ldots + e_n^2$$\n\u003e Expanding with $e_i = y_i - \\hat{y}_i$ \u0026 $\\hat{y}_i = \\hat{\u0008eta}_0 + \\hat{\u0008eta}_1x_i$ we get:\n\u003e $$RSS = (y_1 - \\hat{\u0008eta}_0 - \\hat{\u0008eta}_1x_1)^2 + (y_2 - \\hat{\u0008eta}_0 - \\hat{\u0008eta}_1x_2)^2 + \\ldots + (y_n - \\hat{\u0008eta}_0 - \\hat{\u0008eta}_1x_n)^2$$\n\n\u003e [!info] Least Squares approach to Regression\n\u003e Using calculus, one can show that the values of $\\hat{\u0008eta}_0$ and $\\hat{\u0008eta}_1$ that minimise the RSS are:\n\u003e $$\u0008eta_1 = \u000crac{\\sum_{i=1}^{n} (x_i - \u0008ar{x})(y_i - \u0008ar{y})}{\\sum_{i=1}^{n} (x_i - \u0008ar{x})^2}$$\n\u003e $$\\hat{\u0008eta}_0 = \u0008ar{y} - \\hat{\u0008eta}_1\u0008ar{x}$$\n\u003e Where $\u0008ar{y} \u001bquiv \u000crac{1}{n}\\sum_{i=1}^{n} y_i$ and $\u0008ar{x} \u001bquiv \u000crac{1}{n}\\sum_{i=1}^{n} x_i$ are the sample means.\n\n^40a4e4\n## 3.1.2 Assessing the Accuracy of the Coefficient Estimates\n\n- In linear regression we assume the *true* relationship between X and Y is linear and hence takes the form $Y = f(X) + \u001bpsilon =\u0008eta_0 + \u0008eta_1X + \u001bpsilon$. This is the *population* regression line, ==our best linear approximation to the true relationship between X and Y==.\n    - $\u0008eta_0$ and $\u0008eta_1$ are the intercept and slope estimates. $\u001bpsilon$ is the catch-all (and mean-zero) error term for what we miss by approximating reality with this simple model.Â \n    - The true relationship is likely not linear, we may be missing other X variables that cause variation in Y, and there might be measurement error.\n- The [[#^40a4e4|least squares estimates]] for $\u0008eta_0$ and $\u0008eta_1$ define the *least squares* line.\n\n- We can see the difference between the two in the graph below. The red line is the *population line* plotted from a known linear relationship ($Y = 2 + 3X$). The blue line is the *least squares* line fit on data generated from that linear relationship *plus* the $\u001bpsilon$ error term which is normally distributed with mean zero.\n    - Further, the light blue lines in the graph on the right are least squares lines fit on samples of the generated data.\n\n![](https://lh3.googleusercontent.com/J_H-NEQ6QEDM65B9crCrA-1_Jz4fKp2KKqPhjv6O9BgX1sCgnHE3KRou6zqNw-kVvqaqcceDHxAhNclFyg3zQ0o22WmqeCKsH-IL9dRsQa6dtuXDN4wCTN1ttmm2uVQoqzitfzuiJ4kKBb66cbwRqJI)\n\n\u003e [!info] Why are there multiple lines?\n\u003e - In practice, the population linear relationship (red line) is not known and must be approximated with data (any of the blue lines).Â \n\u003e - Fundamentally, this is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population.\n\u003e ----\n\u003e - For example, say weâ€™re interested in knowing the population mean $\\mu$ of some random variable $Y$.\n\u003e - Unfortunately, $\\mu$ is unknown, but we do have access to $n$ observations from $Y$, $y_1, \\dots, y_n$ from which we can reasonably estimate $mu \u0007pprox \u0008ar{y} = \u000crac{1}{n} \\sum_{i=1}^{n} y_i$ (we are estimating the population mean using the sample mean).\n\u003e - In other words, the sample mean and population mean are different, but the sample mean provides a good approximation of the population mean. This same concept applies to the approximation of $\u0008eta_0$ and $\u0008eta_1$ using $\\hat{\u0008eta_0}$ and $\\hat{\u0008eta_1}$ estimated from sample data.\n\n### Bias \u0026 Variance\n#### Unbiased estimator\n\n- $\\hat{\u0008eta_0}$ and $\\hat{\u0008eta_1}$ are *unbiased estimates* of the true population parameters, which means that over multiple samples, the estimates will on average equal the true parameter.\n    - I.e. These estimates will not systematically underor over-estimate the true population parameters.\n#### Standard error\n\n- The natural follow-up question is how accurate are $\\hat{\u0008eta_0}$ and $\\hat{\u0008eta_1}$ as estimates of $\u0008eta_0$ and $\u0008eta_1$? We answer this question by computing the *standard error* of $\\hat{\\mu}$ ($SE\\left(\\hat{\\mu}\right)$).\n    - The standard error tells us the average amount that the parameter estimates differ from the population parameter value.\n\n\u003e [!info] Standard error of the population mean\n\u003e $$\text{Var}(\\hat{\\mu}) = \text{SE}(\\hat{\\mu})^2 = \u000crac{\\sigma^2}{n}$$\n\n- Where $\\sigma$ is the standard deviation of each of the realisations $y_i$ of $Y$.\n    - The equation above also tells us that the deviation shrinks with $n$ - more observations means smaller standard error.\n    - $\\sigma$ and $\\sigma^2$ are population parameters and are not known, but can be estimated from the data.\n\n\u003e [!info] Residual standard error (our estimate of $\\sigma$)\n\u003e $$\\hat{\\sigma} = RSE = \\sqrt{\u000crac{RSS}{n - 2}}$$\n\u003e Where RSS is the [[Residual Sum of Squares]] - the total distance between modelled and observed values for the dataset\n\n- The equations for standard error of $\\hat{\u0008eta_0}$ and $\\hat{\u0008eta_1}$ are:\n\n\u003e [!info] Standard error of the intercept and slope coefficients\n\u003e $$\text{SE}(\\hat{\u0008eta}_0)^2 = \\sigma^2 \\left( \u000crac{1}{n} + \u000crac{\u0008ar{x}^2}{\\sum_{i=1}^{n} (x_i - \u0008ar{x})^2} \right)$$\n\u003e $$\text{SE}(\\hat{\u0008eta}_1)^2 = \u000crac{\\sigma^2}{\\sum_{i=1}^{n} (x_i - \u0008ar{x})^2}$$\n\n\u003e [!info] Intuition\n\u003e - $\text{SE}(\\hat{\u0008eta}_1)^2$ (standard error for the slope estimate) is smaller when the $x_i$ are more spread out. Intuitively, we have more leverage to estimate the slope when this is the case.\n\u003e - $\text{SE}(\\hat{\u0008eta}_0)^2$ (standard error for the intercept estimate) is the same as $\text{SE}(\\hat{\\mu})$ if $\u0008ar{x} = 0$. If this were the case, $\\hat{\u0008eta}_0)$ would equal $\u0008ar{y}$.\n### Confidence intervals\n\n- We can use standard errors to compute [[2 Confidence Intervals|confidence intervals]].\n    - A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter.\n    - In other words, if we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter.\n\n- In linear regression, the 95% confidence intervals for $\u0008eta_1$ approximately takes the form:\n\n\u003e [!info] Confidence intervals for $\u0008eta_0$ and $\u0008eta_1$\n\u003e $$\u0008eta_1 \\pm 2 ","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/3-Linear-Regression/3.2-Multiple-Linear-Regression":{"title":"3.2 Multiple Linear Regression","content":"#multiple-linear-regression #regression \n\n- When modelling something that is influenced by multiple predictors, fitting a separate simple linear regression model for each predictor is not sufficient as each model ignores the other predictors. This can lead to issues of spurious or misleading associations between a predictor and the response that may arise when predictors are correlated.\n- Instead of using one predictor, multiple linear regression uses multiple predictors. It does this by giving each predictor a separate slope coefficient in a single model.Â \n- Instead of using separate simple linear regression models for each predictor, a better approach is to extend the simple linear regression model to accommodate multiple predictors directly. The multiple linear regression model with $p$ distinct predictors takes the form:\n$$ Y = \u0008eta_0 + \u0008eta_1X_1 + \u0008eta_2X_2 + \\ldots + \u0008eta_pX_p + \u001bpsilon $$\n- Where $X_j$ represents the $j$-th predictor and $\u0008eta_j$ quantifies the association between that variable and the response $Y$. The coefficient $\u0008eta_j$ represents the average effect on $Y$ of a one unit increase in $X_j$, holding all other predictors fixed.\n\n- For example, in the Advertising data, we have examined the relationship between sales and TV advertising, and we also have data for radio and newspaper advertising. To understand whether these two additional predictors are associated with sales, we need to use a multiple linear regression model:\n$$ \text{sales} = \u0008eta_0 + \u0008eta_1 \times \text{TV} + \u0008eta_2 \times \text{radio} + \u0008eta_3 \times \text{newspaper} + \u001bpsilon $$\n## 3.2.1 Estimating the regression coefficients \n\n- In multiple linear regression, the model with $p$ predictors takes the form:\n$$ Y = \u0008eta_0 + \u0008eta_1 X_1 + \u0008eta_2 X_2 + \\ldots + \u0008eta_p X_p + \u001bpsilon \\quad \text{(equation 3.19)} $$\n- Where $X_j$ represents the $j$-th predictor, $\u0008eta_j$ quantifies the association between that predictor and the response $Y$, and $\u001bpsilon$ is the error term.\n\n- To estimate the regression coefficients $\u0008eta_0, \u0008eta_1, \\ldots, \u0008eta_p$, we use the least squares approach. The goal is to find the values of $\\hat{\u0008eta}_0, \\hat{\u0008eta}_1, \\ldots, \\hat{\u0008eta}_p$ that minimise the sum of squared residuals (RSS):\n$$ \text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\hat{\u0008eta}_0 - \\hat{\u0008eta}_1 x_{i1} - \\hat{\u0008eta}_2 x_{i2} - \\ldots - \\hat{\u0008eta}_p x_{ip})^2 \\quad \text{(equation 3.22)} $$\n- Where $\\hat{y}_i$ is the predicted value of $Y$ for the $i$-th observation based on the estimated coefficients.\n\n![](https://lh3.googleusercontent.com/Fth10domzk7Dm9vE-Vx3e3GOl4wxobpLixV-WsOSymd3nBHRXopNivc1HoR0vEXYAf4bG6bCEP1YWE8Zkl-UjPuPqnZmccleJA2WiMYv52nxkGDQc79T0VLgprdp1o3hbbMWEAooLUKv2CcDkmqM2tc)\n\n\u003e [!info]\n\u003e - In multiple regression, it's possible to find no direct relationship between a predictor and the response variable, while a simple linear regression might suggest otherwise. \n\u003e - This can occur when predictor variables are correlated with each other. The correlated predictors can make it seem like a particular variable has an impact on the response, but its influence disappears when considering all predictors together. This phenomenon is common in real-life situations, and it underscores the importance of using multiple regression to properly analyse and interpret the individual effects of predictors while accounting for correlations between them.\n\n## 3.2.2 Some important questions\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/introduction-to-statistical-learning/3-Linear-Regression/3.3-Other-Considerations-in-the-Regression-Model":{"title":"3.3 Other Considerations in the Regression Model","content":"\n\n\n\n\n$$\ny_i = \u0008eta_0 + \u0008eta_1 x_i + \u001bpsilon_i =\n    \u0008egin{cases}\n      \u0008eta_0 + \u0008eta_1 + \u001bpsilon_i \u0026 \text{if } i\text{th person owns a house} \\\n      \u0008eta_0 + \u001bpsilon_i \u0026 \text{if } i\text{th person does not.}\n    \u001bnd{cases}\n$$\n\n$$\u0008egin{align*} Y \u0026 = \u0008eta_0 + \u0008eta_1X_1 + \u0008eta_2X_2 + \u0008eta_3X_1X_2 + \u001bpsilon \\ \u0026 = \u0008eta_0 + (\u0008eta_1 + \u0008eta_3X_2)X_1 + \u0008eta_2X_2 + \u001bpsilon \\ \u0026 = \u0008eta_0 + \tilde{\u0008eta}_1X_1 + \u0008eta_2X_2 + \u001bpsilon \u001bnd{align*}\n$$\n$$\n\u0008egin{align*}\nY \u0026 = \u0008eta_0 + \u0008eta_1X_1 + \u0008eta_2X_2 + \u0008eta_3X_1X_2 + \u001bpsilon \\\n  \u0026 = \u0008eta_0 + (\u0008eta_1 + \u0008eta_3X_2)X_1 + \u0008eta_2X_2 + \u001bpsilon \\\n  \u0026 = \u0008eta_0 + \tilde{\u0008eta}_1X_1 + \u0008eta_2X_2 + \u001bpsilon\n\u001bnd{align*}\n$$\n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/mathematical-thinking/1-Introduction-Mathematical-Thinking":{"title":"1 Introduction - Mathematical Thinking","content":"#course_coursera-intro-maths-thinking \n\n## Introductory lecture\n\n- Mathematical thinking is a powerful way of thinking that has been developed over 3,000 years.\n- You won't learn any mathematics techniques in this course; this is about the thinking involved.\n- The transition to pure, abstract math is often difficult. Before this, the focus is on memorising procedures \u0026 thinking inside the box. \n    - If you got good at this, you likely got good at recognising certain kinds of problems and knowing how to address them. This course is about how to address problems that don't fit templates you're familiar with - and how to think them through.\n\n- Key steps:\n    1. Stop looking for a formula to apply or a procedure to follow\n    2. Figure out what the problem *actually* says, not the form of the problem. Focus on understanding, not doing.\n\n## Reading assignment - What is mathematics\n### More than arithmetic\n\n- Most applied math evolved in the past 3-4 centuries, but school math curriculum is largely based on theories at least 300 years old.\n- Math's origin is tied to counting money.\n    - Egyptians and Babylonians added geometry and trigonometry.\n    - From 500BCE to 300BCE, Greeks focused on geometry. Their exploration stopped upon encountering irrational numbers.\n    - Greeks transformed math from a tool into a formal study. Thales of Miletus, around 500BCE, introduced the concept of theorems with logical proofs.\n    - The rise of modern arithmetic in India during the First Millennium, complemented by the Middle Eastern exploration of algebra, propelled math further.\n    - High school math today primarily features the above, plus 17th-century additions: calculus and probability.\n- In the last century, math expanded from around 12 areas to over 60, leading to its modern description as the *science of patterns* (e.g., number theory for numbers, geometry for shapes).\n\n### Mathematical notation\n\n- Mathematicians use abstract notation due to the abstract patterns they examine; using other forms would be overly complex.\n- While modern math books are filled with symbols, just as musical notes aren't the music itself, mathematical notation isn't the entirety of mathematics. A music sheet symbolises a musical piece.\n\n### Modern college-level mathematics\n\n- Until roughly 150 years ago, despite studying more than just numbers, mathematicians viewed math mainly as a tool for calculation. Skill in math meant being adept at calculations and symbolic manipulations.\n- To ensure the reliability of mathematical discoveries, mathematicians began analysing the subject internally, shifting their focus from merely *doing* math to truly *understanding* it.\n\n### Why are you having to learn this stuff?\n\n- The 19th-century transition in math from computation to concept was primarily within the expert community. For many practitioners, like scientists and engineers, their mathematical approach remained largely unchanged and still does. Today's students not only use mathematical methods but also learn to prove them.\n- There's a rising demand for experts who can mathematically decode and precisely analyze new challenges, like in manufacturing. Educating these experts requires a deep focus on the foundational concepts behind mathematical techniques.\n\n## Lecture 1 - \n","lastmodified":"2023-08-18T01:52:42.91398933Z","tags":[]},"/statistics/predictive-modelling/Farrington-Flexible-Aberration-Detection":{"title":"Farrington Flexible Aberration Detection","content":"#excess-mortality #regression #farrington-flexible\n\nA quasi-Poisson regression-based aberration detection algorithm\n## Useful links\n\n- [R `surveillance` function definition for `farringtonFlexible()`](https://github.com/r-forge/surveillance/blob/master/pkg/R/farringtonFlexible.R)\n\t- [Official documentation](https://surveillance.r-forge.r-project.org/pkgdown/reference/farringtonFlexible.html)\n- [*Comparison of statistical algorithms for daily syndromic surveillance aberration detection*, Noufaily et al. 2019](https://academic.oup.com/bioinformatics/article/35/17/3110/5301313#151566659)\n- *An improved algorithm for outbreak detection in multiple surveillance systems*, Noufaily et al. 2012, doi:10.1002/sim.5595 - best detailed look at Farrington Flexible\n- *Monitoring count time series inÂ R: Aberration detection in public health surveillance*, Salmon et al. 2016, [doi:10.18637/jss.v070.i10](https://doi.org/10.18637/jss.v070.i10) - paper on the `surveillance` package, explaining how Farrington Flexible is implemented\n\n## Motivation\n\n- We need a way to detect outbreaks quickly and accurately to allow time to respond\n- Initial motivation was detection of bio-terrorist attacks but this can be used in any environment that requires swift and automated anomaly detection.\n\n## Method summary\n\n- General idea is to model baseline counts of events (generally trained over 5 years of baseline data)\n- The expected counts in the current week is then projected\n- If the observed value in this week is far enough away from the expected that the *exceedance score* is breached, this week is flagged as a possible outbreak.\n\n## Method details\n\n- Since the events in question are often counts, they may be best modelled by the Poisson distribution. However, this restricts variance to be equal to the mean, and variance often exceeds the mean. Hence, this algorithm uses an *overdispersed* [[Poisson regression]] to model counts - ie. [[Negative Binomial regression]], a generalisation of Poisson regression.\n- The original method took seasonality into account by **only** using a window around the current week in previous years as training data - so the model is trained on a subset of each year. Noufaily et al.'s method uses all available historical data and takes seasonality into account with a 10-level factor.\n- Noufaily et al.'s method also excludes the last 26 weeks before $t_0$ (the week we're predicting over to detect an outbreak) in an attempt to make the method less sensitive to outbreaks that may have started recently.\n- In addition, there are a few extra terms:\n\t- A time-varying trend is always fitted, regardless of if it is statistically significant\n\t- To capture seasonality, a yearly 10-level factor, whose reference period comprises comparable weeks in previous years (as discussed above)\n- The functional form is as follows ($i$ refers to the week number):\n \n$$log(\\mu_i) = \theta + \u0008eta t_i + \\delta_{j(t_i)}$$\n\nWhere:\n- $\\mu_i$ is the mean (expected) count, with variance $\\phi\\mu_i$\n- $\theta$ is the constant/intercept\n- $\\delta_{j(t_i)}$ is the 10-level factor\n\n- An exceedance score is then calculated finding the distance between the observed count in the target week and what was modelled for it with the quasi-Poisson regression. \n- The count is flagged as an outbreak if this distance is larger than the distance between the modelled mean and the 95% (or $\u0007lpha$%) upper threshold of the Negative Binomial quantile.\n\t- In other words the exceedance score, $X$ is calculated; and if $X \\ge 1$ then the week's count is flagged as an outbreak, where:\n$$X = \u000crac{y_0 - \\hat{\\mu_0}}{U-\\hat{\\mu_0}}$$\nWhere:\n- $y_0$ is the observed count in the target week\n- $\\hat{\\mu_0}$ is the expected count, calculated as $\\hat{\\mu_0} = \\hat\theta + \\hat{\u0008eta}t_0+\\delta_{j(t_0)}$\n- $U$ is the upper threshold, the $100(1-\u0007lpha)\\%$ Negative Binomial quantile\n\n\u003e [!info]\n\u003e Another approach to computeÂ *U*Â uses the 2/3 power transformation of the Poisson distribution which is approximately normal.\n## Example\n\nWe'll use the example data provided in the R `surveillance` package ([`salmonella.agona`](https://rdrr.io/cran/surveillance/man/salmonella.agona.html)), a dataset containing reported number of cases of the Salmonella Agona serovar in the UK 1990-1995.\n\nWe need this to be in `sts` form to work with functions in the `surveillance` package. The `salmonella.agona` dataset is stored in `disProg` (the old data type used by `surveillance`), so we need to run:\n\n```r\ndata(\"salmonella.agona\")\n\n# Create the corresponding sts object from the old disProg object\nsalm \u003c- disProg2sts(salmonella.agona)\n```\n\nNext, we need to provide a list of controls to the `farringtonFlexible` function. See below for the 'Flexible' controls (as per Salmon et al. 2016):\n\n```r\ncontrols \u003c- list(\n  range = 282:312, # date/week range to operate within\n  noPeriods = 10, # 10 seasonal factors\n  b = 4, w = 3, # controls relating to the window size TODO: FIGURE THIS OUT\n  weightsThreshold = 2.58,\n  pastWeeksNotIncluded = 26, # not including last few weeks to dampen impact of recent epidemics\n  pThresholdTrend = 1,\n  alpha = 0.1\n)\n```\n\nThis is what the data look like, as well as the seasonal factor \n","lastmodified":"2023-08-18T01:52:42.917989371Z","tags":[]},"/statistics/reinforcement-learning/1-RL-Introduction-to-Reinforcement-Learning":{"title":"1 RL Introduction to Reinforcement Learning","content":"#course_google-deepmind-reinforcement-learning #reinforcement-learning\n## About reinforcement learning\n\n![[Pasted image 20230816152354.png]]\n\n- *Reinforcement learning* (RL) sits at the intersection of many different fields of science. Many different fields try to understand the optimal way to make decisions, and these all relate to reinforcement learning.\n\n![[Pasted image 20230816152543.png]]\n\n- How is RL distinct from supervised and unsupervised learning?\n    - There is no supervisor, only a reward signal\n    - Feedback is delayed, possibly by several steps - and is not instantaneous\n    - Time actually matters, we're looking at sequential processes in which an agent moves throughout a world which means non-i.i.d data\n    - The agent's actions can influence its environment and therefore the subsequent data it sees\n\n\u003e [!tip] Examples of RL use cases\n\u003e - **Fly stunt manoeuvres in a helicopter** - a reward if the manoeuvre is performed successfully with no crashes\n\u003e - **Defeat the world champion in Backgammon or play Atari games better than humans** - good reward if a game is won\n\u003e    - Don't even need to tell it the rules of the game, just set up the reward structure\n\u003e - **Manage an investment portfolio** - the reward might be financial returns\n\u003e - **Make a humanoid robot walk** - rewards for making progress\n\n## The reinforcement learning problem\n### Reward\n\n- A *reward* $R_t$ - a scalar feedback signal. Indicates how well the agent is doing at step *t*. The agent's job is to maximise cumulative reward.\n    - The overarching goal is to select actions to maximise total future reward.\n        - Actions might have long term consequences\n        - Reward may be delayed\n        - It may be better to sacrifice immediate reward to gain more long-term reward, strategic thinking is part of the process\n    - What if the goal is to perform an action in the fastest amount of time? Typically what's done is to define the reward as `-1` per time step.\n    - You can use multi-faceted reward systems, but you can generally represent these in a summarised fashion as a scalar reward.\n\n\u003e [!info] Definition: Reward Hypothesis\n\u003e All goals can be described by the *maximisation* of *expected cumulative reward*.\n\u003e \n\u003e - Reinforcement learning is based on this hypothesis.\n\n\u003e [!tip] Examples of $R_t$\n\u003e - **Fly stunt manoeuvres in a helicopter**: +ve for following desired trajectory, (large) -ve for crashing\n\u003e     - Re-fueling itself might prevent a crash in several hours (stop and lose a small amount of reward for not following path to mitigate a crash later due to no fuel)\n\u003e - **Defeat the world champion at Backgammon**: no intermediate rewards, but at end of game give a +/-ve signal based on winning/losing the game\n\u003e - **Manage an investment portfolio signal**: +ve reward for each $ in bank\n\u003e     - Investment decisions might take months to come to fruition (delayed reward)\n\u003e - **Controlling a power station**: +ve reward for producing power, -ve reward for exceeding safety thresholds\n\u003e - **Making a humanoid robot walk**: +ve reward for forward motion, (large) -ve reward for falling over\n\u003e - **Making a car go around a track**: -ve reward for exceeding track limits/moving x% away from designated racing line, -ve reward for every second taken to get around the track\n\u003e     - The former needs to be big enough that the optimal strategy isn't to exceed track limits to gain time\n\n### Agent and the environment\n\n![[Pasted image 20230816155812.png]]\n\n- The brain represents the *agent*, this is what we're building. Our agent will be composed of algorithms that will be able to respond to stimuli and take actions (such as move controls, or make investments). So at each step the agent will:\n    - Execute action $A_t$\n    - Receive observation $O_t$\n    - Receive scalar reward $R_t$\n\n- The *environment* is represented by the world (what's on the other side of the agent). There will be a loop over time, where in every step the agent sees a snapshot of the world at this moment generated by the environment. We have no control over the environment except through the agent's action $A_t$. So the environment will:\n    - Receive action $A_t$\n    - Emit observation $O_t$\n    - Emit scalar reward $R_t$\n- If we have a *multi-agent* system, the other agents can be seen as part of this environment to any given agent.\n\n- The *history* is the sequence of observations, actions, and rewards (all observable variables up to time *t*; the sensorimotor stream of the agent). This is everything the agent has seen so far.\n$$H_T = A_1, O_1, R_1, \\dots, A_t, O_t, R_t$$\n- What happens next depends on this history.\n    - Our goal is to build a mapping (i.e. an algorithm) from one of these histories to picking the next action. the agent will use this to select the next action.\n    - The environment will select the observations it emits and the rewards it provides at each time stamp.\n    - The history isn't very useful though because it is typically long, we want to have agents that have long lives that can deal with microsecond interactions; so what we talk about instead is *state*.\n\n### State\n\n- *State* is a summary of the information that's used to determine what happens next. We replace the history with some concise summary with all the info we need to act. Formally, it is a function of the history:\n$$S_t=f\\left(H_t\right)$$\n- There are three definitions of state; what do they mean and how do they relate to each other?\n\n    - The *environment* state ($S_t^e$); this is the information that's used in the environment to determine what happens next, i.e. the next observation/reward. \n        - There will be some set of numbers that describes the process that drives what happens next. The summarisation of that which is fed to the agent is the environment state.\n        - The environment state is not usually visible to the agent; the agent can only see what is provided by the environment in the observation.\n        - Even if we could see this information, sometimes it may not be the right information we'd want to use to make the decision (i.e. does the atomic makeup of the game world help the agent make better decisions? Maybe its own subjective view of the environment may be better)\n\n    - The *agent* state ($S_t^a$) captures what's happened to the agent so far, summarises it all into useful information, and it uses these numbers to pick the next action.\n        - Our decision is how to process those observations and what to remember/throw away.\n        - It is the information used by the RL algorithm and can be any function of the history $S_t^a=\\left(H_t\right)$.\n\n    - The *information* state (a.k.a. *Markov* state - a concept from information theory) contains all useful information from history.\n        - By definition, the environment state is Markov. $S_t^e$ at any given moment is Markov since that's what its using to pick the next observation it will emit and the reward.\n        - By definition, the history $H_t$ is Markov. If we retain the whole history of everything and make our decisions on the entire history, the entire history contains as much information as the entire history.\n        - It's always possible to come up with some Markov state, the question is how do we find a useful representation in practice.\n\n\u003e [!info] Definition: Markov State\n\u003e A state $S_t$ is *Markov* iff:\n\u003e $$\\mathbb{P}[S_{t+1}|S_t]=\\mathbb{P}[S_{t+1}|S_1, \\dots, S_t]$$\n\u003e In other words, the probability of the next state given the state you're in is the same as if you showed all of the previous states to the system.\n\u003e \n\u003e i.e. This is a *memoryless* system. The current state fully characterises the distribution of over future actions/observations/rewards.\n\u003e $$H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\\infty}$$\n\n^c3f6cb\n\n\u003e [!tip] Example: Helicopter manoeuvres\n\u003e For the helicopter, its current velocity, orientation, wind speed, etc. may form a rough Markov state for the agent. When it knows this, it doesn't matter where it was before this, from this it knows where it will be in the next moment.\n\u003e \n\u003e An imperfect state might be if you only had the position but not the velocity. This is not Markov because from this state, you don't know where it will be in future since you don't know how fast it's moving. You have to look back in time to figure out what its velocity is and what its momentum will be.\n\n\u003e [!tip] Example: The rat\n\u003e ![[Pasted image 20230816163729.png]]\n\u003e Take for example, a rat in an experiment. Say that it experiences the three sequences shown above.\n\u003e - If you were the agent and used the last three items in the sequence as your state, you would believe that you were about to be electrocuted.\n\u003e - If you chose your agent state to be the count of lights, bells, and levers, you'd expect that the cheese would appear.\n\u003e - If your agent state was the complete sequence we wouldn't know what happens next.\n\u003e \n\u003e ==What we believe will happen next depends on our representation of state.==\n\n- A *fully observable environment* is one in which the agent *directly observes* the environment state. This is a nice case, the agent sees all the numbers in the environment state. i.e. $O_t=S_t^a=S_t^e$.\n    - When we have this situation, this is formally a *Markov decision process* (MDP). We'll discuss this in the next lecture and the majority of the course.\n\n- *Partial observability* describes the situation in which the agent *indirectly observes* the environment. e.g. A robot with camera vision isn't told its absolute location, and poker agent only observes the public cards.\n    - Now the agent must construct its own state representation $S_t^a$ that's distinct from the environment state. There are many ways to do this:\n        - **Complete history** - naive approach: $S_t^a=H_t$\n        - **Build beliefs of the environment state** - the Bayesian approach. Don't know what's happening in the environment but going to keep a probability distribution over where we think we are in the environment: $S_t^a = (\\mathbb{P}[S_t^e = s^1], \\ldots, \\mathbb{P}[S_t^e = s^n])$; i.e. we have some probability that the state is $s^1$ or $s^n$, etc\n        - Recurrent neural network: $S_t^a = \\sigma(S_{t-1}^aW_s + O_tW_o)$, take a linear combination of the agent state you had at the last time step with your current observation to generate your new state\n    - Formally, this is a *partially observable Markov decision process* (POMDP)\n## Inside an RL agent\n\nSo far we've only talked about the problem, not yet how to solve the problem.\n\n- An RL agent may include one or more of these components:\n\n    - **Policy**: the agent's *behaviour function*\n        - A map from state to action. \n        - Deterministic policy: If we're in some state $s$, we have some policy $\\pi$ that then determines our action from that state, i.e. $a=\\pi(s)$.\n        - Stochastic policy: Can be useful, helps us make random, exploratory decisions, i.e. $\\pi(a|s)=\\mathbb{P}[A=a|S=s]$\n\n    - **Value function**: how good is each state and/or action. How much reward do I expect to get if we take this action in this particular state.\n        - A prediction of expected future reward. This is how we choose between action 1 and action 2.\n        - Since the value will depend on what your action/policy is, we index by $\\pi$: $v_\\pi(s)=\\mathbb{E}_\\pi\\left[R_t+\\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\dots|S_t=s\right]$\n            - i.e. The value function tells us how much total reward we expect going into the future. We can also have discounting ($\\gamma$) that goes into the future which helps us care more about immediate rewards if we want.\n\n    - **Model**: agent's subjective representation of the how the environment works. Its sometimes useful to learn the behaviour of the environment and use that model of the environment to help figure out what to do next.\n        - It is optional to do this; a lot of the course we'll focus on model-free methods that don't use a model at all.\n        - The way we normally do this is to have two parts of the model:\n            - Transition model: $\\mathscr{P}$ predicts the next state, predicts the dynamics of the environment. If this were the helicopter, this is the function that would predict where the helicopter would be next given its current state.\n                - Formally this is represented in a state transition model that is the probability of being in the next state given the previous state and action: $\\mathscr{P}^a_{ss'}=\\mathbb{P}[S'=s'|S=s,A=a]$\n            - Reward model: $\\mathscr{R}$ predicts the next (immediate) reward. The helicopter can learn that if its in this position then it's not crashing and therefore doing well.\n                - Formally this is represented as a function that tells us the expected reward given the previous state and action: $\\mathscr{R}^a_s=\\mathbb{E}[R | S=s,A=a]$\n\n\u003e [!tip] Example: Maze\n\u003e ![[Pasted image 20230817132701.png]]\n\u003e \n\u003e Here the goal is to navigate this maze as quickly as possible, so:\n\u003e - Reward: -1 for every second taken\n\u003e - Actions: N, E, S, W\n\u003e - State: Agent's location in the maze\n\u003e \n\u003e An example of policy is like the following. For each state (location), there is an action (the arrows) mapped to it ($\\pi(s)$)\n\u003e ![[Pasted image 20230817133000.png]]\n\u003e \n\u003e An example of the value function ($v_\\pi(s)$), each state has an expected future value attached. You can see that its -1 at the last state since it knows there's only one more step to the end. It can also go higher such as -24 when you've taken a wrong turn and now it'll take you more steps to get to the end.\n\u003e ![[Pasted image 20230817133131.png]]\n\u003e \n\u003e An example of the agent's model of reality is like the following. The agent is trying to build its own map of the environment. The map represents the transition model ($\\mathscr{P}^a_{ss'}$) and the numbers in each grid represent the expected reward in each state ($\\mathscr{R}^a_s$)\n\u003e ![[Pasted image 20230817133422.png]]\n\n### Categorising RL agents\n\n- We can build a taxonomy of RL agents based on which of the key components our agent contains. The fundamental distinction in RL is whether the algorithm is model-free or model-based. Second to this is whether a policy or value function is used.\n    - **Value based**: stores a value function. If it's got a value function then the policy is implicit - it just needs to pick actions greedily with respect to the value function\n    - **Policy based**: we explicitly represent the policy and the agent will work on creating a policy that maximises the total cumulative reward - without ever storing an explicit value function\n    - **Actor Critic**: stores both the policy and the value function\n    - **Model Free**: has a policy and/or value; we do not try to explicitly understand the dynamics of the environment, we just see our policy/rewards and base our actions on that\n    - **Model Based**: first step is to build a dynamics model of how the environment works. Has a policy and/or value\n\n![[Pasted image 20230817134408.png]]\n\n## Problems within reinforcement learning\n\n### Learning and planning\n\nTwo fundamental problems in sequential decision making\n\n- Reinforcement Learning:\n    - The environment is unknown; the agent isn't told how the environment works.\n    - The agent interacts with the environment with the aim of getting the most cumulative reward, trial and error.\n    - The agent improves its policy.\n\n- Planning:\n    - A model of the environment is known, we tell the agent this\n    - Instead of interacting with the environment, the agent performs internal computations with its model\n    - As a result of this, the agent improves its policy.\n\nYou could also learn how the environment works, and then do planning\n\n\u003e [!tip] Example: Atari\n\u003e Reinforcement Learning setup: Rules of the game are unknown, the agent learns directly from interactive gameplay, picks actions on the joystick and sees pixels and scores.\n\u003e ![[Pasted image 20230817135332.png]]\n\u003e \n\u003e Planning setup: Rules of the game are known and told to the agent, can query an emulator (consider this a perfect model in the agent's brain) to plan its next step using look-ahead or tree search.\n\u003e ![[Pasted image 20230817135436.png]]\n\n### Exploration and exploitation\n\nHow do we balance exploration and exploitation?\n\n- Reinforcement learning is like trial and error learning. \n    - The problem is that the agent is losing reward along the way. \n    - We want to figure out a good policy without giving up opportunities to exploit the things it has discovered.\n\n- Exploration finds more information about the environment, giving up reward to do so. Could you potentially gain more reward by trying something you haven't done yet?\n- Exploitation exploits known information to maximise reward\n- It is important to do both, the question is what is the best balance?\n\n\u003e [!tip] Examples\n\u003e - Restaurant Selection\n\u003e     - *Exploitation*: go to your favourite restaurant\n\u003e     - *Exploration*: Try a new restaurant - you'll never know if you'll find \n\u003e - Online Banner Advertisements\n\u003e     - *Exploitation*: Show the most successful advert\n\u003e     - *Exploration*: Show a different advert - which might be more successful\n\n### Prediction and control\n\n- Prediction: evaluate the future given a policy\n- Control: what is the optimal policy, what policy should you choose?\n\nTypically you need to solve the prediction problem in order to solve the control problem\n\n\u003e [!tip] Example: GridWorld\n\u003e Prediction setup: if we perform a fixed policy of moving randomly across the grid, how much reward will we get?\n\u003e ![[Pasted image 20230817141004.png]]\n\u003e \n\u003e Control setup: whats the optimal behaviour of this GridWorld? If I behave optimally, now what's the value function? This is very different to the prediction problem\n\u003e ![[Pasted image 20230817141117.png]]\n","lastmodified":"2023-08-18T01:52:42.917989371Z","tags":[]},"/statistics/reinforcement-learning/2-RL-Markov-Decision-Processes":{"title":"2 RL Markov Decision Processes","content":"#course_google-deepmind-reinforcement-learning #reinforcement-learning \n## Markov processes\n\n- *Markov decision processes* (MDPs) formally describe an environment for machine learning\n    - We'll start with the nice case where the environment is *fully observable*. The agent gets all the information from the environment.\n    - Almost all RL problems can be formalised as MDPs\n        - Optimal control deals with continuous MDPs\n        - Any partially observable problems can be converted into MDPs\n        - Bandits (common formalism, you get a set of actions, you perform one action, get rewarded and the round ends) are MDPs with one state\n\nAs discussed in the [[1 RL Introduction to Reinforcement Learning#^c3f6cb|previous lecture]]:\n\n\u003e [!info] Definition - Recap: Markov State\n\u003e A state $S_t$ is *Markov* iff:\n\u003e $$\\mathbb{P}[S_{t+1}|S_t]=\\mathbb{P}[S_{t+1}|S_1, \\dots, S_t]$$\n\u003e In other words, the probability of the next state given the state you're in is the same as if you showed all of the previous states to the system.\n\u003e \n\u003e i.e. The current state is a sufficient statistic of the future\n\n### State transition matrix\n\nFor a Markov state $s$ and a successor state $s'$, the *state transition probability* is defined by $\\mathscr{P}_{ss'}=\\mathbb{P}\\left[S_{t+1}=s'|S_t=s\right]$.\n- Recall that $S_t$ characterises everything about what happens next (Markov by definition), that should mean that there is some well-defined transition probability that i will transition to another state.\n","lastmodified":"2023-08-18T01:52:42.917989371Z","tags":[]},"/web-dev-programming/Document-Object-Model":{"title":"Document Object Model","content":"TheÂ Document Object ModelÂ (DOM) is the data representation of the objects that comprise the structure and content of a document on the web. This allows programmatic access to the elements on a webpage - as well as attaching event handlers to objects.\n\nThe DOM represents the content of a web document as a logical tree where each branch of the tree ends in a node and each node contains objects.\n\nThe DOM can be access using an API (which is a collection of multiple APIs) in languages such as JavaScript. e.g.:\n\n```js\nconst paragraphs = document.querySelectorAll(\"p\");\n```\n\nNote that the DOM API is not inherently a part of JavaScript - and can be access via other languages such as Python as well.\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/d3/1-D3-Overview":{"title":"1 D3 Overview","content":"#course_codecademy-d3 #d3\n\n* D3 stands for Data-Driven Documents and has risen to popularity due to various reasons:\n\t* D3 has revolutionised the way that data visualisation is built natively on the web. It did this by associating data with elements on a website's [[Document Object Model]].\n\t* D3 offers a wide variety of pre-packaged visualisations\n\t* D3 is very low level (since it controls elements in the DOM) so can be leveraged to build a wide variety of bespoke visualisations\n\t* D3 supports interactivity driven by browser events\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/d3/2-D3-Selection":{"title":"2 D3 Selection","content":"#course_codecademy-d3 #d3\n\nD3 works by 'injecting' data visualisations onto an object's [[Document Object Model]], and associates data with a set of DOM elements.\n\nTo do this, D3 first needs to 'select' the elements you want to inject data into. These selections are created using the `.selectAll()` or `.select()` methods.\n* Both methods take a CSS3 selector string as a parameter and will return an array-like structure of all elements that match that selector (in the case of `.selectAll()`) or the first element that matches that selector (in the case of `.select()`).\n\nNote that the elements that are being selected don't necessarily need to be present in the DOM. You can select items prior to them being added to the web-page as a way of [['setting the stage']].\n\n## Examples\n\n```html\n\u003chtml\u003e\n\t\u003chead\u003e\n\t\t\u003clink rel=\"stylesheet\" href=\"style.css\"\u003e\n\t\t\u003c!-- Loading D3 --\u003e\n\t\t\u003cscript src=\"https://d3js.org/d3.v5.min.js\"\u003e \u003c/script\u003e\n\t\t\u003cscript src=\"main.js\" defer\u003e\u003c/script\u003e\n\t\u003c/head\u003e\n\t...\n\u003c/html\u003e\n```\n\n```js\n// Injects the text 'These are divs' into all div elements\nd3.selectAll('div').text('These are divs')\n```\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/d3/3-D3-Data-Element-Relationship":{"title":"3 D3 Data-Element Relationship","content":"#course_codecademy-d3 #d3\n\nWith elements [[2 D3 Selection|selected]], we can now associate data per element. You do this with `.data()` which takes an array of any type and binds its elements to the selected objects returned by `.select/selectAll()`. e.g.:\n\n```js\nlet dataset =Â [55,34,23,22,59];  \nd3.selectAll(\"p\").data(dataset);\n```\n\n`.data()` does not recycle the items in the `dataset` by default. If you provide a dataset that has less elements than the selected items, then the elements that don't have a matching element are left blank. On the other hand, if `dataset` is longer than the selected items then the surplus items will simply be left out.\n\nIf you want to address the mismatch between dataset length and selected elements, use the `.join()` method (more on how this works under the hood on the [[5 D3 Enter, Exit and Update|Enter/Update/Exit page]]); e.g.:\n\n```js\nlet fruits = ['Apple', 'Orange', 'Mango'];\n\nd3\n  .select(\".d3_fruit\")\n  .selectAll(\"p\")\n  .data(fruits)\n  .join(\"p\") // Create 3 `p` elements to match the length of `fruits`\n    .attr(\"class\", \"d3_fruit\") // Set class of these 3 p elements to `d3_fruit`\n    .text((d) =\u003e d); // Set text in these elements\n\n// html\n\n\u003cdiv class=\"d3_fruit\"\u003e\u003c/div\u003e\n```\n\nIn the code block above, there are no `p` elements within the div of class `d3_fruit`. `.join()` realises this and creates 3 `p` elements to match the `3` elements in the `fruits` dataset.\n\nNote that the indentation levels of certain lines are longer than others. More on this in [[6 D3 Chaining and Returning]].\n\nThis can also be addressed using `.enter()` and `.append()`.\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/d3/4-D3-The-d-Parameter":{"title":"4 D3 The d Parameter","content":"#course_codecademy-d3 #d3\n\nAfter we've [[2 D3 Selection|selected]] our elements and [[3 D3 Data-Element Relationship|bound]] our dataset to them, we'll need to find a way to properly display that data in those elements.\n\nThis comes in the form of multiple methods, each taking an anonymous function with argument `d` - where `d` represents each individual datum in your dataset. e.g.:\n\n```js\nlet dataset =Â [55,34,23,22,59];  \nd3.selectAll(\"p\").data(dataset).text((d) =\u003e d);\n```\n\nThe `.text()` method simply says to take each element of `dataset` and apply them to the selected elements.\n\n\u003e [!info]\n\u003e You can also include an `i` parameter in the anonymous function to access the index of the element.\n\nThere are multiple possible methods that can be used in this manner:\n\n| Method        | Usage                                                                                             |\n| ------------- | ------------------------------------------------------------------------------------------------- |\n| `.attr()`     | Update selected element attribute (can be used for class changes, etc)                            |     \n| `.classed()`  | Assigns or unassigns the specified CSS class names on the selected elements                       |     \n| `.style()`    | Updates the style property                                                                        |     \n| `.property()` | Used to set an element property                                                                   |     \n| `.text()`     | Updates selected element text content                                                             |     \n| `.html()`     | Sets the inner HTML to the specified value on all selected elements                               |     \n| `.append()`   | Appends a new element as the last child of each of the selected element/s                                   |     \n| `.insert()`   | Works the same as theÂ `.append()`Â method, except you can specify another element to insert before |     \n| `.remove()`   | Removes selected element from the DOM |\n\n`.attr()` and `.style()` take two parameters; the attribute/style rule you want to change, and what you want to change it to. .e.g:\n\n```js\nlet dataset =Â [55,34,23,22,59];  \n  \nlet svg =Â d3\n  .select(\"body\")  \n  .selectAll(\"div\")  \n    .data(dataset)  \n    .attr(\"id\", function(d,i) { return \"element-\" +Â i; })  \n    .style(\"width\", function(d) { return d +Â \"px\"; });\n```\n\n## Further sources\n\n* [D3.js Tutorial â€“ Data Visualization for Beginners](https://www.freecodecamp.org/news/d3js-tutorial-data-visualization-for-beginners/#data-driven)\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/d3/5-D3-Enter-Exit-and-Update":{"title":"5 D3 Enter, Exit and Update","content":"#course_codecademy-d3 #d3\n\n## Enter/Update/Exit\n\nWhen you seeÂ `selectAll()`Â followed byÂ `.data()`Â we are:\n- Selecting all matching elements\n- For each element that exists, we bind an item from the data array to it\n- `.data()` then returns an *update selection* containing the existing elements (if any) with data bound to them\n\nIf the number of selected elements doesn't match the number of elements in the data array then `.data()` will also return an *enter selection* or an *exit selection* in addition to an *update selection*.\n\n- If we have excess data items, `.data()` provides an *enter selection* with one element for every item we need to add in order to have number of DOM elements = number of data items.\n\n  - In this case, we often want to add more DOM elements to make up the difference and give all data points an element.\n  - The *enter selection* contains placeholders for all elements we'd need to add. Calling `.enter()` on the update selection selects these placeholders, then `.append(tagname)` will create these elements in the DOM.\n    - If you add attributes to the selection at this stage, you'll only be styling the newly added elements. To ensure that you apply your changes to all elements (old and new), use `.merge()` to merge the *enter* and *update selections*\n\n```js\nlet circle = svg.selectAll(\"circle\")\n  .data([1,2,3,4])\n\ncircle.exit().remove()\n\ncircle.enter()\n  .append(\"circle\") \n    .merge(circle)\n    .attr(â€¦)\n```\n\n- If we have excess DOM elements, then we get an *exit selection* in addition to the *update selection*.\n    - In this case, we often want to delete the excess elements since we don't need them. Calling `.exit()` will return the *exit selection* and calling `.remove()` will remove the excess elements.\n\n## Join\n\n`.join()` exists as a convenience method that:\n- Removes the *exit selection*; and,\n- Returns a merged *update selection* \u0026 *enter selection* that you can continue to style as needed.\n\nThe following block is equivalent to the previous block\n\n```javascript\nlet circle = svg.selectAll(\"circle\")\n  .data([1,2,3,4])\n  .join(\"circle\")\n  .attr(â€¦)\n```\n\n## Further sources\n\n- [What is the difference between .append and .join in D3.js - Stack Overflow](https://stackoverflow.com/a/69820794)\n- [D3 Enter, Exit and Update](https://www.d3indepth.com/enterexit/)\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/d3/6-D3-Chaining-and-Returning":{"title":"6 D3 Chaining and Returning","content":"#course_codecademy-d3 #d3 \n\nConventional chaining styling for D3 will use a single-indented line to indicate when the selection is changing, and a double-indented line for operations on the current selection, e.g.:\n\n```js\nlet dataset =Â [55,34,23,22,59];  \n  \nd3.select(\"body\")  \n  .selectAll(\"div\") // Selection has changed\n    .data(dataset)  \nÂ    .enter()  \n  .append(\"div\")    // Selection has changed to the enter selection\nÂ Â Â  .text(\"Some text\");\n```\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/d3/7-D3-Interactivity-with-Events":{"title":"7 D3 Interactivity with Events","content":"#course_codecademy-d3 #d3 \n\nAnother powerful aspect of D3 is that it has access to [[Document Object Model|DOM]] events.\n\nThe `.on()` method takes the event as a string and a function to run upon that event and binds this function to the elements in the selection. e.g.:\n\n```js\nselection  \n  .on(\"mouseover\", function(d,i) {  \n    d3.select(this).text(d);  \n  });\n```\n\nThe following assigns poem verses to each `p` tag and starts them all with 'Click me!' text. The `.on()` method then allows each element to switch over to their bound data element upon click.\n\n```js\nlet poemVerses = [\n  \"Always\", \"in the middle\", \n  \"of our bloodiest battles\", \n  \"you lay down your arms\",\n  \"like flowering mines\",\n  \"to conquer me home.\"\n];\n\nd3.select(\"#viz\")\n    .selectAll('p')\n    .data(poemVerses)\n    .enter()\n  .append('p')\n    .text('Click Me!')\n    .on('click', function(d, i) {\n      d3.select(this).text(d);\n    })\n```\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/typescript/1-TS-Introduction":{"title":"1 TS Introduction","content":"#course_codecademy-typescript #typescript\n\n## Introduction\n\n- JavaScript was designed to be flexible and easy to use for small applications. However, this makes the language less ideal for building larger applications.\n- Stricter programming languages will inform the developer whenever a change in one area of code will break other areas. This doesn't happen in JavaScript, leading to confusing issues at runtime.\n\n- Microsoft developed TypeScript in 2012 as a way of blending the flexibility of JavaScript with stricter languages (i.e. it is strongly typed).\n    - TypeScript adds types to JavaScript allowing us to clarify the structure of and help refactor our code if need be.\n    - In addition, TypeScript adds arrow functions and classes to the language years before they were added to JavaScript.\n\n## What is TypeScript?\n\n- TypeScript code is a *superset* of JavaScript code, so it contains everything that JavaScript has, but adds more functionality to it.\n- We write TypeScript code in files with the `.ts` extension. This is then run through the TypeScript *transpiler* which checks the codes for errors and code that doesn't adhere to TypeScript's standards.\n    - Assuming everything goes to plan, the transpiler will output a JavaScript version of the file (`.js`).\n    - You can run the transpiler with the `tsc` bash command on a `.ts` script. Run the resulting `.js` file with the `node filehere.js` command.\n\nFor example, this gets compiled toâ€¦\n\n```ts\nlet firstName = 'Anders';\n```\n\nâ€¦ this (which is exactly the same since there's no TypeScript-specific code here)\n\n```js\nlet firstName =Â 'Anders';\n```\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/typescript/2-TS-Type-Inferences":{"title":"2 TS Type Inferences","content":"#course_codecademy-typescript #typescript \n\nJavaScript lets us assign any value to any variable which makes it flexible and easy to use in the first instance. However, assigning different value *types* to a variable throughout a script can lead to buggy and difficult to understand code.\n\nTypeScript makes it so that a value's type can never be changed once initially set. This is an example of *type inference* - TypeScript expects the type of the variable to match that of the value it was initially assigned. TS recognises JavaScript's primitive types:\n\n- boolean\n- number\n- null\n- string\n- undefined\n\n```ts\nlet order = 'first';  \n  \norder = 1;\n\n// Type 'number' is not assignable to type 'string'\n//  - This is because TS expects `order` to be a string, not a numeric\n```\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/typescript/3-TS-Type-Shapes":{"title":"3 TS Type Shapes","content":"#course_codecademy-typescript #typescript \n\nSince TS knows what *type* our objects are, it also knows what *shape* they take. The *shape* of an object describes (amongst other things) what methods and properties it may contain. For example:\n\n- All strings are known to have `.length` property and a `.toLowerCase()` method\n\nTS's transpiler will let you know if you're trying to access a property or method that doesn't exist on that object's type.\n\n```ts\n\"MY\".toLowercase();  \n// Property 'toLowercase' does not exist on type '\"MY\"'.  \n// Did you mean 'toLowerCase'?\n```\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]},"/web-dev-programming/javascript/typescript/4-TS-Variable-Type-Annotations":{"title":"4 TS Variable Type Annotations","content":"#course_codecademy-typescript #typescript \n\nWe can assert the type of a variable upon initialisation by using a *type annotation*. We do this with the `:` symbol.\n\n```ts\nlet mustBeAString: string;  \nmustBeAString = 'Catdog';  \n  \nmustBeAString = 1337;\n// Error: Type 'number' is not assignable to type 'string'\n\nvar age: number = 32; // number variable\nvar name: string = \"John\";// string variable\nvar isUpdated: boolean = true;// Boolean variable\n```\n\nThese are automatically removed when TS transpiles this to JavaScript.\n\n## Any\n\nThere are some situations when TS won't try to infer a type, generally when a variable is initialised without assigning a value to it. In cases where TS isn't able to infer a type, it will give the variable the type `any`.\n\nThese variables can be re-assigned any type later on with no errors.\n\n```ts\nlet onOrOff;  \n  \nonOrOff = 1;  \nonOrOff = false;\n```\n","lastmodified":"2023-08-18T01:52:42.965989868Z","tags":[]}}