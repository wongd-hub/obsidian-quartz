<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="#course_google-deepmind-reinforcement-learning #reinforcement-learning
Markov processes  Markov decision processes (MDPs) formally describe an environment for machine learning  We&rsquo;ll start with the nice case where the environment is fully observable."><meta property="og:title" content="2 RL Markov Decision Processes"><meta property="og:description" content="#course_google-deepmind-reinforcement-learning #reinforcement-learning
Markov processes  Markov decision processes (MDPs) formally describe an environment for machine learning  We&rsquo;ll start with the nice case where the environment is fully observable."><meta property="og:type" content="website"><meta property="og:image" content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><meta property="og:url" content="https://wongd-hub.github.io/obsidian-quartz/statistics/reinforcement-learning/2-RL-Markov-Decision-Processes/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="2 RL Markov Decision Processes"><meta name=twitter:description content="#course_google-deepmind-reinforcement-learning #reinforcement-learning
Markov processes  Markov decision processes (MDPs) formally describe an environment for machine learning  We&rsquo;ll start with the nice case where the environment is fully observable."><meta name=twitter:image content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><title>2 RL Markov Decision Processes</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://wongd-hub.github.io/obsidian-quartz//icon.png><link href=https://wongd-hub.github.io/obsidian-quartz/styles.19109a40042e9f0e72e952fda4442a34.min.css rel=stylesheet><link href=https://wongd-hub.github.io/obsidian-quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://wongd-hub.github.io/obsidian-quartz/js/darkmode.df266a900377a99b203da11b154c20ec.min.js></script>
<script src=https://wongd-hub.github.io/obsidian-quartz/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://wongd-hub.github.io/obsidian-quartz/",fetchData=Promise.all([fetch("https://wongd-hub.github.io/obsidian-quartz/indices/linkIndex.9ec627c56be251fbc5efeba39b537935.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://wongd-hub.github.io/obsidian-quartz/indices/contentIndex.ea4342dde016081d86b60fe7a78e5a5c.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://wongd-hub.github.io/obsidian-quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://wongd-hub.github.io/obsidian-quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/wongd-hub.github.io\/obsidian-quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=wongd-hub.github.io/obsidian-quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://wongd-hub.github.io/obsidian-quartz/>The Second ðŸ§ </a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>2 RL Markov Decision Processes</h1><p class=meta>Last updated
Aug 19, 2023
<a href=https://github.com/wongd-hub/obsidian-quartz/tree/hugo/content/statistics/reinforcement-learning/2%20RL%20Markov%20Decision%20Processes.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#markov-processes>Markov processes</a><ol><li><a href=#state-transition-matrix>State transition matrix</a></li><li><a href=#markov-chains>Markov chains</a></li></ol></li><li><a href=#markov-reward-processes>Markov reward processes</a><ol><li><a href=#return>Return</a></li></ol></li><li><a href=#value-function>Value function</a></li><li><a href=#bellman-equation-for-mrps>Bellman equation for MRPs</a></li></ol></nav></details></aside><p>#course_google-deepmind-reinforcement-learning #reinforcement-learning</p><a href=#markov-processes><h2 id=markov-processes><span class=hanchor arialabel=Anchor># </span>Markov processes</h2></a><ul><li><em>Markov decision processes</em> (MDPs) formally describe an environment for machine learning<ul><li>We&rsquo;ll start with the nice case where the environment is <em>fully observable</em>. The agent gets all the information from the environment.</li><li>Almost all RL problems can be formalised as MDPs<ul><li>Optimal control deals with continuous MDPs</li><li>Any partially observable problems can be converted into MDPs</li><li>Bandits (common formalism, you get a set of actions, you perform one action, get rewarded and the round ends) are MDPs with one state</li></ul></li></ul></li></ul><p>As discussed in the <a href=/obsidian-quartz/statistics/reinforcement-learning/1-RL-Introduction-to-Reinforcement-Learning#c3f6cb rel=noopener class=internal-link data-src=/obsidian-quartz/statistics/reinforcement-learning/1-RL-Introduction-to-Reinforcement-Learning>previous lecture</a>:</p><blockquote class=info-callout><p>Definition - Recap: Markov State
A state $S_t$ is <em>Markov</em> iff:
$$\mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1, \dots, S_t]$$
In other words, the probability of the next state given the state you&rsquo;re in is the same as if you showed all of the previous states to the system.</p><p>i.e. The current state is a sufficient statistic of the future</p></blockquote><a href=#state-transition-matrix><h3 id=state-transition-matrix><span class=hanchor arialabel=Anchor># </span>State transition matrix</h3></a><p>For a Markov state $s$ and a successor state $s&rsquo;$, the <em>state transition probability</em> is defined by $\mathscr{P}<em>{ss&rsquo;}=\mathbb{P}\left[S</em>{t+1}=s&rsquo;|S_t=s
ight]$.</p><ul><li>Recall that $S_t$ characterises everything about what happens next (Markov by definition), that should mean that there is some well-defined transition probability that i will transition to another state.</li></ul><p>From this, we can produce a matrix which contains all transition probabilities from one state (rows) to another (columns). Note that each row of the matrix sums to 1.
$$
\mathscr{P} = egin{bmatrix}
\mathscr{P}<em>{11} & \dots & \mathscr{P}</em>{1n}</p><p>dots & \ddots & dots<br>\mathscr{P}<em>{n1} & \dots & \mathscr{P}</em>{nn}
nd{bmatrix}
$$
We can now keep sampling from this matrix to move through states.</p><a href=#markov-chains><h3 id=markov-chains><span class=hanchor arialabel=Anchor># </span>Markov chains</h3></a><ul><li>Formally, a Markov process is a memoryless random process. i.e. A sequence of random states $S_1, S_2, \dots$ with the Markov property</li><li>All that&rsquo;s needed to define this is a state space $\mathscr{S}$ and the set of transition properties $\mathscr{P}$. This fully defines the dynamics of the whole system.</li></ul><blockquote class=info-callout><p>Definition: Markov process
A Markov process or a Markov chain is a tuple ($\mathscr{S}$, $\mathscr{P}$)</p><ul><li>$\mathscr{S}$ is a finite set of states</li><li>$\mathscr{P}$ is a state transition probability matrix $\mathscr{P}<em>{ss&rsquo;}=\mathbb{P}\left[S</em>{t+1}=s&rsquo;|S_t=s
ight]$</li></ul></blockquote><blockquote class=tip-callout><p>Example: Student behaviour
<a class="internal-link broken">Pasted image 20230818154209.png</a>
We can take <em>samples</em> from this Markov chain which just means following an agent through a sequence of states until they hit a <em>terminal</em> state (one with no exit probabilities).</p><p><a class="internal-link broken">Pasted image 20230818154408.png</a>
See above the state transition matrix for this particular Markov process.</p></blockquote><a href=#markov-reward-processes><h2 id=markov-reward-processes><span class=hanchor arialabel=Anchor># </span>Markov reward processes</h2></a><ul><li>A <em>Markov reward process</em> is a Markov process with value judgements, i.e. we add two things to our Markov process: a <em>reward function</em> and a <em>discount factor</em>.</li></ul><blockquote class=info-callout><p>Definition: Markov reward process
A Markov process or a Markov chain is a tuple ($\mathscr{S}$, $\mathscr{P}$, <mark>$\mathscr{R}$</mark>, <mark>$\gamma$</mark>)</p><ul><li>$\mathscr{S}$ is a finite set of states</li><li>$\mathscr{P}$ is a state transition probability matrix $\mathscr{P}<em>{ss&rsquo;}=\mathbb{P}\left[S</em>{t+1}=s&rsquo;|S_t=s
ight]$</li><li>==$\mathscr{R}$ is a reward function $\mathscr{R}<em>s=\mathbb{E}[R</em>{t+1}|S_t=s]$==. i.e. If we&rsquo;re at time $t$ and state $s$, then our immediate reward (that we get at the next time step) is $\mathscr{R}_s$.</li><li><mark>$\gamma$ is a discount factor, $\gamma \in [0, 1]$</mark></li></ul></blockquote><blockquote class=tip-callout><p>Example: Student MRP
<a class="internal-link broken">Pasted image 20230818160241.png</a>
Say you get negative rewards from Facebook and going to class. What we really care about is the total reward we get across the whole sample.</p></blockquote><a href=#return><h3 id=return><span class=hanchor arialabel=Anchor># </span>Return</h3></a><ul><li>The <em>return</em> is what we want to maximise.<ul><li>Most Markov reward and decision processes are discounted.<ul><li>Mathematically convenient to do so and avoids infinite returns in cyclic Markov processes</li><li>There is more uncertainty in the future due to the fact that we don&rsquo;t have a perfect model of the environment</li><li>Natural in financial settings since
<a href=https://www.investopedia.com/articles/03/082703.asp rel=noopener>money earned now will earn more interest than money earned later</a></li><li>There is a natural preference for immediate rewards over future rewards in human/animal behaviour</li><li>It is possible to use un-discounted Markov processes (i.e. $\gamma=1$) if we know that all sequences end in a terminal node</li></ul></li></ul></li></ul><blockquote class=info-callout><p>Definition: Return
The <em>return</em> $G_t$ is the total <em>discounted</em> return from time-step $t$
$$G_t=R_{t+1}+\gamma R_{t+2}+\dots=\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
$\gamma \in [0, 1]$, our discount factor, tells us how much you care now about rewards you get in the future. In other words, the value of receiving $R$ in $k+1$ time-steps is $\gamma^kR$</p><ul><li>$\gamma = 0$ means you care only about immediate returns</li><li>$\gamma = 1$ means you care about future returns equally as much as present returns</li></ul></blockquote><a href=#value-function><h2 id=value-function><span class=hanchor arialabel=Anchor># </span>Value function</h2></a><ul><li>This is the central quantity we&rsquo;re interested in and is the total reward that you expect to get from a given state onwards.<ul><li>In other words, $v(s)$ gives the long-term value of state $s$</li></ul></li></ul><blockquote class=info-callout><p>Definition: State value function
The <em>state value function</em> $v(s)$ of an MRP is the expected return starting from state $s$
$$v(s)=\mathbb{E}[G_t|S_t=s]$$
This has to be an expectation because the environment we&rsquo;re in (a Markov process) is stochastic.</p></blockquote><blockquote class=tip-callout><p>Example: Student Markov Chain returns
<a class="internal-link broken">Pasted image 20230819230607.png</a>
This is the return starting at time step 1. How would we find the value (i.e. the expected return from time step 1 onwards)? We could take a bunch of samples starting in the first state and average out all possible returns.</p><p>Now consider the case where $\gamma=0$; i.e. we are maximally short-sighted and don&rsquo;t care about future returns. Here&rsquo;s what the state-value function might look like for our example. The value function at each state is equal to the reward at each state since we only care what we get in the immediate time step.</p><p><a class="internal-link broken">Pasted image 20230819233220.png</a></p><p>The scenario where $\gamma=0.9$ leads to different values at each state since we&rsquo;re now accounting for returns at other states as well as their transition probabilities.</p><p><a class="internal-link broken">Pasted image 20230819233526.png</a></p></blockquote><a href=#bellman-equation-for-mrps><h2 id=bellman-equation-for-mrps><span class=hanchor arialabel=Anchor># </span>Bellman equation for MRPs</h2></a><p>This is a fundamental relationship in RL</p><ul><li>The idea is that the value function obeys a recursive decomposition. You can take a sequence of rewards (the value function) and break it up into two parts:<ul><li>The immediate reward ($R_{t+1}$)</li><li>Discounted value of the value function from the next state onwards ($\gamma v(S_{t+1})$)</li></ul></li></ul><p>$$
egin{align*}
v(s) & = \mathbb{E} [G_t | St = s]</p><p>& = \mathbb{E} \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \mid St = s
ight]<br>& = \mathbb{E} \left[ R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \ldots) \mid St = s
ight]<br>& = \mathbb{E} \left[ R_{t+1} + \gamma G_{t+1} \mid St = s
ight]<br>& = \mathbb{E} \left[ R_{t+1} + \gamma v(S_{t+1}) \mid St = s
ight]
nd{align*}
$$</p><blockquote class=tip-callout><p>The lecturer explains here that the use of the $t+1$ index to indicate immediate reward (i.e. $R_{t+1}$) comes from the idea that the agent&rsquo;s action must be seen by the environment before the environment updates its state and provides a reward to the agent. So you get the reward for your current action in the next time step. He also notes that this might be slightly inconsistent in the slides since there are different conventions surrounding this.</p></blockquote></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://wongd-hub.github.io/obsidian-quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Darren Wong using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://wongd-hub.github.io/obsidian-quartz/>Home</a></li><li><a href=https://www.herdmentality.xyz/>Herd Mentality (Blog)</a></li><li><a href=https://github.com/wongd-hub>GitHub</a></li></ul></footer></div></div></body></html>