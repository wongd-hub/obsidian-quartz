<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="#course_google-deepmind-reinforcement-learning #reinforcement-learning
 Slide link
Introduction  Dynamic programming is one of the ways in in which we solve Markov Decision Processes."><meta property="og:title" content="3 RL Planning by Dynamic Programming"><meta property="og:description" content="#course_google-deepmind-reinforcement-learning #reinforcement-learning
 Slide link
Introduction  Dynamic programming is one of the ways in in which we solve Markov Decision Processes."><meta property="og:type" content="website"><meta property="og:image" content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><meta property="og:url" content="https://wongd-hub.github.io/obsidian-quartz/statistics/reinforcement-learning/3-RL-Planning-by-Dynamic-Programming/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="3 RL Planning by Dynamic Programming"><meta name=twitter:description content="#course_google-deepmind-reinforcement-learning #reinforcement-learning
 Slide link
Introduction  Dynamic programming is one of the ways in in which we solve Markov Decision Processes."><meta name=twitter:image content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><title>3 RL Planning by Dynamic Programming</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://wongd-hub.github.io/obsidian-quartz//icon.png><link href=https://wongd-hub.github.io/obsidian-quartz/styles.19109a40042e9f0e72e952fda4442a34.min.css rel=stylesheet><link href=https://wongd-hub.github.io/obsidian-quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://wongd-hub.github.io/obsidian-quartz/js/darkmode.df266a900377a99b203da11b154c20ec.min.js></script>
<script src=https://wongd-hub.github.io/obsidian-quartz/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://wongd-hub.github.io/obsidian-quartz/",fetchData=Promise.all([fetch("https://wongd-hub.github.io/obsidian-quartz/indices/linkIndex.ca75706ed504e375f6d58451c5ad7b93.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://wongd-hub.github.io/obsidian-quartz/indices/contentIndex.2fd49916e5d0125a50ba1759b26c63da.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://wongd-hub.github.io/obsidian-quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://wongd-hub.github.io/obsidian-quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/wongd-hub.github.io\/obsidian-quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=wongd-hub.github.io/obsidian-quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://wongd-hub.github.io/obsidian-quartz/>The Second ðŸ§ </a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>3 RL Planning by Dynamic Programming</h1><p class=meta>Last updated
Oct 9, 2023
<a href=https://github.com/wongd-hub/obsidian-quartz/tree/hugo/content/statistics/reinforcement-learning/3%20RL%20Planning%20by%20Dynamic%20Programming.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a><ol><li><a href=#what-is-dynamic-programming>What is dynamic programming?</a></li><li><a href=#planning-by-dynamic-programming>Planning by dynamic programming</a></li></ol></li><li><a href=#policy-evaluation>Policy evaluation</a><ol><li><a href=#iterative-policy-evaluation>Iterative policy evaluation</a></li></ol></li></ol></nav></details></aside><p>#course_google-deepmind-reinforcement-learning #reinforcement-learning</p><p><a href=https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf rel=noopener>Slide link</a></p><a href=#introduction><h2 id=introduction><span class=hanchor arialabel=Anchor># </span>Introduction</h2></a><ul><li>Dynamic programming is one of the ways in in which we solve Markov Decision Processes. It is one of the fundamental building blocks behind RL.</li><li>We&rsquo;ll walk through three basic paradigms in which dynamic programming can be applied:<ul><li><strong>Policy evaluation</strong>: if someone gives you a policy, how can you evaluate how good that policy is?</li><li><strong>Policy iteration</strong>: This is the first method with which we can start to solve the MDP (i.e. find the optimal policy). This essentially takes policy evaluation and turns it into a loop which runs until we find our optimal policy.</li><li><strong>Value iteration</strong>: A related approach, which works directly on value functions and not the policy space. This uses the Bellman equation iteratively until we arrive at the optimal solution.</li></ul></li><li>We&rsquo;ll also go through some extensions to dynamic programming. In subsequent lectures, we&rsquo;ll pull out the key ideas of the three methods above and turn them into more general RL methods.</li></ul><a href=#what-is-dynamic-programming><h3 id=what-is-dynamic-programming><span class=hanchor arialabel=Anchor># </span>What is dynamic programming?</h3></a><blockquote class=info-callout><p>Dynamic programming
&lsquo;<em>Dynamic</em>&rsquo; implies that we&rsquo;re trying to solve a problem that has a sequential or temporal aspect</p><p>&lsquo;<em>Programming</em>&rsquo; refers to mathematical programming (like linear programming), i.e. optimising some program (in our case, the policy - a mapping between states and actions)</p><blockquote class=info-callout><p>Dynamic programming lets us solve complex problems by breaking them down into subproblems and putting them back together to get the overall solution.</p></blockquote><p>As well as in RL, the method of dynamic programming is applied in many fields:</p><ul><li>Scheduling algorithms</li><li>Graph algorithms (e.g. shortest path)</li><li>Graphical models (e.g. Viterbi algorithm)</li><li>Bioinformatics (e.g. lattice models)</li></ul></blockquote><p>This is a general solution method for problems that have two properties:</p><ul><li><p><strong>Optimal substructure</strong> - i.e. the principle of optimality applies - the optimal solution can be decomposed into subproblems ^3a68c9</p><ul><li>This means that you can solve some overall problem by breaking it down into pieces, and the optimal solution to those pieces tells you how to get the optimal solution to your overall problem.</li><li>If this didn&rsquo;t apply, it would be fruitless to solve your subproblems since they won&rsquo;t take you any closer to solving the overall problem.</li><li>An example is the shortest path problem. The shortest path between you and some point X can be broken down into the shortest path between you and some closer point Y, then the shortest path between Y and X.</li></ul></li><li><p><strong>Overlapping subproblems</strong> - i.e. recurring subproblems, which means that solving one subproblem helps you solve others</p><ul><li>This means that we can cache our solutions to these subproblems and reuse them.</li></ul></li><li><p><strong>Markov Decision Processes satisfy both properties</strong>.</p><ul><li>The Bellman equation provides a recursive decomposition and tells us how to break down the optimal value function into the optimal behaviour for one step and the optimal behaviour after that step. This means MDPs satisfy the principle of optimality.</li><li>The value function allows us to cache and reuse information we&rsquo;ve figured out about the MDP. From any state, the value function tells you the best way to behave from that state onwards. Once you have this, you can work your way backwards without having to re-compute the rest of the states.</li></ul></li></ul><a href=#planning-by-dynamic-programming><h3 id=planning-by-dynamic-programming><span class=hanchor arialabel=Anchor># </span>Planning by dynamic programming</h3></a><p>The focus of this lecture will be <em>planning</em> via dynamic programming.</p><ul><li><p>Recall that planning is not the full RL problem, but is where someone tells you the structure of an MDP, and given the knowledge of how the environment works we try to solve for the perfect behaviour.</p></li><li><p>We can solve two cases of planning in the MDP</p><ul><li>We can plan to solve the <em>prediction</em> problem where the input is the MDP ($\langle \mathscr{S}, \mathscr{A}, \mathscr{P}, \mathscr{R}, \gamma
angle$) or Markov Reward Process and a policy $\pi$. The output of the planning is the value function $v_\pi$, specifying how much you&rsquo;re going to get from each state. This is policy evaluation.<ul><li>Recall that an MDP can be specified by its state and action spaces, the reward function, and the discounting factor</li></ul></li><li>We can plan to solve the <em>control</em> problem where we&rsquo;re trying to figure out the best thing to do in the MDP (i.e. full optimisation). Again, someone tells you the full MDP but no policy, as the output of planning here will be the optimal value function, $v_<em>$. Knowing the optimal value function implies that we&rsquo;ll also know the optimal policy $\pi_</em>$, which is what we ultimately care about.</li></ul></li></ul><a href=#policy-evaluation><h2 id=policy-evaluation><span class=hanchor arialabel=Anchor># </span>Policy evaluation</h2></a><a href=#iterative-policy-evaluation><h3 id=iterative-policy-evaluation><span class=hanchor arialabel=Anchor># </span>Iterative policy evaluation</h3></a><p>Someone tells you the policy, and you need to evaluate it.</p><ul><li><p><strong>Problem</strong>: evaluate a given policy, $\pi$</p></li><li><p><strong>Solution</strong>: iterative application of Bellman expectation backup</p><ul><li>Note that we use the Bellman expectation equation to do policy evaluation, but later we&rsquo;ll use the Bellman optimality equation to do control.</li></ul></li><li><p>We will apply the Bellman expectation equation iteratively to do this.</p><ul><li>We&rsquo;ll start with some arbitrary initial value (for example 0) for all states in the MDP (call this a vector, $v_1$).</li><li>We&rsquo;ll then apply our Bellman expectation equation in a one-step lookahead from all states, and using this we&rsquo;ll update the value of each state to a vector, $v_2$.</li><li>Iterate this process many times and we eventually converge on the true value function, $v_\pi$ (this will be proven at the end of the lecture).</li></ul></li></ul><blockquote class=tip-callout><p>Synchronous backups
We use <em>synchronous</em> backups, which means that we consider all states at every iteration. In other words:</p><blockquote><p>At each iteration ($k + 1$), update $v_{k+1}(s)$ from $v_k(s^\prime)$ $ orall \space s \in \mathscr{S}$, where $s^\prime$ is a successor state to $s$.</p></blockquote><p>We will discuss <em>asynchronous</em> backups later.</p></blockquote><p>How are we actually performing this update?
<a class="internal-link broken">Pasted image 20231004215136.png</a></p><ul><li><p>Consider again this one-step lookahead tree. Recall that to find the value of some state $s$ (the root of this tree), we need to:</p><ol><li>Look ahead across all actions we could take from this state</li><li>See what state $s^\prime$ the environment might place us in once we take each of those actions, in our case these are represented by the leaves</li><li>Then back up the value functions of each successor state, the expected value of $v_{k+1}(s)$ is then the sum of these value functions weighted by the probability that you&rsquo;ll end up in them</li></ol></li><li><p>We can see this in equation form (normal and matrix form) below. Note how we are summing - for all possible actions - the probability of taking that action from the current state, multiplied by the immediate return, and the discounted and probability-weighted value functions for all possible successor states.</p><ul><li><mark>In other words, we define the value at the next iteration by plugging in the previous iteration&rsquo;s value functions at the leaves and backing those up to the root.</mark></li><li>We do this for every single state in the MDP at every iteration.</li><li>Note also that since this is the <em>expectation</em> equation, we are using a sum, not a max.
$$v_{k+1}(s) = \sum_{a \in \mathscr{A}} \pi(a|s) \left( \mathscr{R}^a_s + \gamma \sum_{s&rsquo; \in \mathscr{S}} \mathscr{P}_{ss&rsquo;}^a v_k(s&rsquo;)
ight)$$
$$\mathbf{v}^{k+1} = \mathscr{R}^\pi + \gamma \mathscr{P}^\pi \mathbf{v}^k$$</li></ul></li></ul><blockquote><p>Small gridworld example - evaluating a random policy
<a class="internal-link broken">Pasted image 20231004221751.png</a></p><p>Consider a small gridworld with a 4 x 4 grid.</p><ul><li>The two shaded states in the edges are terminal states where you no longer get any reward, since rewards are negative, the goal is to get into one of these terminal states.</li><li>The reward for any change in state is -1 ($r = -1$), and a each state you can choose from 4 actions, move north, east, south, or west.<ul><li>But if you choose an action that would take you off the grid, you remain in your current state and still incur a -1 reward.</li></ul></li></ul><p>Based on this, you can think of the optimal policy as tell you how long it&rsquo;ll take you to get to one of the terminal states.</p><ul><li>We will consider a random policy (i.e. $\pi(n|</li></ul></blockquote></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://wongd-hub.github.io/obsidian-quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Darren Wong using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2024</p><ul><li><a href=https://wongd-hub.github.io/obsidian-quartz/>Home</a></li><li><a href=https://www.herdmentality.xyz/>Herd Mentality (Blog)</a></li><li><a href=https://github.com/wongd-hub>GitHub</a></li></ul></footer></div></div></body></html>