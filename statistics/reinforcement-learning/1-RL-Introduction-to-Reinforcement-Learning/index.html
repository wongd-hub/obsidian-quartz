<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="#course_google-deepmind-reinforcement-learning #reinforcement-learning
 Slide link
About reinforcement learning ![[Pasted image 20230816152354.png]]
 Reinforcement learning (RL) sits at the intersection of many different fields of science."><meta property="og:title" content="1 RL Introduction to Reinforcement Learning"><meta property="og:description" content="#course_google-deepmind-reinforcement-learning #reinforcement-learning
 Slide link
About reinforcement learning ![[Pasted image 20230816152354.png]]
 Reinforcement learning (RL) sits at the intersection of many different fields of science."><meta property="og:type" content="website"><meta property="og:image" content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><meta property="og:url" content="https://wongd-hub.github.io/obsidian-quartz/statistics/reinforcement-learning/1-RL-Introduction-to-Reinforcement-Learning/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="1 RL Introduction to Reinforcement Learning"><meta name=twitter:description content="#course_google-deepmind-reinforcement-learning #reinforcement-learning
 Slide link
About reinforcement learning ![[Pasted image 20230816152354.png]]
 Reinforcement learning (RL) sits at the intersection of many different fields of science."><meta name=twitter:image content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><title>1 RL Introduction to Reinforcement Learning</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://wongd-hub.github.io/obsidian-quartz//icon.png><link href=https://wongd-hub.github.io/obsidian-quartz/styles.19109a40042e9f0e72e952fda4442a34.min.css rel=stylesheet><link href=https://wongd-hub.github.io/obsidian-quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://wongd-hub.github.io/obsidian-quartz/js/darkmode.df266a900377a99b203da11b154c20ec.min.js></script>
<script src=https://wongd-hub.github.io/obsidian-quartz/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://wongd-hub.github.io/obsidian-quartz/",fetchData=Promise.all([fetch("https://wongd-hub.github.io/obsidian-quartz/indices/linkIndex.7c496a27b9d489158ff7af8be9224ea8.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://wongd-hub.github.io/obsidian-quartz/indices/contentIndex.0b7a8cdfce38ef21b28487133348ab07.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://wongd-hub.github.io/obsidian-quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://wongd-hub.github.io/obsidian-quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/wongd-hub.github.io\/obsidian-quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=wongd-hub.github.io/obsidian-quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://wongd-hub.github.io/obsidian-quartz/>The Second ðŸ§ </a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>1 RL Introduction to Reinforcement Learning</h1><p class=meta>Last updated
Dec 2, 2023
<a href=https://github.com/wongd-hub/obsidian-quartz/tree/hugo/content/statistics/reinforcement-learning/1%20RL%20Introduction%20to%20Reinforcement%20Learning.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#about-reinforcement-learning>About reinforcement learning</a></li><li><a href=#the-reinforcement-learning-problem>The reinforcement learning problem</a><ol><li><a href=#reward>Reward</a></li><li><a href=#agent-and-the-environment>Agent and the environment</a></li><li><a href=#state>State</a></li></ol></li><li><a href=#inside-an-rl-agent>Inside an RL agent</a><ol><li><a href=#categorising-rl-agents>Categorising RL agents</a></li></ol></li><li><a href=#problems-within-reinforcement-learning>Problems within reinforcement learning</a><ol><li><a href=#learning-and-planning>Learning and planning</a></li><li><a href=#exploration-and-exploitation>Exploration and exploitation</a></li><li><a href=#prediction-and-control>Prediction and control</a></li></ol></li></ol></nav></details></aside><p>#course_google-deepmind-reinforcement-learning #reinforcement-learning</p><p><a href=https://www.davidsilver.uk/wp-content/uploads/2020/03/intro_RL.pdf rel=noopener>Slide link</a></p><a href=#about-reinforcement-learning><h2 id=about-reinforcement-learning><span class=hanchor arialabel=Anchor># </span>About reinforcement learning</h2></a><p><a class="internal-link broken">Pasted image 20230816152354.png</a></p><ul><li><em>Reinforcement learning</em> (RL) sits at the intersection of many different fields of science. Many different fields try to understand the optimal way to make decisions, and these all relate to reinforcement learning.</li></ul><p><a class="internal-link broken">Pasted image 20230816152543.png</a></p><ul><li>How is RL distinct from supervised and unsupervised learning?<ul><li>There is no supervisor, only a reward signal</li><li>Feedback is delayed, possibly by several steps - and is not instantaneous</li><li>Time actually matters, we&rsquo;re looking at sequential processes in which an agent moves throughout a world which means non-i.i.d data</li><li>The agent&rsquo;s actions can influence its environment and therefore the subsequent data it sees</li></ul></li></ul><blockquote class=tip-callout><p>Examples of RL use cases</p><ul><li><strong>Fly stunt manoeuvres in a helicopter</strong> - a reward if the manoeuvre is performed successfully with no crashes</li><li><strong>Defeat the world champion in Backgammon or play Atari games better than humans</strong> - good reward if a game is won<ul><li>Don&rsquo;t even need to tell it the rules of the game, just set up the reward structure</li></ul></li><li><strong>Manage an investment portfolio</strong> - the reward might be financial returns</li><li><strong>Make a humanoid robot walk</strong> - rewards for making progress</li></ul></blockquote><a href=#the-reinforcement-learning-problem><h2 id=the-reinforcement-learning-problem><span class=hanchor arialabel=Anchor># </span>The reinforcement learning problem</h2></a><a href=#reward><h3 id=reward><span class=hanchor arialabel=Anchor># </span>Reward</h3></a><ul><li>A <em>reward</em> $R_t$ - a scalar feedback signal. Indicates how well the agent is doing at step <em>t</em>. The agent&rsquo;s job is to maximise cumulative reward.<ul><li>The overarching goal is to select actions to maximise total future reward.<ul><li>Actions might have long term consequences</li><li>Reward may be delayed</li><li>It may be better to sacrifice immediate reward to gain more long-term reward, strategic thinking is part of the process</li></ul></li><li>What if the goal is to perform an action in the fastest amount of time? Typically what&rsquo;s done is to define the reward as <code>-1</code> per time step.</li><li>You can use multi-faceted reward systems, but you can generally represent these in a summarised fashion as a scalar reward.</li></ul></li></ul><blockquote class=info-callout><p>Definition: Reward Hypothesis
All goals can be described by the <em>maximisation</em> of <em>expected cumulative reward</em>.</p><ul><li>Reinforcement learning is based on this hypothesis.</li></ul></blockquote><blockquote class=tip-callout><p>Examples of $R_t$</p><ul><li><strong>Fly stunt manoeuvres in a helicopter</strong>: +ve for following desired trajectory, (large) -ve for crashing<ul><li>Re-fueling itself might prevent a crash in several hours (stop and lose a small amount of reward for not following path to mitigate a crash later due to no fuel)</li></ul></li><li><strong>Defeat the world champion at Backgammon</strong>: no intermediate rewards, but at end of game give a +/-ve signal based on winning/losing the game</li><li><strong>Manage an investment portfolio signal</strong>: +ve reward for each $ in bank<ul><li>Investment decisions might take months to come to fruition (delayed reward)</li></ul></li><li><strong>Controlling a power station</strong>: +ve reward for producing power, -ve reward for exceeding safety thresholds</li><li><strong>Making a humanoid robot walk</strong>: +ve reward for forward motion, (large) -ve reward for falling over</li><li><strong>Making a car go around a track</strong>: -ve reward for exceeding track limits/moving x% away from designated racing line, -ve reward for every second taken to get around the track<ul><li>The former needs to be big enough that the optimal strategy isn&rsquo;t to exceed track limits to gain time</li></ul></li></ul></blockquote><a href=#agent-and-the-environment><h3 id=agent-and-the-environment><span class=hanchor arialabel=Anchor># </span>Agent and the environment</h3></a><p><a class="internal-link broken">Pasted image 20230816155812.png</a></p><ul><li><p>The brain represents the <em>agent</em>, this is what we&rsquo;re building. Our agent will be composed of algorithms that will be able to respond to stimuli and take actions (such as move controls, or make investments). So at each step the agent will:</p><ul><li>Execute action $A_t$</li><li>Receive observation $O_t$</li><li>Receive scalar reward $R_t$</li></ul></li><li><p>The <em>environment</em> is represented by the world (what&rsquo;s on the other side of the agent). There will be a loop over time, where in every step the agent sees a snapshot of the world at this moment generated by the environment. We have no control over the environment except through the agent&rsquo;s action $A_t$. So the environment will:</p><ul><li>Receive action $A_t$</li><li>Emit observation $O_t$</li><li>Emit scalar reward $R_t$</li></ul></li><li><p>If we have a <em>multi-agent</em> system, the other agents can be seen as part of this environment to any given agent.</p></li><li><p>The <em>history</em> is the sequence of observations, actions, and rewards (all observable variables up to time <em>t</em>; the sensorimotor stream of the agent). This is everything the agent has seen so far.
$$H_T = A_1, O_1, R_1, \dots, A_t, O_t, R_t$$</p></li><li><p>What happens next depends on this history.</p><ul><li>Our goal is to build a mapping (i.e. an algorithm) from one of these histories to picking the next action. the agent will use this to select the next action.</li><li>The environment will select the observations it emits and the rewards it provides at each time stamp.</li><li>The history isn&rsquo;t very useful though because it is typically long, we want to have agents that have long lives that can deal with microsecond interactions; so what we talk about instead is <em>state</em>.</li></ul></li></ul><a href=#state><h3 id=state><span class=hanchor arialabel=Anchor># </span>State</h3></a><ul><li><p><em>State</em> is a summary of the information that&rsquo;s used to determine what happens next. We replace the history with some concise summary with all the info we need to act. Formally, it is a function of the history:
$$S_t=f\left(H_t
ight)$$</p></li><li><p>There are three definitions of state; what do they mean and how do they relate to each other?</p><ul><li><p>The <em>environment</em> state ($S_t^e$); this is the information that&rsquo;s used in the environment to determine what happens next, i.e. the next observation/reward.</p><ul><li>There will be some set of numbers that describes the process that drives what happens next. The summarisation of that which is fed to the agent is the environment state.</li><li>The environment state is not usually visible to the agent; the agent can only see what is provided by the environment in the observation.</li><li>Even if we could see this information, sometimes it may not be the right information we&rsquo;d want to use to make the decision (i.e. does the atomic makeup of the game world help the agent make better decisions? Maybe its own subjective view of the environment may be better)</li></ul></li><li><p>The <em>agent</em> state ($S_t^a$) captures what&rsquo;s happened to the agent so far, summarises it all into useful information, and it uses these numbers to pick the next action.</p><ul><li>Our decision is how to process those observations and what to remember/throw away.</li><li>It is the information used by the RL algorithm and can be any function of the history $S_t^a=\left(H_t
ight)$.</li></ul></li><li><p>The <em>information</em> state (a.k.a. <em>Markov</em> state - a concept from information theory) contains all useful information from history.</p><ul><li>By definition, the environment state is Markov. $S_t^e$ at any given moment is Markov since that&rsquo;s what its using to pick the next observation it will emit and the reward.</li><li>By definition, the history $H_t$ is Markov. If we retain the whole history of everything and make our decisions on the entire history, the entire history contains as much information as the entire history.</li><li>It&rsquo;s always possible to come up with some Markov state, the question is how do we find a useful representation in practice.</li></ul></li></ul></li></ul><blockquote class=info-callout><p>Definition: Markov State
A state $S_t$ is <em>Markov</em> iff:
$$\mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1, \dots, S_t]$$
In other words, the probability of the next state given the state you&rsquo;re in is the same as if you showed all of the previous states to the system.</p><p>i.e. This is a <em>memoryless</em> system. The current state fully characterises the distribution of all future actions/observations/rewards.
$$H_{1:t}
ightarrow S_t
ightarrow H_{t+1:\infty}$$</p></blockquote><p>^c3f6cb</p><blockquote class=tip-callout><p>Example: Helicopter manoeuvres
For the helicopter, its current velocity, orientation, wind speed, etc. may form a rough Markov state for the agent. When it knows this, it doesn&rsquo;t matter where it was before this, from this it knows where it will be in the next moment.</p><p>An imperfect state might be if you only had the position but not the velocity. This is not Markov because from this state, you don&rsquo;t know where it will be in future since you don&rsquo;t know how fast it&rsquo;s moving. You have to look back in time to figure out what its velocity is and what its momentum will be.</p></blockquote><blockquote class=tip-callout><p>Example: The rat
<a class="internal-link broken">Pasted image 20230816163729.png</a>
Take for example, a rat in an experiment. Say that it experiences the three sequences shown above.</p><ul><li>If you were the agent and used the last three items in the sequence as your state, you would believe that you were about to be electrocuted.</li><li>If you chose your agent state to be the count of lights, bells, and levers, you&rsquo;d expect that the cheese would appear.</li><li>If your agent state was the complete sequence we wouldn&rsquo;t know what happens next.</li></ul><p><mark>What we believe will happen next depends on our representation of state.</mark></p></blockquote><ul><li><p>A <em>fully observable environment</em> is one in which the agent <em>directly observes</em> the environment state. This is a nice case, the agent sees all the numbers in the environment state. i.e. $O_t=S_t^a=S_t^e$.</p><ul><li>When we have this situation, this is formally a <em>Markov decision process</em> (MDP). We&rsquo;ll discuss this in the next lecture and the majority of the course.</li></ul></li><li><p><em>Partial observability</em> describes the situation in which the agent <em>indirectly observes</em> the environment. e.g. A robot with camera vision isn&rsquo;t told its absolute location, and poker agent only observes the public cards.</p><ul><li>Now the agent must construct its own state representation $S_t^a$ that&rsquo;s distinct from the environment state. There are many ways to do this:<ul><li><strong>Complete history</strong> - naive approach: $S_t^a=H_t$</li><li><strong>Build beliefs of the environment state</strong> - the Bayesian approach. Don&rsquo;t know what&rsquo;s happening in the environment but going to keep a probability distribution over where we think we are in the environment: $S_t^a = (\mathbb{P}[S_t^e = s^1], \ldots, \mathbb{P}[S_t^e = s^n])$; i.e. we have some probability that the state is $s^1$ or $s^n$, etc</li><li>Recurrent neural network: $S_t^a = \sigma(S_{t-1}^aW_s + O_tW_o)$, take a linear combination of the agent state you had at the last time step with your current observation to generate your new state</li></ul></li><li>Formally, this is a <em>partially observable Markov decision process</em> (POMDP)</li></ul></li></ul><a href=#inside-an-rl-agent><h2 id=inside-an-rl-agent><span class=hanchor arialabel=Anchor># </span>Inside an RL agent</h2></a><p>So far we&rsquo;ve only talked about the problem, not yet how to solve the problem.</p><ul><li><p>An RL agent may include one or more of these components:</p><ul><li><p><strong>Policy</strong>: the agent&rsquo;s <em>behaviour function</em></p><ul><li>A map from state to action.</li><li>Deterministic policy: If we&rsquo;re in some state $s$, we have some policy $\pi$ that then determines our action from that state, i.e. $a=\pi(s)$.</li><li>Stochastic policy: Can be useful, helps us make random, exploratory decisions, i.e. $\pi(a|s)=\mathbb{P}[A=a|S=s]$</li></ul></li><li><p><strong>Value function</strong>: how good is each state and/or action. How much reward do I expect to get if we take this action in this particular state.</p><ul><li>A prediction of expected future reward. This is how we choose between action 1 and action 2.</li><li>Since the value will depend on what your action/policy is, we index by $\pi$: $v_\pi(s)=\mathbb{E}<em>\pi\left[R_t+\gamma R</em>{t+1} + \gamma^2 R_{t+2} + \dots|S_t=s
ight]$<ul><li>i.e. The value function tells us how much total reward we expect going into the future. We can also have discounting ($\gamma$) that goes into the future which helps us care more about immediate rewards if we want.</li></ul></li></ul></li><li><p><strong>Model</strong>: agent&rsquo;s subjective representation of how the environment works. Its sometimes useful to learn the behaviour of the environment and use that model of the environment to help figure out what to do next.</p><ul><li>It is optional to do this; a lot of the course we&rsquo;ll focus on model-free methods that don&rsquo;t use a model at all.</li><li>The way we normally do this is to have two parts of the model:<ul><li>Transition model: $\mathscr{P}$ predicts the next state, predicts the dynamics of the environment. If this were the helicopter, this is the function that would predict where the helicopter would be next given its current state.<ul><li>Formally this is represented in a state transition model that is the probability of being in the next state given the previous state and action: $\mathscr{P}^a_{ss&rsquo;}=\mathbb{P}[S&rsquo;=s&rsquo;|S=s,A=a]$</li></ul></li><li>Reward model: $\mathscr{R}$ predicts the next (immediate) reward. The helicopter can learn that if its in this position then it&rsquo;s not crashing and therefore doing well.<ul><li>Formally this is represented as a function that tells us the expected reward given the previous state and action: $\mathscr{R}^a_s=\mathbb{E}[R | S=s,A=a]$</li></ul></li></ul></li></ul></li></ul></li></ul><blockquote class=tip-callout><p>Example: Maze
<a class="internal-link broken">Pasted image 20230817132701.png</a></p><p>Here the goal is to navigate this maze as quickly as possible, so:</p><ul><li>Reward: -1 for every second taken</li><li>Actions: N, E, S, W</li><li>State: Agent&rsquo;s location in the maze</li></ul><p>An example of policy is like the following. For each state (location), there is an action (the arrows) mapped to it ($\pi(s)$)
<a class="internal-link broken">Pasted image 20230817133000.png</a></p><p>An example of the value function ($v_\pi(s)$), each state has an expected future value attached. You can see that its -1 at the last state since it knows there&rsquo;s only one more step to the end. It can also go higher such as -24 when you&rsquo;ve taken a wrong turn and now it&rsquo;ll take you more steps to get to the end.
<a class="internal-link broken">Pasted image 20230817133131.png</a></p><p>An example of the agent&rsquo;s model of reality is like the following. The agent is trying to build its own map of the environment. The map represents the transition model ($\mathscr{P}^a_{ss&rsquo;}$) and the numbers in each grid represent the expected reward in each state ($\mathscr{R}^a_s$)
<a class="internal-link broken">Pasted image 20231202135742.png</a></p></blockquote><a href=#categorising-rl-agents><h3 id=categorising-rl-agents><span class=hanchor arialabel=Anchor># </span>Categorising RL agents</h3></a><ul><li>We can build a taxonomy of RL agents based on which of the key components our agent contains. The fundamental distinction in RL is whether the algorithm is model-free or model-based. Second to this is whether a policy or value function is used.<ul><li><strong>Value based</strong>: stores a value function. If it&rsquo;s got a value function then the policy is implicit - it just needs to pick actions greedily with respect to the value function</li><li><strong>Policy based</strong>: we explicitly represent the policy and the agent will work on creating a policy that maximises the total cumulative reward - without ever storing an explicit value function</li><li><strong>Actor Critic</strong>: stores both the policy and the value function</li><li><strong>Model Free</strong>: has a policy and/or value; we do not try to explicitly understand the dynamics of the environment, we just see our policy/rewards and base our actions on that</li><li><strong>Model Based</strong>: first step is to build a dynamics model of how the environment works. Has a policy and/or value</li></ul></li></ul><p><a class="internal-link broken">Pasted image 20230817134408.png</a></p><a href=#problems-within-reinforcement-learning><h2 id=problems-within-reinforcement-learning><span class=hanchor arialabel=Anchor># </span>Problems within reinforcement learning</h2></a><a href=#learning-and-planning><h3 id=learning-and-planning><span class=hanchor arialabel=Anchor># </span>Learning and planning</h3></a><p>Two fundamental problems in sequential decision making</p><ul><li><p>Reinforcement Learning:</p><ul><li>The environment is unknown; the agent isn&rsquo;t told how the environment works.</li><li>The agent interacts with the environment with the aim of getting the most cumulative reward, trial and error.</li><li>The agent improves its policy.</li></ul></li><li><p>Planning:</p><ul><li>A model of the environment is known, we tell the agent this</li><li>Instead of interacting with the environment, the agent performs internal computations with its model</li><li>As a result of this, the agent improves its policy.</li></ul></li></ul><p>You could also learn how the environment works, and then do planning</p><blockquote class=tip-callout><p>Example: Atari
Reinforcement Learning setup: Rules of the game are unknown, the agent learns directly from interactive gameplay, picks actions on the joystick and sees pixels and scores.
<a class="internal-link broken">Pasted image 20230817135332.png</a></p><p>Planning setup: Rules of the game are known and told to the agent, can query an emulator (consider this a perfect model in the agent&rsquo;s brain) to plan its next step using look-ahead or tree search.
<a class="internal-link broken">Pasted image 20230817135436.png</a></p></blockquote><a href=#exploration-and-exploitation><h3 id=exploration-and-exploitation><span class=hanchor arialabel=Anchor># </span>Exploration and exploitation</h3></a><p>How do we balance exploration and exploitation?</p><ul><li><p>Reinforcement learning is like trial and error learning.</p><ul><li>The problem is that the agent is losing reward along the way.</li><li>We want to figure out a good policy without giving up opportunities to exploit the things it has discovered.</li></ul></li><li><p>Exploration finds more information about the environment, giving up reward to do so. Could you potentially gain more reward by trying something you haven&rsquo;t done yet?</p></li><li><p>Exploitation exploits known information to maximise reward</p></li><li><p>It is important to do both, the question is what is the best balance?</p></li></ul><blockquote class=tip-callout><p>Examples</p><ul><li>Restaurant Selection<ul><li><em>Exploitation</em>: go to your favourite restaurant</li><li><em>Exploration</em>: Try a new restaurant - you&rsquo;ll never know if you&rsquo;ll find</li></ul></li><li>Online Banner Advertisements<ul><li><em>Exploitation</em>: Show the most successful advert</li><li><em>Exploration</em>: Show a different advert - which might be more successful</li></ul></li></ul></blockquote><a href=#prediction-and-control><h3 id=prediction-and-control><span class=hanchor arialabel=Anchor># </span>Prediction and control</h3></a><ul><li>Prediction: evaluate the future given a policy</li><li>Control: what is the optimal policy, what policy should you choose?</li></ul><p>Typically you need to solve the prediction problem in order to solve the control problem</p><blockquote class=tip-callout><p>Example: GridWorld
Prediction setup: if we perform a fixed policy of moving randomly across the grid, how much reward will we get?
<a class="internal-link broken">Pasted image 20230817141004.png</a></p><p>Control setup: whats the optimal behaviour of this GridWorld? If I behave optimally, now what&rsquo;s the value function? This is very different to the prediction problem
<a class="internal-link broken">Pasted image 20230817141117.png</a></p></blockquote></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://wongd-hub.github.io/obsidian-quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Darren Wong using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2025</p><ul><li><a href=https://wongd-hub.github.io/obsidian-quartz/>Home</a></li><li><a href=https://www.herdmentality.xyz/>Herd Mentality (Blog)</a></li><li><a href=https://github.com/wongd-hub>GitHub</a></li></ul></footer></div></div></body></html>