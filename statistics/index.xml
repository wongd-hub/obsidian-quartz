<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistics on</title><link>https://wongd-hub.github.io/obsidian-quartz/statistics/</link><description>Recent content in Statistics on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://wongd-hub.github.io/obsidian-quartz/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/course-frontmatters/Course-Inferential-Statistics-Coursera/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/course-frontmatters/Course-Inferential-Statistics-Coursera/</guid><description>#coursera-inferential-stats #moc
Instructor: Mine Çetinkaya-Rundel | Link to course
About this Course This course covers commonly used statistical inference methods for numerical and categorical data.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/course-frontmatters/Textbook-Introduction-to-Statistical-Learning-2nd-edn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/course-frontmatters/Textbook-Introduction-to-Statistical-Learning-2nd-edn/</guid><description>#textbook-intro-to-statistical-learning #moc
Authors: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani | Website
Table of contents Introduction [[1.1 An Overview of Statistical Learning]] [[1.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/forecasting-principles-and-practice/1.1-What-can-be-forecast/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/forecasting-principles-and-practice/1.1-What-can-be-forecast/</guid><description/></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/1-Central-Limit-Theorem-and-Sampling-Distributions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/1-Central-Limit-Theorem-and-Sampling-Distributions/</guid><description>#coursera-inferential-stats #statistics
Table of contents [[#Introduction]] [[#Sampling variability and Central Limit Theorem]] [[#Sampling distribution]] [[#The Central Limit Theorem]] [[#CLT (for the mean), further examples]] Introduction We start this course off with an example study by Pew Research which quotes:</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/2-Confidence-Intervals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/2-Confidence-Intervals/</guid><description>#coursera-inferential-stats #statistics
Table of contents [[#Introduction]] [[#Confidence interval (for a mean)]] [[#Accuracy vs. Precision (of confidence intervals)]] [[#Required sample size for Margin of Error]] [[#CI (for the mean) examples]] Introduction A plausible range of values for the population parameter is called a confidence interval.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/3-Hypothesis-Testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/3-Hypothesis-Testing/</guid><description>#coursera-inferential-stats #statistics
Table of contents [[#Another Introduction to Inference]] [[#Hypothesis Testing (for a mean)]] [[#HT for the mean examples]] Another Introduction to Inference We&amp;rsquo;ll start with an example of a first-principles hypothesis test that the last course ended on.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/4-Statistical-Significance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/4-Statistical-Significance/</guid><description>#coursera-inferential-stats #statistics
Table of contents [[#Inference for Other Estimators]] [[#Decision Errors]] [[#Significance vs. Confidence Level]] [[#Statistical vs. Practical Significance]]] Inference for Other Estimators The methods we&amp;rsquo;ve been learning can be applied to other estimators that have nearly-normal sampling distributions, which are listed here:</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/5-t-distribution-and-Comparing-Two-Means/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/5-t-distribution-and-Comparing-Two-Means/</guid><description>#coursera-inferential-stats #statistics
Table of contents [[#Inference for Numerical Variables]] [[#t-distribution]] [[#Inference for a mean]] [[#Inference for comparing two independent means]] [[#Inference for comparing two paired means]] [[#Power]] Inference for Numerical Variables We&amp;rsquo;ll build upon the tools we&amp;rsquo;ve used to perform inference on numerical variables in this unit.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/6-ANOVA-and-Bootstrapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/6-ANOVA-and-Bootstrapping/</guid><description>#coursera-inferential-stats #statistics
Table of contents [[#Comparing more than two means]] [[#ANOVA]] [[#Conditions for ANOVA]] [[#Multiple comparisons]] [[#Bootstrapping]] Comparing more than two means The data we&amp;rsquo;ll consider here comes from the General Social Survey.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/7-Inference-for-Proportions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/7-Inference-for-Proportions/</guid><description>#coursera-inferential-stats #statistics
Table of contents [[#Introduction]] [[#Sampling variability and CLT for proportions]] [[#Confidence interval for a proportion]] [[#Hypothesis test for a proportion]] [[#Estimating the difference between two proportions]] [[#Hypothesis test for comparing two proportions]] Introduction This week we discuss categorical variables where the metric of interest is a proportion (e.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/8-Simulation-based-inference-for-proportions-and-chi-square-testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/8-Simulation-based-inference-for-proportions-and-chi-square-testing/</guid><description>#coursera-inferential-stats #statistics
Table of contents [[#Small sample proportions]] [[#Examples]] [[#Comparing two small sample proportions]] [[#Chi-Square GOF test]] [[#Chi-Square independence test]] Small sample proportions If the success-failure ≥ 10 rule (i.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/1-Introduction/1.1-An-Overview-of-Statistical-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/1-Introduction/1.1-An-Overview-of-Statistical-Learning/</guid><description>#textbook-intro-to-statistical-learning
Statistical learning refers to a set of tools for understanding data. Can be [[2.1.4 Supervised Versus Unsupervised Learning#^31a009|supervised]] (predicting/estimating an output using inputs) or [[2.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/1-Introduction/1.2-Notation-and-Simple-Matrix-Algebra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/1-Introduction/1.2-Notation-and-Simple-Matrix-Algebra/</guid><description>#textbook-intro-to-statistical-learning
General notation
$n$ denotes the number of distinct data points available $p$ denotes the number of variables that are available for use Variable names are referred to with coloured monospace font Feature space</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1-What-is-Statistical-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1-What-is-Statistical-Learning/</guid><description>#textbook-intro-to-statistical-learning
For any dataset, we assume there&amp;rsquo;s a relationship between $Y$ and $X = (X_1, X_2, \dots, X_p)$ which can be represented in the general form: $$Y = f(x) + \epsilon$$ Where $f$ is a fixed but unknown function of $X_1, \dots, X_p$, and $\epsilon$ is a random error term (independent of $X$ and mean 0).</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.1-Why-Estimate-f/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.1-Why-Estimate-f/</guid><description>#textbook-intro-to-statistical-learning
Estimating $f$ can be useful for both prediction (predicting the response for future observations - prediction accuracy is paramount here) and inference (better understanding the relationship between the inputs and the output - interpretability is the priority).</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.2-How-Do-We-Estimate-f/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.2-How-Do-We-Estimate-f/</guid><description>#textbook-intro-to-statistical-learning
Techniques for estimating $f$ can be characterised as either parametric or non-parametric. Parametric techniques require that a functional form be assumed for the data, which provides a set of parameters that one needs to estimate using the data.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.3-The-Trade-Off-Between-Prediction-Accuracy-and-Model-Interpretability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.3-The-Trade-Off-Between-Prediction-Accuracy-and-Model-Interpretability/</guid><description>#textbook-intro-to-statistical-learning
Why would we use a more restrictive method instead of a flexible approach? There is often a trade-off between how restrictive a model is and how interpretable it is.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.4-Supervised-Versus-Unsupervised-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.4-Supervised-Versus-Unsupervised-Learning/</guid><description>#textbook-intro-to-statistical-learning
Supervised learning refers to the problem set where for each observation of the predictor measurements ($x_i$, $i=1,\dots,n$) there is an associated response measurement ($y_i$).</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.5-Regression-Versus-Classification-Problems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.1.5-Regression-Versus-Classification-Problems/</guid><description>#textbook-intro-to-statistical-learning
Variables are either quantitative (numerical) or qualitative (categorical). We refer to problems with a quantitative response as regression problems; whereas those with qualitative responses are referred to as classification problems.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2-Assessing-Model-Accuracy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2-Assessing-Model-Accuracy/</guid><description>#textbook-intro-to-statistical-learning
There is no free lunch in statistics. No one method dominates all others over all possible datasets.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.1-Measuring-the-Quality-of-Fit-Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.1-Measuring-the-Quality-of-Fit-Regression/</guid><description>#textbook-intro-to-statistical-learning
We need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.2-The-Bias-Variance-Trade-Off/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.2-The-Bias-Variance-Trade-Off/</guid><description>#textbook-intro-to-statistical-learning
It is possible to show that the expected test MSE (for a given value $x_0$) can be decomposed into the sum of the variance of $\hat{f}(x_0)$, the squared bias of $\hat{f}(x_0)$ and the variance of the irreducible error terms $\epsilon$.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.3-Measuring-the-Quality-of-Fit-Classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.3-Measuring-the-Quality-of-Fit-Classification/</guid><description>#textbook-intro-to-statistical-learning
The concepts discussed so far (like the bias-variance trade-off) apply to both regression and classification problems, albeit with some differences.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.3.1-The-Bayes-Classifier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.3.1-The-Bayes-Classifier/</guid><description>#textbook-intro-to-statistical-learning
The Bayes Classifier minimises the test error rate by assigning each observation to the most likely class, given its predictor values.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.3.2-K-Nearest-Neighbours/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/introduction-to-statistical-learning/2-Statistical-Learning/2.2.3.2-K-Nearest-Neighbours/</guid><description>#textbook-intro-to-statistical-learning
Many methods try to estimate the conditional distribution of Y given X, and then classify an observation to the class with the highest estimated probability.</description></item><item><title/><link>https://wongd-hub.github.io/obsidian-quartz/statistics/predictive-modelling/Farrington-Flexible-Aberration-Detection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://wongd-hub.github.io/obsidian-quartz/statistics/predictive-modelling/Farrington-Flexible-Aberration-Detection/</guid><description>#excess-mortality #regression
A quasi-Poisson regression-based aberration detection algorithm Useful links R surveillance function definition for farringtonFlexible() Official documentation Comparison of statistical algorithms for daily syndromic surveillance aberration detection, Noufaily et al.</description></item></channel></rss>