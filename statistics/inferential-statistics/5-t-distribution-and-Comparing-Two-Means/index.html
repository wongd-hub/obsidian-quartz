<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="#course_coursera-inferential-stats #statistics
Inference for Numerical Variables We&rsquo;ll build upon the tools we&rsquo;ve used to perform inference on numerical variables in this unit."><meta property="og:title" content="5 t-distribution and Comparing Two Means"><meta property="og:description" content="#course_coursera-inferential-stats #statistics
Inference for Numerical Variables We&rsquo;ll build upon the tools we&rsquo;ve used to perform inference on numerical variables in this unit."><meta property="og:type" content="website"><meta property="og:image" content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><meta property="og:url" content="https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/5-t-distribution-and-Comparing-Two-Means/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="5 t-distribution and Comparing Two Means"><meta name=twitter:description content="#course_coursera-inferential-stats #statistics
Inference for Numerical Variables We&rsquo;ll build upon the tools we&rsquo;ve used to perform inference on numerical variables in this unit."><meta name=twitter:image content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><title>5 t-distribution and Comparing Two Means</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://wongd-hub.github.io/obsidian-quartz//icon.png><link href=https://wongd-hub.github.io/obsidian-quartz/styles.19109a40042e9f0e72e952fda4442a34.min.css rel=stylesheet><link href=https://wongd-hub.github.io/obsidian-quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://wongd-hub.github.io/obsidian-quartz/js/darkmode.df266a900377a99b203da11b154c20ec.min.js></script>
<script src=https://wongd-hub.github.io/obsidian-quartz/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://wongd-hub.github.io/obsidian-quartz/",fetchData=Promise.all([fetch("https://wongd-hub.github.io/obsidian-quartz/indices/linkIndex.402b9bb386d7e138dd5867f33a2a51cd.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://wongd-hub.github.io/obsidian-quartz/indices/contentIndex.7f53e721bf8df0330c0701f39cdb4e35.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://wongd-hub.github.io/obsidian-quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://wongd-hub.github.io/obsidian-quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/wongd-hub.github.io\/obsidian-quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=wongd-hub.github.io/obsidian-quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://wongd-hub.github.io/obsidian-quartz/>The Second ðŸ§ </a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>5 t-distribution and Comparing Two Means</h1><p class=meta>Last updated
Aug 17, 2023
<a href=https://github.com/wongd-hub/obsidian-quartz/tree/hugo/content/statistics/inferential-statistics/5%20t-distribution%20and%20Comparing%20Two%20Means.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#inference-for-numerical-variables>Inference for Numerical Variables</a></li><li><a href=#t-distribution>t-distribution</a><ol><li><a href=#conditions>Conditions</a></li><li><a href=#t-distribution-details>t-distribution details</a></li><li><a href=#t-statistic>t-statistic</a></li><li><a href=#origins-of-the-t-distribution>Origins of the t-distribution</a></li></ol></li><li><a href=#inference-for-a-mean>Inference for a mean</a><ol><li></li></ol></li><li><a href=#inference-for-comparing-two-independent-means>Inference for comparing two independent means</a></li><li><a href=#inference-for-comparing-two-paired-means>Inference for comparing two paired means</a></li><li><a href=#power>Power</a><ol><li></li></ol></li></ol></nav></details></aside><p>#course_coursera-inferential-stats #statistics</p><a href=#inference-for-numerical-variables><h2 id=inference-for-numerical-variables><span class=hanchor arialabel=Anchor># </span>Inference for Numerical Variables</h2></a><p>We&rsquo;ll build upon the tools we&rsquo;ve used to perform inference on numerical variables in this unit. We&rsquo;ll be looking at:</p><ul><li>Comparing multiple means against one another (two, or more using <a href=/obsidian-quartz/statistics/inferential-statistics/6-ANOVA-and-Bootstrapping#anova rel=noopener class=internal-link data-src=/obsidian-quartz/statistics/inferential-statistics/6-ANOVA-and-Bootstrapping>ANOVA</a>)</li><li>A technique that allows one to create confidence intervals for estimators that do not adhere to the Central Limit Theorem (such as the median): <a href=/obsidian-quartz/statistics/inferential-statistics/6-ANOVA-and-Bootstrapping#bootstrapping rel=noopener class=internal-link data-src=/obsidian-quartz/statistics/inferential-statistics/6-ANOVA-and-Bootstrapping>bootstrapping</a><ul><li>This is a simulation-based method which does not impose as rigid conditions as using the CLT does</li><li>We&rsquo;ll learn how to bootstrap as well as when to bootstrap</li></ul></li><li>Working with small samples (n &lt; 30) - spoiler: the t-distribution is used here</li></ul><a href=#t-distribution><h2 id=t-distribution><span class=hanchor arialabel=Anchor># </span>t-distribution</h2></a><p>The t-distribution will be useful for describing the distribution of the sample mean when the population $\sigma$ is unknown, which is almost always. Let&rsquo;s discuss the conditions for inference so far, as a motivation for why we need the t-distribution.</p><a href=#conditions><h3 id=conditions><span class=hanchor arialabel=Anchor># </span>Conditions</h3></a><ul><li>What purpose does a large sample serve? As long as your observations are independent, and the population distribution is not extremely skewed, a large sample will ensure that:<ul><li>The sampling distribution of the mean is nearly normal; and,</li><li>The estimate of the standard error is reliable.<ul><li>Recall that $SE = rac{s}{\sqrt{n}}$, where $s$ is the standard deviation of our sample which is our best estimate of the unknown population standard deviation. If our sample is large enough, it&rsquo;ll be more likely that $s$ is a good estimate of the population standard deviation, and therefore the standard error estimation is reliable.</li></ul></li></ul></li></ul><p>What if your sample size is small?</p><ul><li>The uncertainty of the standard error estimate is addressed by using the t-distribution.</li></ul><a href=#t-distribution-details><h3 id=t-distribution-details><span class=hanchor arialabel=Anchor># </span>t-distribution details</h3></a><ul><li><p>When the population $\sigma$ is unknown (almost always), use the t-distribution to address the uncertainty of the standard error estimate</p></li><li><p>Similar to the normal distribution, but with thicker tails, and a less tall modal point</p></li></ul><p><a class="internal-link broken">Pasted image 20230712220139.png</a></p><ul><li>This means that observations are more likely to fall greater than 2 standard deviations from the mean<ul><li>As a result, <a href=/obsidian-quartz/statistics/inferential-statistics/2-Confidence-Intervals rel=noopener class=internal-link data-src=/obsidian-quartz/statistics/inferential-statistics/2-Confidence-Intervals>confidence intervals</a> constructed from the t-distribution will be wider, and therefore more conservative than those from the normal distribution</li></ul></li></ul><p>i.e. these thick tails are helpful for mitigating the effect of a less reliable estimate of the standard error of the sampling distribution.</p><ul><li>The t-distribution is centred at 0 like the normal distribution, and has one parameter - degrees of freedom (df) - which determines the thickness of the tails.<ul><li>As a reminder, the normal distribution has two parameters - the mean and standard deviation</li><li>As the df increases, the t-distribution approaches the normal distribution</li></ul></li></ul><a href=#t-statistic><h3 id=t-statistic><span class=hanchor arialabel=Anchor># </span>t-statistic</h3></a><p>How do we actually use the t-distribution in statistical inference?</p><ul><li><p>Use the t-distribution on a single mean, or for comparing two means, when the population standard deviation is unknown (almost always).</p></li><li><p>Calculate the T-statistic just as you would a Z-score: $T = rac{ ext{obs} - ext{null}}{SE}$, then find the corresponding p-value from the t-distribution</p></li></ul><a href=#examples><h4 id=examples><span class=hanchor arialabel=Anchor># </span>Examples</h4></a><p>We can see that the t-statistic estimate using the same Z-score is closer to the normal distribution estimate when the degrees of freedom are higher. As the degrees of freedom decrease, the tails become heavier, and the p-value associated with a given T-statistic increases.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=c1># P(|Z| &gt; 2)</span>
</span></span><span class=line><span class=cl><span class=nf>pnorm</span><span class=p>(</span><span class=m>-2</span><span class=p>)</span> <span class=o>*</span> <span class=m>2</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 0.0455002638963584</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#P(|t(df = 50)| &gt; 2)</span>
</span></span><span class=line><span class=cl><span class=nf>pt</span><span class=p>(</span><span class=m>-2</span><span class=p>,</span> <span class=n>df</span> <span class=o>=</span> <span class=m>50</span><span class=p>)</span> <span class=o>*</span> <span class=m>2</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 0.0509470687376933</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#P(|t(df = 10)| &gt; 2)</span>
</span></span><span class=line><span class=cl><span class=nf>pt</span><span class=p>(</span><span class=m>-2</span><span class=p>,</span> <span class=n>df</span> <span class=o>=</span> <span class=m>10</span><span class=p>)</span> <span class=o>*</span> <span class=m>2</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 0.0733880347707404</span>
</span></span></code></pre></td></tr></table></div></div><p>If we were considering a <a href=/obsidian-quartz/statistics/inferential-statistics/3-Hypothesis-Testing rel=noopener class=internal-link data-src=/obsidian-quartz/statistics/inferential-statistics/3-Hypothesis-Testing>hypothesis test</a> with a test statistic of 2, under which of these scenarios would you reject a null hypothesis at the 5% level?</p><ol><li><p>You would reject the null hypothesis</p></li><li><p>You would not reject the null hypothesis</p></li><li><p>You would not reject the null hypothesis</p></li></ol><p>As we get more conservative with a t-distribution with lower degrees of freedom, we also become less likely to reject the null hypothesis. We&rsquo;ll discuss how to calculate degrees of freedom later, but in general - the smaller your sample size, the lower the df, and the harder it is to reject the null hypothesis.</p><a href=#origins-of-the-t-distribution><h3 id=origins-of-the-t-distribution><span class=hanchor arialabel=Anchor># </span>Origins of the t-distribution</h3></a><ul><li>Developed by William Gosset (whose pseudonym was Student, hence Student&rsquo;s t-distribution)</li><li>Gosset worked at Guinness Brewing Company as the Head Experimental Brewer, and his job was to gradually create a more consistent and economical Guinness brew.</li><li>Working under constraints, Gosset generally only had small sample sizes to work with, so most of the groundwork laid for this distribution was done in service to the Guinness stout.</li></ul><a href=#inference-for-a-mean><h2 id=inference-for-a-mean><span class=hanchor arialabel=Anchor># </span>Inference for a mean</h2></a><a href=#example><h4 id=example><span class=hanchor arialabel=Anchor># </span>Example</h4></a><p>Research paper discussing and analysing the effects of being distracted while you eat, and how it affects your recall of what you ate. Hypothesis is that people who did not recall what they ate for lunch will be more prone to snacking later on.</p><p>44 people in this study, 22 men and 22 women. Randomised into two groups; one played solitaire whilst eating, and one who just ate. After lunch, they were offered biscuits to snack on</p><p>Results are as follows. The goal for this video is to estimate average snacking level for distracted eaters.</p><p><a class="internal-link broken">Pasted image 20230712220330.png</a></p><p><strong>Estimating the mean</strong></p><p>Let&rsquo;s aim to estimate the mean biscuit intake of the sample who played video games.</p><ul><li><p>To estimate the mean, we&rsquo;ll create a confidence interval. For t-distributions, the calculation of the standard error is the same as the normal distribution, so the only thing that changes in our CI formula is the critical value:
$$ ext{point estimate} \pm t_{df}^* imes rac{s}{\sqrt{n}}$$</p></li><li><p>When working with data from one sample, and working with one single mean,
$$df = n - 1$$</p></li><li><p>We lose one degree of freedom because we&rsquo;re estimating the standard error of the sample mean using the sample standard deviation.</p></li><li><p>To get the critical t-statistic, we use the inverse t-distribution function, <code>qt(percentile, df)</code>:</p></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>qt</span><span class=p>(</span><span class=m>0.025</span><span class=p>,</span> <span class=n>df</span> <span class=o>=</span> <span class=m>22</span> <span class=o>-</span> <span class=m>1</span><span class=p>)</span> <span class=c1># since the sample size was 22</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; -2.07961384472768</span>
</span></span></code></pre></td></tr></table></div></div><p>We now have our mean, sample standard deviation, sample size, and critical t-statistic. Our confidence interval is then:</p><p>$$52.1 \pm 2.08 imes rac{45.1}{\sqrt{22}} = \left(32.1, 72.1
ight)$$</p><p>Therefore, we are 95% confident that distracted eaters consume between 32.1g and 72.1g of snacks post-meal.</p><p><strong>Performing a hypothesis test</strong></p><p>Next, consider that the suggested serving size of biscuits is 30g; do these data suggest that there is a statistically significant difference between the suggested serving size and the amount that distracted eaters eat?</p><ul><li>$H_0: \mu = 30, H_A: \mu
eq 30$</li><li>$T = rac{52.1 - 30}{9.62} = 2.3$</li><li>$df = 22 - 1$</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>pt</span><span class=p>(</span><span class=m>-2.3</span><span class=p>,</span> <span class=m>21</span><span class=p>)</span> <span class=o>*</span> <span class=m>2</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 0.0318022759865702</span>
</span></span></code></pre></td></tr></table></div></div><p>This is less than the significance level of 5%, so we reject the null hypothesis and conclude that distracted eaters consume, on average, more than the suggested serving size of 30.</p><p><strong>Conditions</strong></p><p>Looping back around to check that the conditions are met:</p><ul><li><strong>Independence</strong>: assignment was random, and 22 &lt; 10% of all distracted eaters</li><li><strong>Sample size/skew</strong>: we are given the sample mean and sample standard deviation, we can tell that the distribution will be cut off near 0 since the standard deviation is so close to value of the mean.</li></ul><p><a class="internal-link broken">Pasted image 20230712221251.png</a></p><a href=#inference-for-comparing-two-independent-means><h2 id=inference-for-comparing-two-independent-means><span class=hanchor arialabel=Anchor># </span>Inference for comparing two independent means</h2></a><p><strong>Confidence intervals</strong></p><p>To produce confidence intervals for the difference between two means, we do the following:</p><p>$$\left(ar{x}_1 - ar{x}<em>2
ight) \pm t^*</em>{df} imes\sqrt{ rac{s_1^2}{n_1} + rac{s_2^2}{n_2}}$$</p><p>Note that we add the two variances even though we&rsquo;re looking for the standard error of the difference between the two means. Conceptually, this is bringing together two measures with an inherent variability around them. When you bring two unknowns together, the result should always be more variable (regardless of if you&rsquo;re adding or subtracting them).</p><p>$$df = min(n_1-1, n_2-1)$$</p><p>This is a conservative (since it relies on the lower df) estimate, because the exact df value is tedious to compute by hand.</p><p><strong>Conditions</strong></p><ul><li>Independence<ul><li>Within groups: sampled observations must be independent<ul><li>Random sampling/assignment</li><li>If sampling without replacement, both $n_1$ and $n_2$ to be &lt; 10% of their respective populations</li></ul></li><li>Between groups: groups must be independent of one another (non-paired)<ul><li>If they are paired, we can use other methods</li></ul></li></ul></li><li>Sample size/skew<ul><li>The more skew in the population distibution, the larger the sample size we need</li></ul></li></ul><p><strong>Example</strong></p><p>Coming back to our example,</p><p>$$\left(ar{x}<em>{wd} - ar{x}</em>{wod}
ight) \pm t^*<em>{df} imes\sqrt{ rac{s</em>{wd}^2}{n_{wd}} + rac{s_{wod}^2}{n_{wod}}} = \left(52.1 - 27.1
ight) \pm 2.08 imes \sqrt{ rac{45.1^2}{22} + rac{26.4^2}{22}} = \left(1.83, 48.17
ight)$$</p><p>So we are 95% confident that those who eat with distractions consume 1.83 g and 48.17 g <em>more</em> snacks than those who eat without distractions, on average.</p><p><strong>Hypothesis tests</strong></p><p>Do these data provide convincing evidence of a difference in post-meal consumption between the two groups?</p><ul><li><p>$H_0: \mu_{wd} - \mu_{wod} = 0, H_A: \mu_{wd} - \mu_{wod}
eq 0$</p></li><li><p>$T_{21} = rac{25 - 0}{11.24} = 2.24$; 25 is the difference between the two means, and 11.24 is the previously calculated SE</p></li><li><p>The corresponding p-value is:</p></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>pt</span><span class=p>(</span><span class=m>-2.24</span><span class=p>,</span> <span class=m>21</span><span class=p>)</span> <span class=o>*</span> <span class=m>2</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 0.0360369621567888</span>
</span></span></code></pre></td></tr></table></div></div><p>Hence, there is strong evidence in favour of the alternative hypothesis that there is a difference between the two groups. Note this result agrees with our confidence interval which does not include 0.</p><a href=#inference-for-comparing-two-paired-means><h2 id=inference-for-comparing-two-paired-means><span class=hanchor arialabel=Anchor># </span>Inference for comparing two paired means</h2></a><p>We discuss here methods for dealing with two means that are dependent on one another, this is especially useful for studies that take multiple measurements over the same group of people. We could also design studies as paired if we were studying groups that we suspect are not independent, such as twins or partners. We&rsquo;ll start with an example:</p><p><a class="internal-link broken">Pasted image 20230712221306.png</a></p><ul><li>The median writing score is higher than the median reading score</li><li>Both distributions seem fairly symmetric, but the reading score is slightly more right skewed (the median is closer to the left side of the distribution)</li></ul><p>Can a student&rsquo;s reading and writing score be assumed independent of one another? Likely not, as these are adjacent skillsets.</p><ul><li>When two sets of datasets have this special correspondence, they are said to be <em>paired</em>.</li><li>When looking at paired distributions, it&rsquo;s often useful to look at the difference between the two: diff = read - write</li></ul><p>Here, the parameter of interest is $\mu_{diff}$, the average difference between the reading and writing scores of <em>all</em> high school students.</p><ul><li>Since we don&rsquo;t have this, we&rsquo;ll estimate the average difference between the reading and writing scores of the sampled high school students: $ar{x}_{diff}$</li></ul><p>For the hypothesis test - we&rsquo;re now doing a hypothesis test on a <em>single</em> population mean:</p><ul><li>$H_0: \mu_{diff} = 0, H_A: \mu_{diff}
eq 0$</li><li>We&rsquo;re also told the following: $ar{x}<em>{diff} = -0.545, s</em>{diff} = 8.887, n_{diff} = 200$</li><li>So $T = rac{-0.545 - 0}{ rac{8.887}{\sqrt{200}}} = -0.87$; $df = 200 - 1 = 199$</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>pt</span><span class=p>(</span><span class=m>-0.87</span><span class=p>,</span> <span class=m>199</span><span class=p>)</span> <span class=o>*</span> <span class=m>2</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 0.385348599393971</span>
</span></span></code></pre></td></tr></table></div></div><p>So the probability is 38.5% to obtain a random sample of 200 students where the average difference between reading and writing is at least 0.545, if there truly is no average difference between reading and writing scores.</p><a href=#power><h2 id=power><span class=hanchor arialabel=Anchor># </span>Power</h2></a><p>Recall that the power of the test is the probability of correctly rejecting $H_0$ when it is indeed false. Recall also that there is a trade-off between $lpha$ (the probability of committing a Type I error) and $eta$ (probability of Type II error), and that one way to decrease both is by raising the sample size.</p><p>Two competing considerations when designing experiments:</p><ul><li>We want to detect enough data so that we can detect important effects</li><li>But collecting data is expensive</li></ul><p>In this lecture, we&rsquo;ll look at the example of designing a medicine study so that we have an 80% chance of detecting a practically significant effect (i.e. we have 80% power - a commonly used power cut-off).</p><a href=#example-1><h4 id=example-1><span class=hanchor arialabel=Anchor># </span>Example</h4></a><p>Suppose a pharmaceutical company has developed a new drug for lowering blood pressure and they are preparing a clinical trial to test the drug&rsquo;s effectiveness. They recruit people who are taking a particular standard blood pressure medication, and half of the subjects are given the new drug, this is the treatment group. And the other half continued to take their medication through generic-looking pills to ensure blinding, this is our control group. What are the hypotheses for a two-sided hypothesis test in this context?</p><ul><li>$\mu$ will be the average blood pressure in any one of our groups.<ul><li>$H_0: \mu_{treatment} - \mu_{control} = 0, H_A: \mu_{treatment} - \mu_{control}
eq 0$</li></ul></li></ul><p>Suppose researchers would like to run this clinical trial on patients with systolic blood pressures between 140 and 180 millimeter of mercury. Suppose previously published studies suggest that the standard deviation of the patients&rsquo; blood pressures will be about 12 millimeters of mercury and that the distribution of patients&rsquo; blood pressures will be approximately symmetric.</p><p>If we had 100 patients per group, what would be the approximate standard error for difference in sample means of the treatment and control groups?</p><ul><li>This is a test of difference between two independent groups, therefore the standard error is defined as:</li></ul><p>$$SE = \sqrt{ rac{12^2}{100} + rac{12^2}{100}} = 1.70 mmHg$$</p><p>For what values of the difference between the observed average of blood pressure in treatment and control groups would we reject the null hypothesis at the 5% level?</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>qnorm</span><span class=p>(</span><span class=m>0.025</span><span class=p>,</span> <span class=m>0</span><span class=p>,</span> <span class=m>1.7</span><span class=p>);</span> <span class=nf>qnorm</span><span class=p>(</span><span class=m>0.975</span><span class=p>,</span> <span class=m>0</span><span class=p>,</span> <span class=m>1.7</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; -3.33193877371809</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 3.33193877371809</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nf>pnorm</span><span class=p>(</span><span class=m>-0.2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 0.420740290560897</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>Any difference between the means outside of this range would cause a rejection of the null hypothesis.</li></ul><p><strong>Power from first principles</strong></p><p>Suppose that the company researchers care about finding any effect on blood pressure that is 3 millimeters of mercury or larger versus the standard medication. What is the power of the test that can detect this effect?</p><ul><li><p>3mmHg is the minimum effect size of interest</p></li><li><p>Power is the probability that we will reject the null hypothesis, conditional on the alternative hypothesis being true</p><ul><li>If the alternative hypothesis were true, the true distribution of differences will be normally distributed (as previously given to us about the original distribution) around the -3 difference</li><li>We also know that we will reject the null hypothesis if the difference were at least -3.33</li></ul></li></ul><p>We have enough information to calculate the probability now:
$$Z = rac{-3.33 - (-3)}{1.70} = -0.20 o P(Z &lt; -0.20) = \dots$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>pnorm</span><span class=p>(</span><span class=m>-0.20</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 0.420740290560897</span>
</span></span></code></pre></td></tr></table></div></div><p><a class="internal-link broken">Pasted image 20230712221400.png</a></p><p>So the power of this test is around 42% when the minimum effect size is -3 and the group has a sample size of 100. This is clearly lower than the desired 80% power.</p><p><strong>Deriving the sample size from a given power level</strong></p><ul><li><p>Note that the minimum effect size remains at -3, but the standard error will change since this varies with the sample size</p></li><li><p>Now that we know the power of the test is equivalent to the probability to the left of the critical value that would cause the null hypothesis to be rejected</p><ul><li>So we will revisit the distribution of differences if the true difference were -3, calculate the Z-score where the area to the left of the distribution is 0.8, then see where that sits on the null hypothesis distribution.</li></ul></li></ul><p>The Z-score that marks the 80th percentile of the normal curve is 0.84:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>qnorm</span><span class=p>(</span><span class=m>0.8</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 0.841621233572914</span>
</span></span></code></pre></td></tr></table></div></div><p>So the distance between the centre of the alternative hypothesis distribution and the cutoff region is $0.84 * SE$, where the $SE$ is still unknown.</p><p><a class="internal-link broken">Pasted image 20230712221425.png</a></p><p>Another piece of information is that we know the cutoff region is 1.96SE from the mean of the null hypothesis distribution, so 0 - 1.96SE. What we&rsquo;re about to do assumes that the distribution between the alternative distribution and the null distribution are the same, and this would be true if the drug reduced the average blood pressure, but did not change its overall variability.</p><p>From this, we know now that the distance between 3mmHg is spanned by 0.84 + 1.96 standard errors, and we can solve for $n$:</p><p><a class="internal-link broken">Pasted image 20230712221435.png</a></p><p>(Instructor really got in the way here)</p><p>One final point is that, we know that power increases as sample sizes does, but there are diminishing returns past a certain point. The range of powers that correspond to the following sample sizes are charted as follows:</p><p><a class="internal-link broken">Pasted image 20230712221441.png</a></p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://wongd-hub.github.io/obsidian-quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Darren Wong using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://wongd-hub.github.io/obsidian-quartz/>Home</a></li><li><a href=https://www.herdmentality.xyz/>Herd Mentality (Blog)</a></li><li><a href=https://github.com/wongd-hub>GitHub</a></li></ul></footer></div></div></body></html>