<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="#course_coursera-inferential-stats #statistics
Comparing more than two means The data we&rsquo;ll consider here comes from the General Social Survey. We&rsquo;re interested in the average vocabulary score split by self-identified social class:"><meta property="og:title" content="6 ANOVA and Bootstrapping"><meta property="og:description" content="#course_coursera-inferential-stats #statistics
Comparing more than two means The data we&rsquo;ll consider here comes from the General Social Survey. We&rsquo;re interested in the average vocabulary score split by self-identified social class:"><meta property="og:type" content="website"><meta property="og:image" content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><meta property="og:url" content="https://wongd-hub.github.io/obsidian-quartz/statistics/inferential-statistics/6-ANOVA-and-Bootstrapping/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="6 ANOVA and Bootstrapping"><meta name=twitter:description content="#course_coursera-inferential-stats #statistics
Comparing more than two means The data we&rsquo;ll consider here comes from the General Social Survey. We&rsquo;re interested in the average vocabulary score split by self-identified social class:"><meta name=twitter:image content="https://wongd-hub.github.io/obsidian-quartz/icon.png"><title>6 ANOVA and Bootstrapping</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://wongd-hub.github.io/obsidian-quartz//icon.png><link href=https://wongd-hub.github.io/obsidian-quartz/styles.19109a40042e9f0e72e952fda4442a34.min.css rel=stylesheet><link href=https://wongd-hub.github.io/obsidian-quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://wongd-hub.github.io/obsidian-quartz/js/darkmode.df266a900377a99b203da11b154c20ec.min.js></script>
<script src=https://wongd-hub.github.io/obsidian-quartz/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://wongd-hub.github.io/obsidian-quartz/",fetchData=Promise.all([fetch("https://wongd-hub.github.io/obsidian-quartz/indices/linkIndex.bacec794210094a7a152e23a9bf2ef1e.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://wongd-hub.github.io/obsidian-quartz/indices/contentIndex.0de0a4abbc2598cfcd0f393527435165.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://wongd-hub.github.io/obsidian-quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://wongd-hub.github.io/obsidian-quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/wongd-hub.github.io\/obsidian-quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=wongd-hub.github.io/obsidian-quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://wongd-hub.github.io/obsidian-quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://wongd-hub.github.io/obsidian-quartz/>The Second ðŸ§ </a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>6 ANOVA and Bootstrapping</h1><p class=meta>Last updated
Aug 17, 2023
<a href=https://github.com/wongd-hub/obsidian-quartz/tree/hugo/content/statistics/inferential-statistics/6%20ANOVA%20and%20Bootstrapping.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#comparing-more-than-two-means>Comparing more than two means</a></li><li><a href=#anova>ANOVA</a><ol><li><a href=#breaking-down-anova-output>Breaking down ANOVA output</a></li><li><a href=#conclusion>Conclusion</a></li></ol></li><li><a href=#conditions-for-anova>Conditions for ANOVA</a><ol><li><a href=#independence>Independence</a></li><li><a href=#approximately-normal>Approximately normal</a></li><li><a href=#constantequal-variance>Constant/equal variance</a></li></ol></li><li><a href=#multiple-comparisons>Multiple comparisons</a><ol><li></li></ol></li><li><a href=#bootstrapping>Bootstrapping</a><ol><li></li></ol></li></ol></nav></details></aside><p>#course_coursera-inferential-stats #statistics</p><a href=#comparing-more-than-two-means><h2 id=comparing-more-than-two-means><span class=hanchor arialabel=Anchor># </span>Comparing more than two means</h2></a><p>The data we&rsquo;ll consider here comes from the General Social Survey. We&rsquo;re interested in the average vocabulary score split by self-identified social class:</p><p><a class="internal-link broken">Pasted image 20230712222908.png</a></p><p>We can use Analysis of Variance (ANOVA) to analyse 3+ means (whereas the <a href=/obsidian-quartz/statistics/inferential-statistics/5-t-distribution-and-Comparing-Two-Means rel=noopener class=internal-link data-src=/obsidian-quartz/statistics/inferential-statistics/5-t-distribution-and-Comparing-Two-Means>t-distribution</a> only works for comparing two means).</p><ul><li><p>Note that, as with the t-test, comparison groups with means that are further apart and with tighter variances will be more likely to be found as statistically significantly different from the other means.</p></li><li><p>ANOVA works with a new test statistic called $F$, which is accompanied by the F-distribution</p></li></ul><p><strong>ANOVA</strong></p><ul><li><p>As with in a t-test, ANOVA compares the means of multiple groups to see if they&rsquo;re so far apart that their difference cannot be reasonably attributed to sampling variability.</p></li><li><p>$H_0: \mu_1 = \mu_2 = \dots = \mu_k, H_A = ext{At least one pair of means are different to each other}$</p><ul><li>Where $k$ is the number of groups</li></ul></li><li><p>As with the t-statistic (a ratio of size of effect to standard error), the F-statistic is a ratio of the variability between groups over the variability within groups.</p><ul><li>$F = rac{ ext{variability between groups}}{ ext{variability within groups}}$</li><li>Recall that large test statistics lead to small p-values, which means you&rsquo;re more likely to find that there is at least one difference between a pair of means. Naturally, this means that you have a higher variability between groups relative to the variability within groups.</li></ul></li><li><p>The F-distribution is right skewed and always positive (as its a ratio of two variances, which can never be negative)</p></li></ul><p><a class="internal-link broken">Pasted image 20230712222919.png</a></p><a href=#anova><h2 id=anova><span class=hanchor arialabel=Anchor># </span>ANOVA</h2></a><p>ANOVA works on the idea of <em>variability partitioning</em>, which is the act of taking the overall variance and splitting it into:</p><ul><li>Variance caused by the independent variable (i.e. the between group variability)</li><li>Variance caused by all other factors (i.e. the within group variability)</li></ul><p><a class="internal-link broken">Pasted image 20230712222930.png</a></p><a href=#breaking-down-anova-output><h3 id=breaking-down-anova-output><span class=hanchor arialabel=Anchor># </span>Breaking down ANOVA output</h3></a><p>Here&rsquo;s a look at what an ANOVA output table looks like; we&rsquo;ll go through each component step by step.</p><ul><li>The first row is about between group variability, the second is about within group variability.</li><li>The third row displays the totals.</li></ul><p><a class="internal-link broken">Pasted image 20230712222939.png</a></p><a href=#sum-of-squares-column><h4 id=sum-of-squares-column><span class=hanchor arialabel=Anchor># </span>Sum of Squares column</h4></a><p><em>Sum of Squares Total (SST)</em></p><p>The last value in this column is Sum of Squares Total (SST), and it measures the total variability in the response variable. It is calculated in a similar manner to variance, but without adjusting for sample size. In other words, this is the sum of squared deviation from the mean in the response variable.</p><p>$$SST = \sum^n_{i=1}\left(y_i-ar{y}
ight)^2$$</p><p>Where $y_i$ represents each observation in the dataset, and $ar{y}$ is the grand mean across all observations.</p><p><em>Sum of Squares between Groups (SSG)</em></p><p>The first value in this column is called the Sum of Squares between Groups (SSG), and it measures the variability in the response variable that is explained by the independent variable.</p><p>This is calculated as the sum of deviations of group means from the overall mean, weighted by the sample size of each group:</p><p>$$SSG = \sum^k_{j=1}n_j\left(ar{y}_j-ar{y}
ight)$$</p><p>Where $n_j$ is the number of observations in group $j$, $ar{y}_j$ is the mean of the response variable in group $j$, and $ar{y}$ is the grand mean across all observations.</p><p>For this example, the SSG is about 7.6% of the SST; i.e. 7.6% of total variability in the vocabulary scores is explained by difference in social class, and the remainder is explained by other factors.</p><p><em>Sum of Squares Error (SSE)</em></p><p>This is the second value in the Sum of Square column, and it measures the unexplained variability due to all other variables. The simplest way to calculate this is:</p><p>$$SSE = SST - SSG$$</p><a href=#mean-square-values-column><h4 id=mean-square-values-column><span class=hanchor arialabel=Anchor># </span>Mean Square Values column</h4></a><p>To get from the Sum of Squares column to the Mean Square Values column, we need to scale the sum of squares by a measure that takes into account both:</p><ul><li>Sample size; and,</li><li>Number of groups</li></ul><p>We can use degrees of freedom to represent this; the degrees of freedom associated with ANOVA are:</p><ul><li>Total: $df_T=n-1$</li><li>Group: $df_G=k-1$</li><li>Error: $df_E=df_T-df_G$</li></ul><p>Next, the Mean Squares are calculated as follows:</p><ul><li>Group: $MSG=SSG/df_G$</li><li>Error: $MSE=SSE/df_E$</li></ul><a href=#f-statistic><h4 id=f-statistic><span class=hanchor arialabel=Anchor># </span>F-statistic</h4></a><p>The F-statistic is the ratio between the average between and within group variabilities:</p><p>$$F = rac{MSG}{MSE}$$</p><a href=#p-value><h4 id=p-value><span class=hanchor arialabel=Anchor># </span>P-value</h4></a><p>We can finally calculate our p-value which may be interpreted as the probability of observing our ratio of between and within group variabilities if fact the means of all groups are equal. A few things to note:</p><ul><li><p>The F-distribution uses two degrees of freedom: degrees of freedom between groups ($df_G$), and degrees of freedom of errors ($df_E$)</p></li><li><p>The F-statistic can <strong>never be negative</strong> and hence we only consider the right side of the distribution</p><ul><li>As a result of this, a more extreme statistic will always be more extreme in the positive direction</li></ul></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>pf</span><span class=p>(</span><span class=m>21.735</span><span class=p>,</span> <span class=m>3</span><span class=p>,</span> <span class=m>791</span><span class=p>,</span> <span class=n>lower.tail</span> <span class=o>=</span> <span class=kc>FALSE</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 1.55985531902116e-13</span>
</span></span></code></pre></td></tr></table></div></div><a href=#conclusion><h3 id=conclusion><span class=hanchor arialabel=Anchor># </span>Conclusion</h3></a><p>As with previous <a href=/obsidian-quartz/statistics/inferential-statistics/3-Hypothesis-Testing rel=noopener class=internal-link data-src=/obsidian-quartz/statistics/inferential-statistics/3-Hypothesis-Testing>hypothesis tests</a>:</p><ul><li>If the p-value is small/less than $lpha$, we reject the null hypothesis and conclude that there is evidence that at least one pair of means is different (although we can&rsquo;t tell which one at this stage).</li><li>Otherwise if the p-value is larger than $lpha$, we do not reject the null hypothesis and conclude that there is no evidence that any pair of means is statistically significantly different from the other.</li></ul><a href=#conditions-for-anova><h2 id=conditions-for-anova><span class=hanchor arialabel=Anchor># </span>Conditions for ANOVA</h2></a><p>There are three main conditions that need to be met for ANOVA to be valid:</p><ol><li><p>Independence</p><ul><li>Within groups: sampled observations must be independent</li><li>Between groups: the groups must be independent of each other (i.e. non-paired)</li></ul></li><li><p>Approximate normality: each group must be approximately normal</p></li><li><p>Equal variance: groups should have roughly equal variance</p></li></ol><a href=#independence><h3 id=independence><span class=hanchor arialabel=Anchor># </span>Independence</h3></a><p><em>Independence within groups</em></p><p>We can be confidence that this is the case if we have random sampling/assignment, and if each sample size is less than 10% of the respective population.</p><p><em>Independence between groups</em></p><p>Groups must be independent of another, and this requires careful consideration of the results to ensure there is no paired structure that is being caused by the study design.</p><ul><li>Not the end of the world if this isn&rsquo;t met, there is a method called Repeated Measures ANOVA that can deal with this case.</li></ul><a href=#approximately-normal><h3 id=approximately-normal><span class=hanchor arialabel=Anchor># </span>Approximately normal</h3></a><p>Especially important when the sample sizes are small, but will also be harder to check if that&rsquo;s the case. We can use normal probability plots within each group to check this assumption.</p><a href=#constantequal-variance><h3 id=constantequal-variance><span class=hanchor arialabel=Anchor># </span>Constant/equal variance</h3></a><p>i.e. homoskedastic groups. Especially important if sample sizes vary between groups. Can check this using side-by-side box-plots and by calculating standard deviation within each group.</p><a href=#multiple-comparisons><h2 id=multiple-comparisons><span class=hanchor arialabel=Anchor># </span>Multiple comparisons</h2></a><p>Finding a statistically significant result with ANOVA only tells us that one pair of means is different - but not which pairs differ. The method for determining which pair of means differs is simply to perform many pair-wise t-tests between each group&rsquo;s means. This is referred to as &lsquo;multiple comparisons&rsquo;. Note, to perform these tests, we&rsquo;ll need to reconsider our degrees of freedom and standard error; as well as our significance level.</p><p>Recall that the Type I error rate is the probability that you will commit a Type I error; performing many pair-wise tests simultaneously inflates your Type I error rate which is undesirable.</p><p>The solution to this is simple, use a modified, more conservative significance level. This is achieved by using the <em>Bonferroni correction</em>, which adjusts $lpha$ by the number of comparisons being considered.</p><p><strong>Bonferroni correction</strong></p><p>The adjusted significance level is defined as:</p><p>$$lpha^* = lpha/K$$</p><p>Where $K = rac{k\left(k-1
ight)}{2}$ is the number of comparisons that will take place in your multiple comparisons process ($k$ being the number of means you have on hand to compare).</p><p>Back to our example, we have four means so $k = 4$, and our initial $lpha$ is $0.05$. For this example, $K = 6$, and $lpha^* = 0.05 / 6 pprox 0.0083$</p><p><strong>Standard error for multiple pairwise comparisons</strong></p><p>$$SE = \sqrt{ rac{MSE}{n_1} + rac{MSE}{n_2}}$$</p><p>This is similar to the independent groups t-test, however the numerators are using the <em>mean squared error from the ANOVA output</em> instead of the individual sample standard deviations.</p><p>Recall that the mean squared error is essentially the average within group variance, so we&rsquo;re still getting at the same concept as when we were using the individual variances. If the constant variance assumption is satisfied, this measure will be very close to the sample standard deviations anyway.</p><p><strong>Degrees of freedom for multiple pairwise comparisons</strong></p><p>$$df = df_{E}$$
Instead of $min\left(n_1-1, n_2-1
ight)$ in the independent means t-test, this is the degrees of freedom for the error in the ANOVA output.</p><a href=#example><h4 id=example><span class=hanchor arialabel=Anchor># </span>Example</h4></a><p>Let&rsquo;s pick a pair and compare the means of lower and middle class vocabulary scores.</p><ul><li>Lower class ~ n: 41, mean: 5.07</li><li>Middle class ~ n: 331, mean: 6.76</li></ul><p>On to the hypothesis test:</p><ul><li><p>$H_0: \mu_{middle} - \mu_{lower} = 0, H_A: \mu_{middle} - \mu_{lower}
eq 0$</p></li><li><p>$T = rac{\left(ar{x}<em>{middle} - ar{x}</em>{lower}
ight) - 0}{\sqrt{ rac{MSE}{n_{middle}} + rac{MSE}{n_{lower}}}} = rac{6.76 - 5.07}{\sqrt{ rac{3.628}{41} + rac{3.628}{331}}} = 5.365$</p></li><li><p>$df = 791$ (from the table at the start of this article)</p></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>pt</span><span class=p>(</span><span class=m>-5.365</span><span class=p>,</span> <span class=m>791</span><span class=p>)</span> <span class=o>*</span> <span class=m>2</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 1.06389466632633e-07</span>
</span></span></code></pre></td></tr></table></div></div><p>1.06e-07 &lt; 0.0083, hence we reject the null hypothesis and note there is strong evidence that there is a statistically significant difference between the average vocabulary scores of lower and middle class Americans.</p><a href=#bootstrapping><h2 id=bootstrapping><span class=hanchor arialabel=Anchor># </span>Bootstrapping</h2></a><p>Say we take a random sample of 20 rental properties in Durham and want to estimate the population median (and construct a confidence interval for it).</p><p><a class="internal-link broken">Pasted image 20230712222959.png</a></p><p>This is a small sample and the sample distribution looks fairly skewed, so CLT-based methods would not be reliable here. This is where we draw on simulation-based methods such as bootstrapping.</p><p>In bootstrapping, we assume that each observation in the sample represents others in the population at large. This means that the bootstrap population is made up of the same observations that are in your sample, repeated an arbitrary number of times.</p><p>We don&rsquo;t actually create the bootstrap population, but instead simulate it by drawing from the original sample multiple times <em>with replacement</em>. The overall bootstrapping methodology is:</p><ul><li><p>Randomly sample from your original sample with replacement, until you have the same number of observations as in your original sample</p></li><li><p>Calculate your sample/bootstrap statistic with the bootstrapped sample</p></li><li><p>Repeat the previous two steps as many times as you need to create a bootstrap distribution (which is a sampling distribution, just using bootstrapped samples instead of samples drawn from the population)</p></li></ul><p>Once we have the bootstrap distribution, we can calculate confidence intervals in 2 ways:</p><ol><li><p>Percentile method: estimate a 95% (for example) confidence interval as the middle 95% of the distribution - the bounds of which would be the 2.5th and 97.5th percentiles of the bootstrap distribution.</p></li><li><p>Standard error method (ostensibly more accurate): estimate the interval as $ ext{sample statistic} \pm \space t^*<em>{df=n-1} imes SE</em>{boot}$, where $n$ is the original sample size.</p></li></ol><p><strong>Example</strong></p><p>This dot plot shows the distribution of medians of 100 bootstrap samples from the original sample. We want to estimate the 90% bootstrap confidence interval for the median rent, based on this bootstrap distribution, using the standard error method.</p><p><a class="internal-link broken">Pasted image 20230712223008.png</a></p><p>First, to get our t-statistic:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-r data-lang=r><span class=line><span class=cl><span class=nf>qt</span><span class=p>(</span><span class=m>0.05</span><span class=p>,</span> <span class=m>20</span> <span class=o>-</span> <span class=m>1</span><span class=p>,</span> <span class=n>lower.tail</span> <span class=o>=</span> <span class=kc>FALSE</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>#&gt; 1.72913281152137</span>
</span></span></code></pre></td></tr></table></div></div><p>Then, given that our sample median is 887, we get:</p><p>$$ ext{sample median} \pm t^*<em>{19} imes SE</em>{boot} = 887 \pm 1.73 imes 89.5758 = (732.1, 1041.9)$$</p><p>We&rsquo;re 90% confident that the population median of rental prices in Durham falls between these bounds.</p><a href=#percentile-vs-standard-error-methods><h4 id=percentile-vs-standard-error-methods><span class=hanchor arialabel=Anchor># </span>Percentile vs. Standard Error methods</h4></a><p>We can see that the bounds using either method are close to each other - but are not exactly the same in this case.</p><p><a class="internal-link broken">Pasted image 20230712223017.png</a></p><a href=#closing-remarks><h4 id=closing-remarks><span class=hanchor arialabel=Anchor># </span>Closing remarks</h4></a><ul><li><p>Simulation methods do not have as rigorous assumptions as CLT-based methods, however a larger sample size will yield more reliable results as usual.</p></li><li><p>If the bootstrap distribution is extremely skewed or sparse, the bootstrap interval might be unreliable.</p></li><li><p>A representative sample is still required - garbage in, garbage out.</p></li><li><p>Bootstrap vs sampling distributions</p><ul><li>Sampling distributions are created by taking random samples with replacement from the population</li><li>Bootstrapping distributions are created by taking random samples with replacement from the sample</li><li>Both are distributions of sample statistics</li></ul></li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://wongd-hub.github.io/obsidian-quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Darren Wong using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://wongd-hub.github.io/obsidian-quartz/>Home</a></li><li><a href=https://www.herdmentality.xyz/>Herd Mentality (Blog)</a></li><li><a href=https://github.com/wongd-hub>GitHub</a></li></ul></footer></div></div></body></html>